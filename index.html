<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="google-site-verification" content="OaSWua2pdfv0KF_FFiMg9mzJSLR7r9MytkWJI3mLf_8"/><title>Darmawan Jr - Labs</title><link rel="icon" href="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/portofolio%2Fadmin%2FAvatar.svg?alt=media&amp;token=622405c3-9dff-4483-af0c-ddc95fbe6445" type="image/svg+xml"/><meta name="author" content="Darmawan Jr"/><meta name="description" content="Darma is a Software Engineer known for flexibility in adapting to changing technologies and project demands."/><meta name="keywords" content="Darmawan Jr, Software Engineer, system design, software testing, project management"/><link rel="canonical" href="https://barbarpotato.github.io"/><meta property="og:description" content="Darma is a Software Engineer known for flexibility in adapting to changing technologies and project demands."/><meta property="og:url" content="https://barbarpotato.github.io/"/><meta property="og:type" content="website"/><meta name="next-head-count" content="12"/><link rel="preload" href="/Labs-6b86b273ff34f/_next/static/css/c4bf4af8da8590e9.css" as="style"/><link rel="stylesheet" href="/Labs-6b86b273ff34f/_next/static/css/c4bf4af8da8590e9.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/Labs-6b86b273ff34f/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/Labs-6b86b273ff34f/_next/static/chunks/webpack-c4a76f13c6256ea1.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/chunks/framework-ecc4130bc7a58a64.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/chunks/main-43fd309413b9c3ed.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/chunks/pages/_app-a647296dda1bec88.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/chunks/pages/index-952b33b83aa9d528.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/LEepomGiW7qIKpSxf8WMV/_buildManifest.js" defer=""></script><script src="/Labs-6b86b273ff34f/_next/static/LEepomGiW7qIKpSxf8WMV/_ssgManifest.js" defer=""></script></head><body><div id="__next"><style data-emotion="css-global 1b7scut">:host,:root,[data-theme]{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-3xs:0.45rem;--chakra-fontSizes-2xs:0.625rem;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0-5:0.125rem;--chakra-space-1-5:0.375rem;--chakra-space-2-5:0.625rem;--chakra-space-3-5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0-5:0.125rem;--chakra-sizes-1-5:0.375rem;--chakra-sizes-2-5:0.625rem;--chakra-sizes-3-5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-prose:60ch;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;--chakra-breakpoints-base:0em;--chakra-breakpoints-sm:30em;--chakra-breakpoints-md:48em;--chakra-breakpoints-lg:62em;--chakra-breakpoints-xl:80em;--chakra-breakpoints-2xl:96em;}.chakra-ui-light :host:not([data-theme]),.chakra-ui-light :root:not([data-theme]),.chakra-ui-light [data-theme]:not([data-theme]),[data-theme=light] :host:not([data-theme]),[data-theme=light] :root:not([data-theme]),[data-theme=light] [data-theme]:not([data-theme]),:host[data-theme=light],:root[data-theme=light],[data-theme][data-theme=light]{--chakra-colors-chakra-body-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-body-bg:var(--chakra-colors-white);--chakra-colors-chakra-border-color:var(--chakra-colors-gray-200);--chakra-colors-chakra-inverse-text:var(--chakra-colors-white);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-100);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-600);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-gray-500);}.chakra-ui-dark :host:not([data-theme]),.chakra-ui-dark :root:not([data-theme]),.chakra-ui-dark [data-theme]:not([data-theme]),[data-theme=dark] :host:not([data-theme]),[data-theme=dark] :root:not([data-theme]),[data-theme=dark] [data-theme]:not([data-theme]),:host[data-theme=dark],:root[data-theme=dark],[data-theme][data-theme=dark]{--chakra-colors-chakra-body-text:var(--chakra-colors-whiteAlpha-900);--chakra-colors-chakra-body-bg:var(--chakra-colors-gray-800);--chakra-colors-chakra-border-color:var(--chakra-colors-whiteAlpha-300);--chakra-colors-chakra-inverse-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-700);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-400);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-whiteAlpha-400);}</style><style data-emotion="css-global fubdgu">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;margin:0;font-feature-settings:"kern";}:where(*, *::before, *::after){border-width:0;border-style:solid;box-sizing:border-box;word-wrap:break-word;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}:where(pre, code, kbd,samp){font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}:where(b, strong){font-weight:bold;}small{font-size:80%;}:where(sub,sup){font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}:where(button, input, optgroup, select, textarea){font-family:inherit;font-size:100%;line-height:1.15;margin:0;}:where(button, input){overflow:visible;}:where(button, select){text-transform:none;}:where(
          button::-moz-focus-inner,
          [type="button"]::-moz-focus-inner,
          [type="reset"]::-moz-focus-inner,
          [type="submit"]::-moz-focus-inner
        ){border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}:where([type="checkbox"], [type="radio"]){box-sizing:border-box;padding:0;}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}input[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}input[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}:where(
          blockquote,
          dl,
          dd,
          h1,
          h2,
          h3,
          h4,
          h5,
          h6,
          hr,
          figure,
          p,
          pre
        ){margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}:where(ol, ul){margin:0;padding:0;}textarea{resize:vertical;}:where(button, [role="button"]){cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}:where(h1, h2, h3, h4, h5, h6){font-size:inherit;font-weight:inherit;}:where(button, input, optgroup, select, textarea){padding:0;line-height:inherit;color:inherit;}:where(img, svg, video, canvas, audio, iframe, embed, object){display:block;}:where(img, video){max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]):not(
          [data-focus-visible-disabled]
        ){outline:none;box-shadow:none;}select::-ms-expand{display:none;}:root,:host{--chakra-vh:100vh;}@supports (height: -webkit-fill-available){:root,:host{--chakra-vh:-webkit-fill-available;}}@supports (height: -moz-fill-available){:root,:host{--chakra-vh:-moz-fill-available;}}@supports (height: 100dvh){:root,:host{--chakra-vh:100dvh;}}</style><style data-emotion="css-global 1cgn62j">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-chakra-body-text);background:var(--chakra-colors-chakra-body-bg);transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::-moz-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*:-ms-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*,*::before,::after{border-color:var(--chakra-colors-chakra-border-color);}</style><style data-emotion="css 13luf83">.css-13luf83{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;gap:var(--chakra-space-2);}@media screen and (min-width: 48em){.css-13luf83{padding:var(--chakra-space-10);}}</style><div id="navigation" class="css-13luf83"><style data-emotion="css 1diqifa">.css-1diqifa{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);}</style><div class="css-1diqifa"><a href="https://barbarpotato.github.io"><style data-emotion="css k23joz">.css-k23joz{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);line-height:1.33;cursor:pointer;font-size:var(--chakra-fontSizes-2xl);color:#faf9ff;}@media screen and (min-width: 48em){.css-k23joz{line-height:1.2;}}</style><h2 class="chakra-heading css-k23joz"><span style="color:#bd93f9;font-weight:bold">🚀D</span>armawan</h2></a></div><style data-emotion="css 17xejub">.css-17xejub{-webkit-flex:1;-ms-flex:1;flex:1;justify-self:stretch;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;}</style><div class="css-17xejub"></div><style data-emotion="css k008qs">.css-k008qs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-k008qs"><style data-emotion="css dho3mt">.css-dho3mt{display:none;}@media screen and (min-width: 48em){.css-dho3mt{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><div class="css-dho3mt"><style data-emotion="css 1obbvhl">.css-1obbvhl{font-family:var(--chakra-fonts-heading);font-size:var(--chakra-fontSizes-xl);line-height:1.2;-webkit-margin-start:var(--chakra-space-6);margin-inline-start:var(--chakra-space-6);-webkit-margin-end:var(--chakra-space-6);margin-inline-end:var(--chakra-space-6);font-weight:small;color:#faf9ff;}</style><h2 class="chakra-heading navbar css-1obbvhl"><a href="https://barbarpotato.github.io/">Home</a></h2></div></div></div><style data-emotion="css 1y0lbsc">.css-1y0lbsc{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);text-align:center;margin-top:15px;margin-bottom:30px;color:#faf9ff;}@media screen and (min-width: 0em) and (max-width: 47.98em){.css-1y0lbsc{font-size:var(--chakra-fontSizes-xl);line-height:1.2;}}@media screen and (min-width: 48em){.css-1y0lbsc{font-size:var(--chakra-fontSizes-3xl);line-height:1.33;margin-top:50px;}@media screen and (min-width: 48em){.css-1y0lbsc{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}}</style><h2 class="chakra-heading css-1y0lbsc">What Are You Looking For?</h2><style data-emotion="css tz7fsy">.css-tz7fsy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-tz7fsy"><style data-emotion="css pf17z8">.css-pf17z8{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:90%;text-align:center;}@media screen and (min-width: 48em){.css-pf17z8{width:50%;}}</style><div class="css-pf17z8"><style data-emotion="css rvqnhz">.css-rvqnhz{width:100%;height:var(--input-height);font-size:var(--input-font-size);-webkit-padding-start:var(--input-padding);padding-inline-start:var(--input-padding);-webkit-padding-end:var(--input-padding);padding-inline-end:var(--input-padding);min-width:0px;outline:2px solid transparent;outline-offset:2px;position:relative;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);border:1px solid;background:inherit;color:#faf9ff;border-radius:var(--chakra-radii-2xl);border-width:3px;border-color:#536189;}.css-rvqnhz:disabled,.css-rvqnhz[disabled],.css-rvqnhz[aria-disabled=true],.css-rvqnhz[data-disabled]{opacity:0.4;cursor:not-allowed;}@media screen and (min-width: 0em) and (max-width: 61.98em){.css-rvqnhz{--input-font-size:var(--chakra-fontSizes-md);--input-padding:var(--chakra-space-4);--input-border-radius:var(--chakra-radii-md);--input-height:var(--chakra-sizes-10);}}@media screen and (min-width: 62em){.css-rvqnhz{--input-font-size:var(--chakra-fontSizes-lg);--input-padding:var(--chakra-space-4);--input-border-radius:var(--chakra-radii-md);--input-height:var(--chakra-sizes-12);}}.css-rvqnhz:hover,.css-rvqnhz[data-hover]{border-color:var(--chakra-colors-gray-300);}.css-rvqnhz[aria-readonly=true],.css-rvqnhz[readonly],.css-rvqnhz[data-readonly]{box-shadow:var(--chakra-shadows-none)!important;-webkit-user-select:all;-moz-user-select:all;-ms-user-select:all;user-select:all;}.css-rvqnhz[aria-invalid=true],.css-rvqnhz[data-invalid]{border-color:#E53E3E;box-shadow:0 0 0 1px #E53E3E;}.css-rvqnhz:focus-visible,.css-rvqnhz[data-focus-visible]{z-index:1;border-color:#ff79c6;box-shadow:0 0 0 1px #ff79c6;}</style><input placeholder="Search Content Labs..." class="chakra-input css-rvqnhz" value=""/></div></div><div style="opacity:0"><style data-emotion="css 1f86ups">.css-1f86ups{display:grid;grid-gap:var(--chakra-space-6);grid-template-columns:repeat(1, 1fr);margin-top:60px;margin-inline:20px;}@media screen and (min-width: 48em){.css-1f86ups{grid-template-columns:repeat(2, 1fr);margin-inline:12%;}}@media screen and (min-width: 80em){.css-1f86ups{grid-template-columns:repeat(3, 1fr);}}</style><div class="css-1f86ups"><style data-emotion="css 1yx4pbc">.css-1yx4pbc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;position:relative;min-width:0px;word-wrap:break-word;--card-bg:var(--chakra-colors-chakra-body-bg);background-color:var(--card-bg);color:var(--chakra-colors-chakra-body-text);border-width:var(--card-border-width, 0);border-color:var(--card-border-color);--card-radius:var(--chakra-radii-md);--card-padding:var(--chakra-space-5);--card-shadow:var(--chakra-shadows-base);border-radius:var(--chakra-radii-2xl);margin-bottom:var(--chakra-space-20);box-shadow:var(--chakra-shadows-dark-lg);}.chakra-ui-dark .css-1yx4pbc:not([data-theme]),[data-theme=dark] .css-1yx4pbc:not([data-theme]),.css-1yx4pbc[data-theme=dark]{--card-bg:var(--chakra-colors-gray-700);}</style><div class="chakra-card css-1yx4pbc"><style data-emotion="css 1n2mv2k">.css-1n2mv2k{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-1n2mv2k"><style data-emotion="css 1xv7wfv">.css-1xv7wfv{height:300px;border-radius:var(--chakra-radii-lg);}</style><img alt="rabbit mq" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090540692_rabbitmq.webp" class="chakra-image css-1xv7wfv"/></div><style data-emotion="css 150is1h">.css-150is1h{padding:var(--card-padding);-webkit-flex:1 1 0%;-ms-flex:1 1 0%;flex:1 1 0%;background-color:#292b37;}</style><div class="chakra-card__body css-150is1h"><style data-emotion="css 1y9k7lx">.css-1y9k7lx{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;height:200px;margin-top:var(--chakra-space-6);}</style><div class="css-1y9k7lx"><style data-emotion="css 1xlie6w">.css-1xlie6w{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-xl);line-height:1.2;color:#faf9ff;}</style><h2 class="chakra-heading css-1xlie6w">Understanding RabbitMQ: A Favorite Simple Messaging Service!</h2><style data-emotion="css 17oyhw1">.css-17oyhw1{margin-top:var(--chakra-space-2);margin-bottom:var(--chakra-space-2);color:#faf9ff;}</style><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-03-16 03:44:13</p><style data-emotion="css 1bp0ng0">.css-1bp0ng0{text-align:justify;color:#faf9ff;}</style><p class="chakra-text css-1bp0ng0">RabbitMQ is a robust, open-source message broker that facilitates communication between applications...</p><a href="/Labs-6b86b273ff34f/Understanding-RabbitMQ-A-Favorite-Simple-Messaging-Service" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Model Context Protocol" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742033609624_MCP.png" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Supercharge Your AI with MCP: The Future of Custom AI Tools</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-03-15 10:15:00</p><p class="chakra-text css-1bp0ng0">What if your AI assistant could do exactly what you need it to—like count words in a sentence, fetch...</p><a href="/Labs-6b86b273ff34f/Supercharge-Your-AI-with-MCP-The-Future-of-Custom-AI-Tools" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Circuit breaker" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741948558177_circuit_breaker.png" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Understanding Circuit Breakers in Software Engineering: From Traditional to Serverless</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-03-14 10:46:27</p><p class="chakra-text css-1bp0ng0">Imagine you’re using electricity at home, and a short circuit occurs. The circuit breaker in your el...</p><a href="/Labs-6b86b273ff34f/Understanding-Circuit-Breakers-in-Software-Engineering-From-Traditional-to-Serverless" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Apache Cassandra" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741944106846_apache_cassandra.png" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Apache Cassandra: The NoSQL Powerhouse</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-03-14 09:26:37</p><p class="chakra-text css-1bp0ng0">In today&#x27;s world of big data, scalability and performance are crucial. Apache Cassandra, an open-sou...</p><a href="/Labs-6b86b273ff34f/Apache-Cassandra-The-NoSQL-Powerhouse" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Api Versioning" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739712690763_api-versioning-strategy.jpg" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Why API Versioning is Really Important: A Lesson from My Own Mistake</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-02-16 13:35:40</p><p class="chakra-text css-1bp0ng0">As a developer, I&#x27;ve built countless APIs for my personal projects. Some were experimental, some tur...</p><a href="/Labs-6b86b273ff34f/Why-API-Versioning-is-Really-Important-A-Lesson-from-My-Own-Mistake" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Terraform with GCP" src="https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739669992396_gcp-terraform.png" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Terraform Labs: Automating Google Cloud Infrastructure Deployment</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-02-16 02:15:22</p><p class="chakra-text css-1bp0ng0">Manually managing cloud infrastructure can be time-consuming and error-prone. Terraform changes the ...</p><a href="/Labs-6b86b273ff34f/Terraform-Labs-Automating-Google-Cloud-Infrastructure-Deployment" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Jenkins Intro image" src="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fjenkins_background.png?alt=media&amp;token=8d8f21c7-f6bd-4157-8343-12090e88d13a" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Jenkins Unleashed: Transforming Your CI/CD Workflow for Lightning-Fast Delivery</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2025-01-06 12:32:13</p><p class="chakra-text css-1bp0ng0">In the fast-paced world of modern software development, delivering high-quality applications quickly...</p><a href="/Labs-6b86b273ff34f/Jenkins-Unleashed-Transforming-Your-CICD-Workflow-for-Lightning-Fast-Delivery" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="Kubernetes+GRPC Background" src="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc%20with%20kubernetes.png?alt=media&amp;token=ae61e7f8-2088-416f-924c-512461e18206" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Building a Robust Microservices Architecture: From gRPC to Kubernetes</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2024-12-26 07:34:04</p><p class="chakra-text css-1bp0ng0">In the ever-evolving world of software architecture, building a robust and scalable system is key to...</p><a href="/Labs-6b86b273ff34f/Building-a-Robust-Microservices-Architecture-From-gRPC-to-Kubernetes" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div><div class="chakra-card css-1yx4pbc"><div class="css-1n2mv2k"><img alt="HLS background" src="https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FHLS-IMAGE.jpg?alt=media&amp;token=8077c433-3d64-4627-86d5-e9605a6aa9a2" class="chakra-image css-1xv7wfv"/></div><div class="chakra-card__body css-150is1h"><div class="css-1y9k7lx"><h2 class="chakra-heading css-1xlie6w">Building a Video Streaming Platform with AWS S3, HLS, and Node.js</h2><p class="chakra-text css-17oyhw1">Published: <!-- -->2024-12-15 10:42:44</p><p class="chakra-text css-1bp0ng0">Ever wondered how your favorite streaming platforms deliver smooth, high-quality videos? Streaming v...</p><a href="/Labs-6b86b273ff34f/Building-a-Video-Streaming-Platform-with-AWS-S3-HLS-and-Nodejs" style="color:#bd93f9;text-decoration:underline">Read More...</a></div></div></div></div></div><style data-emotion="css 36xtvc">.css-36xtvc{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;margin-top:var(--chakra-space-10);}</style><div class="css-36xtvc"><style data-emotion="css 1uodvt1">.css-1uodvt1{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;gap:var(--chakra-space-2);}</style><div class="chakra-stack css-1uodvt1"><style data-emotion="css t6ko9n">.css-t6ko9n{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);border:1px solid;border-color:currentColor;background:var(--chakra-colors-transparent);color:var(--chakra-colors-white);}.css-t6ko9n:focus-visible,.css-t6ko9n[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-t6ko9n:disabled,.css-t6ko9n[disabled],.css-t6ko9n[aria-disabled=true],.css-t6ko9n[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-t6ko9n:hover,.css-t6ko9n[data-hover]{background:var(--chakra-colors-purple-50);}.css-t6ko9n:hover:disabled,.css-t6ko9n[data-hover]:disabled,.css-t6ko9n:hover[disabled],.css-t6ko9n[data-hover][disabled],.css-t6ko9n:hover[aria-disabled=true],.css-t6ko9n[data-hover][aria-disabled=true],.css-t6ko9n:hover[data-disabled],.css-t6ko9n[data-hover][data-disabled]{background:initial;}.chakra-button__group[data-attached][data-orientation=horizontal]>.css-t6ko9n:not(:last-of-type){-webkit-margin-end:-1px;margin-inline-end:-1px;}.chakra-button__group[data-attached][data-orientation=vertical]>.css-t6ko9n:not(:last-of-type){margin-bottom:-1px;}.css-t6ko9n:active,.css-t6ko9n[data-active]{background:var(--chakra-colors-purple-100);}</style><button type="button" class="chakra-button css-t6ko9n" disabled="">Prev</button><style data-emotion="css 167lkek">.css-167lkek{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);background:var(--chakra-colors-pink-500);color:var(--chakra-colors-white);}.css-167lkek:focus-visible,.css-167lkek[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-167lkek:disabled,.css-167lkek[disabled],.css-167lkek[aria-disabled=true],.css-167lkek[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-167lkek:hover,.css-167lkek[data-hover]{background:var(--chakra-colors-pink-600);}.css-167lkek:hover:disabled,.css-167lkek[data-hover]:disabled,.css-167lkek:hover[disabled],.css-167lkek[data-hover][disabled],.css-167lkek:hover[aria-disabled=true],.css-167lkek[data-hover][aria-disabled=true],.css-167lkek:hover[data-disabled],.css-167lkek[data-hover][data-disabled]{background:var(--chakra-colors-pink-500);}.css-167lkek:active,.css-167lkek[data-active]{background:var(--chakra-colors-pink-700);}</style><button type="button" class="chakra-button css-167lkek">1</button><button type="button" class="chakra-button css-t6ko9n">2</button><button type="button" class="chakra-button css-t6ko9n">3</button><button type="button" class="chakra-button css-t6ko9n">4</button><button type="button" class="chakra-button css-t6ko9n">Next</button></div></div><style data-emotion="css 1gk9h4c">.css-1gk9h4c{margin-top:var(--chakra-space-4);text-align:center;color:#faf9ff;}</style><p class="chakra-text css-1gk9h4c">Page <!-- -->1<!-- --> of <!-- -->4</p><style data-emotion="css 1n99gr6">.css-1n99gr6{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-top:100px;-webkit-margin-start:var(--chakra-space-10);margin-inline-start:var(--chakra-space-10);-webkit-margin-end:var(--chakra-space-10);margin-inline-end:var(--chakra-space-10);}</style><div class="css-1n99gr6"><div class="css-0"><style data-emotion="css keto3r">.css-keto3r{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);line-height:1.2;font-size:var(--chakra-fontSizes-2xl);color:#faf9ff;}</style><h2 class="chakra-heading css-keto3r"><span style="color:#bd93f9;font-weight:bold">🚀D</span>armawan</h2></div><style data-emotion="css 17xejub">.css-17xejub{-webkit-flex:1;-ms-flex:1;flex:1;justify-self:stretch;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;}</style><div class="css-17xejub"></div><style data-emotion="css k008qs">.css-k008qs{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}</style><div class="css-k008qs"><style data-emotion="css 12t13ll">.css-12t13ll{-webkit-margin-start:var(--chakra-space-1);margin-inline-start:var(--chakra-space-1);-webkit-margin-end:var(--chakra-space-1);margin-inline-end:var(--chakra-space-1);}</style><div class="css-12t13ll"><a target="_blank" href="https://www.instagram.com/darmajr94?igsh=OGgwNTRnaGFxeTY1" rel="noreferrer" aria-label="Visit my Instagram profile"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="social-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"></path></svg></a></div><div class="css-12t13ll"><a target="_blank" href="https://www.linkedin.com/in/darmawan-jr-b16135220/" rel="noreferrer" aria-label="Visit my LinkedIn profile"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="social-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div><div class="css-12t13ll"><a target="_blank" href="https://github.com/Barbarpotato" rel="noreferrer" aria-label="Visit my GitHub profile"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="social-icon" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></div><style data-emotion="css c54551">.css-c54551{-webkit-margin-start:var(--chakra-space-10);margin-inline-start:var(--chakra-space-10);-webkit-margin-end:var(--chakra-space-10);margin-inline-end:var(--chakra-space-10);margin-top:var(--chakra-space-10);margin-bottom:var(--chakra-space-10);}</style><div class="css-c54551"><style data-emotion="css 9fotzt">.css-9fotzt{opacity:0.6;border:0;border-color:inherit;border-style:solid;border-bottom-width:1px;width:100%;background-color:#bd93f9;height:2px;}</style><hr aria-orientation="horizontal" class="chakra-divider css-9fotzt"/></div><style data-emotion="css vpg6r9">.css-vpg6r9{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-bottom:var(--chakra-space-10);-webkit-margin-start:var(--chakra-space-10);margin-inline-start:var(--chakra-space-10);-webkit-margin-end:var(--chakra-space-10);margin-inline-end:var(--chakra-space-10);}</style><div class="css-vpg6r9"><style data-emotion="css 7jntw9">.css-7jntw9{font-size:var(--chakra-fontSizes-sm);color:#faf9ff;}</style><p class="chakra-text css-7jntw9">© 2023 - <!-- --> All Rights Reserved</p><style data-emotion="css 17xejub">.css-17xejub{-webkit-flex:1;-ms-flex:1;flex:1;justify-self:stretch;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;}</style><div class="css-17xejub"></div></div><span></span><span id="__chakra_env" hidden=""></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"articles":[{"blog_id":"6234fef8-1547-46f7-ae10-33d577a1d168","title":"Understanding RabbitMQ: A Favorite Simple Messaging Service!","short_description":"RabbitMQ is a robust, open-source message broker that facilitates communication between applications by sending and receiving messages. Whether you're building a microservices architecture or a distributed system, RabbitMQ ensures reliable, scalable, and asynchronous messaging. In this blog, we’ll walk through its core components and concepts, from producers to consumers, and dive into some advanced features like round-robin dispatching and virtual hosts.","timestamp":"2025-03-16 03:44:13","description":"\u003cp\u003eRabbitMQ is a robust, open-source message broker that facilitates communication between applications by sending and receiving messages. Whether you're building a microservices architecture or a distributed system, RabbitMQ ensures reliable, scalable, and asynchronous messaging. In this blog, we’ll walk through its core components and concepts, from producers to consumers, and dive into some advanced features like round-robin dispatching and virtual hosts.\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090760730_rabbit-mg-steps.png\" alt=\"rabbit mq floq\" width=\"720px\"\u003e\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003e1. The Producer: Where It All Begins\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn RabbitMQ, the \u003cstrong\u003eproducer\u003c/strong\u003e is the entity (e.g., an application or service) that generates and sends messages. It doesn’t directly deliver messages to queues—instead, it publishes them to an exchange. Think of the producer as a post office clerk dropping letters into a sorting system rather than delivering them to mailboxes.\u003c/p\u003e\u003cp\u003eProducers connect to RabbitMQ via a client library (available in languages like Python, Java, or Node.js) and specify the message content, routing details, and exchange to use.\u003c/p\u003e\u003cp\u003eOnce a producer has a message—say, a JSON object like {\"order_id\": 123, \"status\": \"pending\"}—it \u003cstrong\u003epublishes\u003c/strong\u003e it to RabbitMQ. The message isn’t just free-floating; it’s sent to an \u003cstrong\u003eexchange\u003c/strong\u003e, a key component that decides where the message goes next. Publishing is typically asynchronous, meaning the producer doesn’t wait for confirmation unless explicitly configured (e.g., with publisher confirms for reliability).\u003c/p\u003e\u003cp\u003eMessages can include metadata like headers or priority levels, but the core payload is what drives the system.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003e2. Exchange: The Message Router\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe \u003cstrong\u003eexchange\u003c/strong\u003e is RabbitMQ’s routing engine. It receives messages from producers and forwards them to queues based on rules called \u003cstrong\u003ebindings\u003c/strong\u003e. RabbitMQ supports several exchange types, each with unique routing logic:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirect Exchange\u003c/strong\u003e: Routes messages to queues based on an exact match between the message’s routing key (e.g., \"order.created\") and the queue’s binding key. Ideal for unicast scenarios.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFanout Exchange\u003c/strong\u003e: Ignores routing keys and broadcasts messages to all bound queues. Perfect for pub/sub patterns where every subscriber gets the message.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTopic Exchange\u003c/strong\u003e: Uses pattern matching on routing keys (e.g., \"order.\u003cem\u003e\" or \"\u003c/em\u003e.created\") to route messages to queues. Flexible for hierarchical or wildcard-based routing.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHeader Exchange\u003c/strong\u003e: Routes based on message header attributes rather than routing keys. Less common but useful for complex metadata-driven routing.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach exchange type serves a specific purpose, making RabbitMQ adaptable to diverse use cases.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003e3. Binding: Connecting Exchanges to Queues\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eA \u003cstrong\u003ebinding\u003c/strong\u003e is the link between an exchange and a queue. It defines the rules for how messages flow. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn a direct exchange, a binding might say, “Send messages with routing key ‘error’ to Queue A.”\u003c/li\u003e\u003cli\u003eIn a topic exchange, a binding could be “Send messages matching ‘*.log’ to Queue B.”\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBindings are configured by the application or administrator, ensuring messages reach the right destination based on the exchange’s logic.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003e4. Queues: The Message Holders\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eQueues\u003c/strong\u003e are where messages land after being routed by the exchange. They act as buffers, storing messages until a consumer retrieves them. Queues are durable (survive broker restarts) or transient, and they can have properties like message TTL (time-to-live) or maximum length.\u003c/p\u003e\u003cp\u003eA queue can be bound to multiple exchanges, and multiple queues can receive messages from the same exchange, depending on the binding rules.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003e5. Consume Message: The Consumer’s Role\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThe \u003cstrong\u003econsumer\u003c/strong\u003e is the application or service that retrieves messages from a queue and processes them. Consumers can operate in two modes:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePush\u003c/strong\u003e: RabbitMQ delivers messages to the consumer as they arrive (using a subscription model).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePull\u003c/strong\u003e: The consumer explicitly requests messages from the queue.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOnce a message is consumed, the consumer acknowledges it (manual or automatic ACK), telling RabbitMQ it’s been processed. If unacknowledged, the message can be requeued for another consumer—ensuring no data is lost.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eExtra Topic 1: Diagram of Round Robin Dispatching\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eRabbitMQ uses \u003cstrong\u003eround-robin dispatching\u003c/strong\u003e to distribute messages fairly among multiple consumers subscribed to the same queue. Here’s how it works:\u003c/p\u003e\u003cp\u003eImagine a queue with three consumers (C1, C2, C3) and five messages (M1, M2, M3, M4, M5). RabbitMQ delivers them like this:\u003c/p\u003e\u003cul\u003e\u003cli\u003eM1 → C1\u003c/li\u003e\u003cli\u003eM2 → C2\u003c/li\u003e\u003cli\u003eM3 → C3\u003c/li\u003e\u003cli\u003eM4 → C1\u003c/li\u003e\u003cli\u003eM5 → C2\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis ensures load balancing across consumers, assuming they’re all available and processing at similar rates. You can tweak this with prefetch settings (e.g., basic.qos) to limit how many unacknowledged messages a consumer handles at once.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eExtra Topic 2: Virtual Hosts\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA \u003cstrong\u003evirtual host\u003c/strong\u003e (vhost) in RabbitMQ is a logical separation within a single broker instance. Think of it as a tenant in a multi-tenant system. Each vhost has its own set of exchanges, queues, bindings, and permissions, isolated from others.\u003c/p\u003e\u003cp\u003eFor example:\u003c/p\u003e\u003cp\u003e- Vhost /app1 might handle order processing.\u003c/p\u003e\u003cp\u003e- Vhost /app2 might manage user notifications.\u003c/p\u003e\u003cp\u003eAdmins create vhosts via the RabbitMQ management interface or API, assigning users specific access rights. This isolation enhances security and organization, especially in shared environments.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRabbitMQ’s architecture—producers publishing to exchanges, exchanges routing via bindings to queues, and consumers processing messages—makes it a versatile tool for messaging needs. Features like round-robin dispatching ensure fair workload distribution, while virtual hosts provide logical separation for complex systems. Whether you’re broadcasting updates with fanout or filtering logs with topic exchanges, RabbitMQ has you covered.\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090540692_rabbitmq.webp","image_alt":"rabbit mq","slug":"Understanding-RabbitMQ-A-Favorite-Simple-Messaging-Service","index":"6b86b273ff34f"},{"blog_id":"1465bd4a-b953-4b4a-90bd-4cb93d97395c","title":"Supercharge Your AI with MCP: The Future of Custom AI Tools","short_description":"What if your AI assistant could do exactly what you need it to—like count words in a sentence, fetch live data, or even control smart devices—without waiting for the developers to add those features? That’s where the Model Context Protocol (MCP) comes in. It’s a powerful new framework from Anthropic that lets you extend AI models like Claude with custom tools you build yourself. In this blog, I’ll break down the concept of MCP, share how I used it to create a simple word-counting tool for Claude Desktop, and explore why this technology is so exciting for the future of AI.","timestamp":"2025-03-15 10:15:00","description":"\u003ch2\u003e\u003cstrong\u003eIntroduction: A New Way to Work with AI\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhat if your AI assistant could do exactly what you need it to—like count words in a sentence, fetch live data, or even control smart devices—without waiting for the developers to add those features? That’s where the \u003cstrong\u003eModel Context Protocol (MCP)\u003c/strong\u003e comes in. It’s a powerful new framework from Anthropic that lets you extend AI models like Claude with custom tools you build yourself. In this blog, I’ll break down the concept of MCP, share how I used it to create a simple word-counting tool for Claude Desktop, and explore why this technology is so exciting for the future of AI.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eWhat is MCP? A Bridge Between AI and Your Ideas\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe Model Context Protocol, or MCP, is like a universal translator that lets AI models (like Claude) talk to external tools you create. Imagine Claude as a super-smart assistant who can follow instructions—but only knows what’s built into it. MCP acts as a bridge, allowing Claude to send requests to your custom tool (say, a word counter) and get answers back, all in a standardized way.\u003c/p\u003e\u003cp\u003eHere’s the basic idea:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eClaude (the AI)\u003c/strong\u003e sends a message saying, “Hey, can you do this task for me?”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eYour Tool (the server)\u003c/strong\u003e listens for that message, does the work, and sends the result back.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Communication\u003c/strong\u003e happens through a simple format called JSON-RPC, using basic input/output channels (like a terminal’s input and output).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor example, I wanted Claude to count words in a sentence. With MCP, I built a small server that listens for Claude’s request, counts the words, and tells Claude the answer—like, “The text has 5 words.” It’s that straightforward!\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhy MCP is a Big Deal\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eSo, why should you care about MCP? Here are a few reasons it’s a game-changer:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMake AI Your Own\u003c/strong\u003e: You’re no longer stuck with what Claude can do out of the box. Want it to analyze your local files? Check stock prices? Turn on your smart lights? With MCP, you can make it happen by writing your own tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWorks with Any Language\u003c/strong\u003e: Whether you prefer Python, JavaScript, or another language, MCP doesn’t care—it’s designed to be flexible.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEndless Possibilities\u003c/strong\u003e: From simple tasks like my word counter to complex integrations (think connecting to APIs or databases), MCP lets you dream big.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen I got MCP working with Claude Desktop, I was amazed at how easy it was to teach Claude something new. I just told it, “Count the words in: hello world,” and it responded, “The text has 2 words.” That moment felt like magic—it showed me how MCP can turn AI into a truly personal assistant.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eHow Does MCP Work? The Big Picture\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAt a high level, MCP connects Claude to your tool in three main steps:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eThe Handshake\u003c/strong\u003e: When your tool starts, Claude sends a message called initialize to say hello and ask what your tool can do. Your tool responds by listing its capabilities—like, “I can count words.”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Request\u003c/strong\u003e: When you ask Claude to do something (e.g., “Count the words in this sentence”), it sends a message to your tool with the task details.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Response\u003c/strong\u003e: Your tool processes the request, does the work (like counting words), and sends the answer back to Claude, which then shares it with you.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis all happens behind the scenes, so to you, it just looks like Claude suddenly gained a new skill. In my case, I set up a Python-based server for Claude Desktop, and once it was running, Claude could count words as if it were a built-in feature.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eMy Journey: Building a Word Counter with MCP\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo see MCP in action, I decided to build a simple word-counting tool for Claude Desktop. The idea was straightforward: I’d ask Claude to count the words in a sentence, and my tool would do the work. Here’s what I learned from the experience:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIt’s Empowering\u003c/strong\u003e: Teaching Claude to count words felt like giving it a superpower. Suddenly, I could imagine all sorts of other tools I could build.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIt’s Accessible\u003c/strong\u003e: You don’t need to be a coding expert to get started. I used Python because it’s beginner-friendly, but the concept is the same no matter what language you choose.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIt’s Fun\u003c/strong\u003e: There’s something thrilling about seeing Claude use your tool for the first time. When I typed “Count the words in: hello world” and got “The text has 2 words” back, I couldn’t stop smiling.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eWhat’s Next for MCP? Imagine the Possibilities\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eMy word counter is just a starting point. MCP opens the door to so many exciting ideas:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProductivity Boosters\u003c/strong\u003e: Build a tool to summarize your local documents or fetch your calendar events.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCreative Helpers\u003c/strong\u003e: Create a tool that generates random writing prompts or analyzes the tone of your text.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSmart Integrations\u003c/strong\u003e: Connect Claude to live data—like weather updates or stock prices—or even control smart home devices.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe best part? MCP isn’t just for one person. Developers can share their tools with the community, creating a library of features anyone can add to Claude. Imagine downloading a “weather reporter” tool or a “code debugger” tool with a single click—that’s the future MCP could enable.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eConclusion: Join the MCP Revolution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe Model Context Protocol is more than just a tech concept—it’s a way to make AI truly yours. By building a simple word-counting tool for Claude Desktop, I got a glimpse of how MCP can transform the way we interact with AI. It’s easy to get started, incredibly powerful, and opens up a world of creativity.\u003c/p\u003e\u003cp\u003eIf you’re curious to try MCP yourself, Anthropic’s \u003ca href=\"https://modelcontextprotocol.io/quickstart/server\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(0, 102, 204);\"\u003e\u003cstrong\u003eMCP Quickstart Guide\u003c/strong\u003e\u003c/a\u003e is a great place to begin. Whether you’re a developer or just someone who loves tinkering with tech, MCP lets you take AI to the next level. What will you build for Claude? I’d love to hear your ideas—drop them in the comments below!\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742033609624_MCP.png","image_alt":"Model Context Protocol","slug":"Supercharge-Your-AI-with-MCP-The-Future-of-Custom-AI-Tools","index":"6b86b273ff34f"},{"blog_id":"86f7440f-033f-4459-b0a5-09f74d7c34ba","title":"Understanding Circuit Breakers in Software Engineering: From Traditional to Serverless","short_description":"Imagine you’re using electricity at home, and a short circuit occurs. The circuit breaker in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services","timestamp":"2025-03-14 10:46:27","description":"\u003ch2\u003eWhat Is a Circuit Breaker?\u003c/h2\u003e\u003cp\u003eImagine you’re using electricity at home, and a short circuit occurs. The \u003cem\u003ecircuit breaker\u003c/em\u003e in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services (APIs, databases, etc.).\u003c/p\u003e\u003ch2\u003eMain Purposes:\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003eDetect Failures\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe first job of a \u003cem\u003ecircuit breaker\u003c/em\u003e is to act as a vigilant watchdog, constantly monitoring interactions between your application and external services like APIs, databases, or third-party systems. It keeps an eye on every request, tracking whether they succeed or fail based on specific criteria, such as receiving an error code (e.g., HTTP 500), timing out after a set duration (e.g., no response within 2 seconds), or encountering exceptions like network disconnections.\u003c/p\u003e\u003cp\u003eTo do this effectively, the circuit breaker collects data over a defined window—perhaps the last 10 requests or the past 30 seconds—and calculates metrics like the total number of failures or the failure rate (e.g., 60% of calls failed). If these metrics cross a configurable threshold—say, five failures in a row or a 50% error rate—it recognizes that something’s wrong with the external service. This detection isn’t just about noticing a single hiccup; it’s about identifying patterns of unreliability that could harm your system if left unchecked. By catching these issues early, the circuit breaker ensures your application doesn’t blindly keep trying a service that’s clearly struggling.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003ePrevent Cascading Failures\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce a failure is detected, the circuit breaker steps in to stop a domino effect known as \u003cem\u003ecascading failures\u003c/em\u003e, where one broken component drags down the entire system. Imagine an e-commerce app where the payment API is down: without a circuit breaker, every user request might hang, waiting for a timeout, piling up server resources, slowing the database, and eventually crashing the whole application.\u003c/p\u003e\u003cp\u003eIn its Closed state, the circuit breaker allows calls to proceed, but as soon as failures hit the threshold, it flips to Open, cutting off all further attempts to contact the faulty service. This immediate halt prevents the problem from rippling through your system—your app stops wasting threads, memory, or CPU cycles on a hopeless task. Instead of letting a single point of failure—like a slow third-party API—overload your servers or exhaust connection pools, the circuit breaker isolates the issue, keeping the rest of your application stable and responsive. It’s like closing a floodgate to protect the town downstream from a burst dam.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eProvide a Fallback Response\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhen the circuit breaker blocks calls in its Open state, it doesn’t just leave users hanging—it offers a fallback response to keep the system usable. This fallback is a preplanned alternative to the failed service’s output, designed to minimize disruption.\u003c/p\u003e\u003cp\u003eFor example, if a weather API fails, the circuit breaker might return a cached forecast from an hour ago or a simple message like \"Weather data unavailable, try again later.\" In a payment system, it could redirect users to an alternative checkout method or log the attempt for later retry. The fallback doesn’t fix the root problem, but it ensures graceful degradation.\u003c/p\u003e\u003cp\u003eYour application keeps running in a limited capacity rather than crashing or showing cryptic errors. Crafting a good fallback requires understanding your use case: it might be static data, a default value, or even a call to a backup service. By providing this safety net, the circuit breaker maintains user trust and buys time for the external service to recover without sacrificing functionality entirely.\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741949039277_Circuit-Breaker-Pattern.jpg\" alt=\"Circuit Breaker Pattern\" width=\"720px\"\u003e\u003c/p\u003e\u003ch2\u003eOverall Mechanism\u003c/h2\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eClosed\u003c/strong\u003e: All calls are forwarded. If failures exceed the threshold (e.g., 5), it switches to Open.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOpen\u003c/strong\u003e: Calls are blocked, and a \u003cem\u003efallback\u003c/em\u003e is used. After a set time (e.g., 30 seconds), it moves to Half-Open.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHalf-Open\u003c/strong\u003e: A test call is made. Success → Closed, Failure → Open.\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eSimple Code Example\u003c/h3\u003e\u003cp\u003eHere’s a basic implementation in JavaScript:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eclass CircuitBreaker {\n\u0026nbsp; constructor(maxFailures = 5, resetTimeout = 30000) {\n\u0026nbsp; \u0026nbsp; this.state = \"CLOSED\";\n\u0026nbsp; \u0026nbsp; this.failureCount = 0;\n\u0026nbsp; \u0026nbsp; this.maxFailures = maxFailures;\n\u0026nbsp; \u0026nbsp; this.resetTimeout = resetTimeout;\n\u0026nbsp; }\n\n\n\u0026nbsp; async call(service) {\n\u0026nbsp; \u0026nbsp; if (this.state === \"OPEN\") {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; if (Date.now() \u0026gt; this.resetTime) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; this.state = \"HALF_OPEN\";\n\u0026nbsp; \u0026nbsp; \u0026nbsp; } else {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; return \"Fallback: Service unavailable\";\n\u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; }\n\n\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; const result = await service();\n\u0026nbsp; \u0026nbsp; \u0026nbsp; if (this.state === \"HALF_OPEN\") {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; this.state = \"CLOSED\";\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; this.failureCount = 0;\n\u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; return result;\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; this.failureCount++;\n\u0026nbsp; \u0026nbsp; \u0026nbsp; if (this.failureCount \u0026gt;= this.maxFailures) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; this.state = \"OPEN\";\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; this.resetTime = Date.now() + this.resetTimeout;\n\u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; return \"Fallback: Service unavailable\";\n\u0026nbsp; \u0026nbsp; }\n\u0026nbsp; }\n}\n\n\n// Example usage\nconst breaker = new CircuitBreaker();\nconst fakeService = () =\u0026gt; Math.random() \u0026gt; 0.5 ? \"Success\" : Promise.reject(\"Error\");\nbreaker.call(fakeService).then(console.log);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003eCircuit Breakers in Serverless\u003c/h2\u003e\u003cp\u003eIn a \u003cem\u003eserverless\u003c/em\u003e environment (e.g., AWS Lambda), \u003cem\u003ecircuit breakers\u003c/em\u003e are still valuable, but their stateless nature poses challenges. The state must be stored externally, such as in DynamoDB.\u003c/p\u003e\u003ch3\u003eExample in AWS Lambda\u003c/h3\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst AWS = require('aws-sdk');\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\n\nasync function handler(event) {\n\u0026nbsp; const serviceName = \"ExternalAPI\";\n\u0026nbsp; const state = await dynamodb.get({\n\u0026nbsp; \u0026nbsp; TableName: \"CircuitBreakerState\",\n\u0026nbsp; \u0026nbsp; Key: { Service: serviceName }\n\u0026nbsp; }).promise();\n\n\n\u0026nbsp; if (state.Item?.State === \"OPEN\" \u0026amp;\u0026amp; Date.now() \u0026lt; state.Item.ResetTime) {\n\u0026nbsp; \u0026nbsp; return { statusCode: 503, body: \"Service unavailable\" };\n\u0026nbsp; }\n\n\n\u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; const response = await callExternalAPI();\n\u0026nbsp; \u0026nbsp; if (state.Item?.State === \"HALF_OPEN\") {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; await dynamodb.update({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; TableName: \"CircuitBreakerState\",\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; Key: { Service: serviceName },\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; UpdateExpression: \"SET #state = :closed\",\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ExpressionAttributeNames: { \"#state\": \"State\" },\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ExpressionAttributeValues: { \":closed\": \"CLOSED\" }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; }).promise();\n\u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; return { statusCode: 200, body: response };\n\u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; // Logic to update failure count and switch to Open\n\u0026nbsp; \u0026nbsp; return { statusCode: 503, body: \"Service unavailable\" };\n\u0026nbsp; }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003eConclusion\u003c/h2\u003e\u003cp\u003e\u003cem\u003eCircuit breakers\u003c/em\u003e are a powerful pattern for building resilient systems, whether on traditional servers or in \u003cem\u003eserverless\u003c/em\u003e environments. With the simulations and code above, I hope you’ve gained a clearer understanding of how they work.\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741948558177_circuit_breaker.png","image_alt":"Circuit breaker","slug":"Understanding-Circuit-Breakers-in-Software-Engineering-From-Traditional-to-Serverless","index":"6b86b273ff34f"},{"blog_id":"19882a74-d1c2-4b31-837e-99cdc1846fcf","title":"Apache Cassandra: The NoSQL Powerhouse","short_description":"In today's world of big data, scalability and performance are crucial. Apache Cassandra, an open-source NoSQL database, is a top choice for handling large-scale, distributed data. Used by giants like Facebook, Netflix, and Twitter, Cassandra offers high availability, fault tolerance, and seamless scalability. Let’s dive into its architecture and key concepts!","timestamp":"2025-03-14 09:26:37","description":"\u003ch2\u003eWhy Choose Apache Cassandra?\u003c/h2\u003e\u003cp\u003eUnlike traditional relational databases, Cassandra is optimized for handling large workloads across distributed environments. Here’s why it stands out:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh Availability\u003c/strong\u003e: With no single point of failure, Cassandra ensures continuous uptime.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHorizontal Scalability\u003c/strong\u003e: Easily scale out by adding more nodes, avoiding the limitations of vertical scaling.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFault Tolerance\u003c/strong\u003e: Data replication across nodes guarantees resilience even in case of hardware failures.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOptimized for Write Operations\u003c/strong\u003e: Handles high-speed writes efficiently while offering reliable read performance.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFlexible Schema\u003c/strong\u003e: Unlike relational databases, Cassandra allows schema evolution without downtime.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eKey Architecture Components\u003c/h2\u003e\u003ch3\u003e1. \u003cstrong\u003eNodes, Clusters, and Data Centers\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNode\u003c/strong\u003e: The fundamental unit storing a portion of the data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCluster\u003c/strong\u003e: A network of nodes working together as a single system.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eData Center\u003c/strong\u003e: A logical grouping of nodes, often used to enhance redundancy across geographical regions.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2. \u003cstrong\u003ePartitioning \u0026amp; Token Ring\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCassandra distributes data across nodes using a \u003cstrong\u003epartitioning strategy\u003c/strong\u003e, ensuring efficient load balancing. Each node is assigned a \u003cstrong\u003etoken range\u003c/strong\u003e, and data is evenly distributed in a \u003cstrong\u003ering-based architecture\u003c/strong\u003e.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eReplication \u0026amp; Consistency\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTo ensure data availability and reliability, Cassandra employs \u003cstrong\u003ereplication\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReplication Factor (RF)\u003c/strong\u003e: Defines the number of copies of data stored across nodes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eConsistency Levels\u003c/strong\u003e: Controls how many nodes must acknowledge a read/write operation (e.g., ONE, QUORUM, ALL), allowing applications to balance performance and reliability.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e4. \u003cstrong\u003eStorage Engine: Commit Log \u0026amp; SSTables\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCommit Log\u003c/strong\u003e: A write-ahead log that captures every write operation for durability before data is flushed to disk.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMemtable\u003c/strong\u003e: A temporary in-memory data structure where writes are stored before being persisted to SSTables.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSSTables (Sorted String Tables)\u003c/strong\u003e: Immutable, append-only files storing actual data on disk, ensuring efficient retrieval and compaction.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCompaction\u003c/strong\u003e: The process of merging multiple SSTables to optimize read performance and free up disk space.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e5. \u003cstrong\u003eGossip Protocol \u0026amp; Failure Detection\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCassandra nodes communicate using the \u003cstrong\u003eGossip Protocol\u003c/strong\u003e, a peer-to-peer mechanism for state-sharing, failure detection, and decentralized management.\u003c/p\u003e\u003cul\u003e\u003cli\u003eEach node periodically exchanges state information with a subset of other nodes.\u003c/li\u003e\u003cli\u003eHelps maintain a decentralized and resilient system by enabling automatic failure recovery.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e6. \u003cstrong\u003eRead \u0026amp; Write Path in Cassandra\u003c/strong\u003e\u003c/h3\u003e\u003ch4\u003e\u003cstrong\u003eWrite Path:\u003c/strong\u003e\u003c/h4\u003e\u003col\u003e\u003cli\u003eData is written to the \u003cstrong\u003eCommit Log\u003c/strong\u003e for durability.\u003c/li\u003e\u003cli\u003eThe data is then stored in a \u003cstrong\u003eMemtable\u003c/strong\u003e (in-memory structure).\u003c/li\u003e\u003cli\u003eOnce the Memtable reaches its threshold, data is flushed to \u003cstrong\u003eSSTables\u003c/strong\u003e on disk.\u003c/li\u003e\u003cli\u003ePeriodic \u003cstrong\u003ecompaction\u003c/strong\u003e optimizes storage by merging SSTables.\u003c/li\u003e\u003c/ol\u003e\u003ch4\u003e\u003cstrong\u003eRead Path:\u003c/strong\u003e\u003c/h4\u003e\u003col\u003e\u003cli\u003eCassandra checks the \u003cstrong\u003eMemtable\u003c/strong\u003e for the latest data.\u003c/li\u003e\u003cli\u003eIf not found, it queries \u003cstrong\u003eBloom Filters\u003c/strong\u003e to identify relevant SSTables.\u003c/li\u003e\u003cli\u003eReads data from SSTables and merges results before returning them to the client.\u003c/li\u003e\u003c/ol\u003e\u003ch2\u003eHow Data is Stored \u0026amp; Queried\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003ePrimary Keys \u0026amp; Partitions\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCassandra structures data into \u003cstrong\u003etables\u003c/strong\u003e, similar to relational databases, but with more flexibility. Each table relies on a \u003cstrong\u003ePrimary Key\u003c/strong\u003e, which consists of:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePartition Key\u003c/strong\u003e: Determines data distribution across nodes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClustering Key\u003c/strong\u003e: Defines the sorting order of data within a partition.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e\u003cstrong\u003eQuerying with CQL (Cassandra Query Language)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCassandra utilizes CQL, a SQL-like query language tailored for distributed storage.\u003c/p\u003e\u003ch4\u003eExample Table Creation:\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eCREATE TABLE users (\n  id UUID PRIMARY KEY,\n  name TEXT,\n  email TEXT,\n  age INT\n);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHowever, to maintain speed and efficiency, Cassandra does not support SQL-like JOINs and complex ACID transactions.\u003c/p\u003e\u003ch2\u003eWhen to Use Cassandra?\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003eBest Use Cases:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eApplications requiring \u003cstrong\u003ehigh availability\u003c/strong\u003e (e.g., messaging apps, IoT data processing, recommendation engines)\u003c/li\u003e\u003cli\u003eLarge-scale \u003cstrong\u003ereal-time analytics\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDistributed content delivery\u003c/strong\u003e systems\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFinancial services\u003c/strong\u003e handling time-series data\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e\u003cstrong\u003eNot Ideal For:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003eComplex transactional applications requiring \u003cstrong\u003estrict ACID compliance\u003c/strong\u003e\u003c/li\u003e\u003cli\u003eApplications needing frequent \u003cstrong\u003eJOIN operations\u003c/strong\u003e and deep relational modeling\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eConclusion\u003c/h2\u003e\u003cp\u003eApache Cassandra is a powerful NoSQL database designed for organizations that need to manage high-velocity, large-scale data efficiently. Its distributed architecture, fault tolerance, and seamless scalability make it a prime choice for modern applications handling mission-critical workloads. If you're looking for a battle-tested NoSQL solution capable of global-scale operations, Cassandra is worth exploring!\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741944106846_apache_cassandra.png","image_alt":"Apache Cassandra","slug":"Apache-Cassandra-The-NoSQL-Powerhouse","index":"6b86b273ff34f"},{"blog_id":"52eb37c3-83bc-4442-a8c5-305bfba74e62","title":"Why API Versioning is Really Important: A Lesson from My Own Mistake","short_description":"As a developer, I've built countless APIs for my personal projects. Some were experimental, some turned into full-fledged applications, and others were simply abandoned over time. At first, managing these APIs felt simple—if I wasn't using an endpoint anymore, I would just delete it. Why keep something that I no longer need, right?  Well, that mindset came back to bite me.","timestamp":"2025-02-16 13:35:40","description":"\u003ch1\u003e\u003cstrong\u003eThe Mistake That Taught Me a Lesson\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOne day, I was cleaning up an old project, removing unused routes and refactoring the backend. There was this one API endpoint—let's call it /user/details—that I thought was no longer in use. Without a second thought, I deleted it and pushed the changes to production.\u003c/p\u003e\u003cp\u003eA few hours later, I started receiving errors from another service I had built months earlier. This service, which I had completely forgotten about, was still making requests to /user/details. Suddenly, parts of my application were broken, and I had no easy way to recover from it.\u003c/p\u003e\u003cp\u003eThat was the moment I truly understood why API versioning is critical.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhy API Versioning Matters\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cstrong\u003e1. Prevents Breaking Changes\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen APIs evolve, clients relying on them should not break due to changes. By implementing versioning (e.g., /v1/user/details), I could have introduced a new version while keeping the old one intact for existing consumers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Maintains Backward Compatibility\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eEven if you think an API is no longer needed, there’s a chance some service or third-party client is still using it. Versioning allows developers to deprecate old APIs gradually rather than abruptly removing them.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Gives Users Time to Migrate\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf an API must change, users need time to update their applications. Providing multiple versions (e.g., /v1/, /v2/) ensures a smooth transition.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4. Helps in Debugging and Maintenance\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen multiple versions exist, issues can be traced more easily. If a bug appears in /v2/ but not in /v1/, it’s easier to identify what changes might have caused it.\u003c/p\u003e\u003ch2\u003eHow to Implement API Versioning\u003c/h2\u003e\u003ch3\u003e1. \u003cstrong\u003eURL Versioning\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe most common and widely adopted approach to API versioning is using version numbers in the URL.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e/v1/users\n/v2/users\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003ePros:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEasy to understand and implement\u003c/strong\u003e – Developers can quickly identify which version is being used.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eClear distinction between versions\u003c/strong\u003e – Each version has its own endpoint, ensuring that changes do not interfere with older versions.\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003eCons:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCan lead to bloated URLs\u003c/strong\u003e – If too many versions exist, the API can become cluttered.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMight require modifying routes and maintaining multiple endpoints\u003c/strong\u003e – Developers must maintain multiple versions, which can increase complexity over time.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2. \u003cstrong\u003eHeader Versioning\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAnother approach is to use HTTP headers to specify the API version instead of embedding it in the URL.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eAccept: application/vnd.myapi.v1+json\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003ePros:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eKeeps URLs clean\u003c/strong\u003e – There’s no need to modify the URL structure, making it aesthetically cleaner.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAllows more flexibility without changing routes\u003c/strong\u003e – Clients can request different versions dynamically using headers.\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003eCons:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRequires clients to send custom headers explicitly\u003c/strong\u003e – Clients must be aware of the correct headers to use, which adds complexity.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMight be harder to test and debug compared to URL versioning\u003c/strong\u003e – Since versioning is not visible in the URL, debugging and API documentation can be more challenging.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e3. \u003cstrong\u003eQuery Parameter Versioning\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis method involves specifying the API version as a query parameter in the request.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e/users?version=1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003ePros:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSimple to implement and does not require changes to routes\u003c/strong\u003e – The backend can handle different versions without modifying the API structure.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCan be easily handled on the backend\u003c/strong\u003e – Developers can dynamically parse the version parameter and route requests accordingly.\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003eCons:\u003c/h4\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCan lead to inconsistent API calls if clients forget to include the version\u003c/strong\u003e – If a request is made without the version parameter, it may result in unintended behavior.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMay clutter the query string with additional parameters\u003c/strong\u003e – This approach can become cumbersome if multiple parameters are needed.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eChoosing the Right API Versioning Strategy\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eEach of these methods has its strengths and weaknesses, and the best approach depends on the specific needs of your project. If you want a simple and widely understood method, \u003cstrong\u003eURL versioning\u003c/strong\u003e might be the best choice. If you prefer a cleaner URL structure, \u003cstrong\u003eheader versioning\u003c/strong\u003e could be a better fit. And if you need quick implementation without altering routes, \u003cstrong\u003equery parameter versioning\u003c/strong\u003e is a viable option.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eFinal Thoughts\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eI learned the hard way that careless API deletions can lead to unexpected failures. If I had implemented proper versioning, I could have safely iterated on my APIs without breaking my own services.\u003c/p\u003e\u003cp\u003eSo, if you're developing APIs—whether for personal projects or production systems—take API versioning seriously. Your future self (and your users) will thank you!\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739712690763_api-versioning-strategy.jpg","image_alt":"Api Versioning","slug":"Why-API-Versioning-is-Really-Important-A-Lesson-from-My-Own-Mistake","index":"6b86b273ff34f"},{"blog_id":"911a9001-3c3e-4f2c-aa83-4ec4f6f71c99","title":"Terraform Labs: Automating Google Cloud Infrastructure Deployment","short_description":"Manually managing cloud infrastructure can be time-consuming and error-prone. Terraform changes the game by allowing you to define infrastructure as code, making deployment faster, scalable, and repeatable. With Terraform, you can automate cloud resource creation, track changes, and collaborate effortlessly.","timestamp":"2025-02-16 02:15:22","description":"\u003ch2\u003e\u003cstrong\u003eHow Does Terraform Work?\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003e1. Declarative Configuration\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform uses a special language called \u003cstrong\u003eHashiCorp Configuration Language (HCL)\u003c/strong\u003e. Instead of writing step-by-step instructions like in traditional programming, you just \u003cstrong\u003edescribe what you want\u003c/strong\u003e (e.g., \"I need a virtual machine with 2 CPUs and 4GB RAM\"), and Terraform figures out how to make it happen.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e2. Configuration Interpretation\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhen you run a Terraform command, it reads your configuration files and \u003cstrong\u003eunderstands what infrastructure you want to create or update\u003c/strong\u003e.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e3. Interaction with Cloud APIs\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform then communicates with cloud providers like \u003cstrong\u003eGoogle Cloud, AWS, or Azure\u003c/strong\u003e by sending \u003cstrong\u003eAPI requests\u003c/strong\u003e. This tells the cloud provider to create, update, or delete the resources you defined.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e4. Execution by Cloud Providers\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe cloud provider takes Terraform’s instructions and \u003cstrong\u003ebuilds your infrastructure\u003c/strong\u003e—creating things like virtual machines, networks, and storage based on your configuration.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e5. State Management\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform keeps track of everything it has created in a \u003cstrong\u003estate file\u003c/strong\u003e. This file helps Terraform know what’s already there, so it only makes \u003cstrong\u003enecessary changes\u003c/strong\u003e when you update your configuration.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eWhy Use Terraform?\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform makes infrastructure management \u003cstrong\u003esimpler, repeatable, and error-free\u003c/strong\u003e. Instead of manually clicking around in cloud dashboards, you can \u003cstrong\u003eautomate everything\u003c/strong\u003e with a few lines of code. This saves time and reduces mistakes.\u003c/p\u003e\u003cp\u003eTerraform enables you to safely and predictably create, change, and improve infrastructure. It is an open-source tool that codifies APIs into declarative configuration files that can be shared among team members, treated as code, edited, reviewed, and versioned.\u003c/p\u003e\u003cp\u003eIn this lab, you create a Terraform configuration with a module to automate the deployment of Google Cloud infrastructure.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eTask 1: Setting Up Terraform and Cloud Shell\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003eInstalling Terraform\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform is pre-installed in Cloud Shell. Verify the installed version.\u003c/p\u003e\u003cp\u003e1 Open \u003cstrong\u003eGoogle Cloud Console\u003c/strong\u003e and click \u003cstrong\u003eActivate Cloud Shell\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e2 If prompted, click \u003cstrong\u003eContinue\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e3 Run the following command to check the Terraform version:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eterraform --version\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Expected output:\u003c/strong\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eTerraform v1.3.3\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cblockquote\u003e\u003cstrong\u003eNote:\u003c/strong\u003e These lab instructions work with Terraform v1.3.3 and later.\u003c/blockquote\u003e\u003cp\u003e4 Create a directory for Terraform configurations:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003emkdir tfinfra\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e5 Open \u003cstrong\u003eCloud Shell Editor\u003c/strong\u003e and navigate to the \u003cstrong\u003e\u003cem\u003etfinfra \u003c/em\u003e\u003c/strong\u003efolder.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eInitializing Terraform\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eTerraform uses plugins to support various cloud providers. Initialize Terraform by setting Google as the provider.\u003c/p\u003e\u003cp\u003e1 Create a new file named \u003cstrong\u003eprovider.tf\u003c/strong\u003e tfinfra folder.\u003c/p\u003e\u003cp\u003e2 Add the following configuration:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eprovider \"google\" {}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e3 Save the file.\u003c/p\u003e\u003cp\u003e4 Run the Terraform initialization command:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecd tfinfra\nterraform init\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Expected output:\u003c/strong\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eprovider.google: version = \"~\u0026gt; 4.43.0\"\nTerraform has been successfully initialized!\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003e\u003cstrong\u003eTask 2: Creating mynetwork and Its Resources\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003eConfiguring mynetwork\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e1 Create a new file named \u003cstrong\u003emynetwork.tf\u003c/strong\u003e inside tfinfra.\u003c/p\u003e\u003cp\u003e2 Add the following configuration:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eresource \"google_compute_network\" \"mynetwork\" {\n  name                    = \"mynetwork\"\n  auto_create_subnetworks = true\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e3 Save the file.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eConfiguring Firewall Rules\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e4 Add the following firewall rules to mynetwork,tf:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eresource \"google_compute_firewall\" \"mynetwork-allow-http-ssh-rdp-icmp\" {\n  name    = \"mynetwork-allow-http-ssh-rdp-icmp\"\n  network = google_compute_network.mynetwork.self_link\n\n  allow {\n    protocol = \"tcp\"\n    ports    = [\"22\", \"80\", \"3389\"]\n  }\n  allow {\n    protocol = \"icmp\"\n  }\n  source_ranges = [\"0.0.0.0/0\"]\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e5 Save the file.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eConfiguring VM Instance\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e1 Create a new folder named \u003cstrong\u003einstance\u003c/strong\u003e inside tfinfra.\u003c/p\u003e\u003cp\u003e2 Create a new file \u003cstrong\u003emain.tf\u003c/strong\u003e inside the instance folder.\u003c/p\u003e\u003cp\u003e3 Add the following basic configuration:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eresource \"google_compute_instance\" \"vm_instance\" {\n  name         = \"my-vm-instance\"\n  machine_type = \"e2-medium\"\n  zone         = \"us-central1-a\"\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-10\"\n    }\n  }\n  network_interface {\n    network = google_compute_network.mynetwork.self_link\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e4 Save the file.\u003c/p\u003e\u003cp\u003eTo rewrite the Terraform configuration files to a canonical format and style, run the following command:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eterraform fmt\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eTo initialize Terraform, run the following command:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eterraform init\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eExpected output:\u003c/strong\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e...\n* provider.google: version = \"~\u0026gt; 4.43.0\"\n\nTerraform has been successfully initialized!\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eYou have successfully set up Terraform in Cloud Shell and created configurations to deploy Google Cloud infrastructure, including a VPC network, firewall rules, and VM instances. Terraform’s execution workflow ensures smooth infrastructure deployment with minimal manual intervention. This setup can be expanded with additional configurations and modules to efficiently automate more complex infrastructure deployments.\u003c/p\u003e\u003cp\u003eHappy learning and coding with Terraform!\u003c/p\u003e","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1739669992396_gcp-terraform.png","image_alt":"Terraform with GCP","slug":"Terraform-Labs-Automating-Google-Cloud-Infrastructure-Deployment","index":"6b86b273ff34f"},{"blog_id":"109a123a-02ae-4b9f-96a9-785428eef2fa","title":"Jenkins Unleashed: Transforming Your CI/CD Workflow for Lightning-Fast Delivery","short_description":"In the fast-paced world of modern software development, delivering high-quality applications quickly is no longer optional—it's essential. This is where Jenkins steps in as a game-changer. Imagine having a virtual assistant that tirelessly builds, tests, and deploys your code, ensuring every update you make reaches production seamlessly.","timestamp":"2025-01-06 12:32:13","description":"\u003ch1\u003e\u003cstrong\u003eWhy Use Jenkins for CI/CD?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eJenkins is not just a tool; it's a culture shifter. With its unparalleled flexibility, thousands of plugins, and vibrant community, Jenkins transforms how teams approach Continuous Integration and Continuous Delivery (CI/CD). Whether you’re a small startup racing to push your MVP or a large enterprise managing complex workflows, Jenkins empowers you to automate repetitive tasks, reduce errors, and accelerate delivery pipelines.\u003c/p\u003e\u003cp\u003eBut why Jenkins? It’s open-source, highly customizable, and scales effortlessly with your team’s growing needs. It’s the bridge between developers and operations teams, breaking down silos and fostering collaboration in ways you’ve never experienced before.\u003c/p\u003e\u003cp\u003eIn this labs, we’ll explore the magic of Jenkins and why it’s the go-to choice for CI/CD pipelines. Ready to revolutionize your development workflow? Let’s dive in!\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eSetting Up CI/CD in Jenkins\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo start setting up CI/CD with Jenkins, the first step is to prepare your environment by installing Java. Jenkins requires Java to run, so you need to install OpenJDK 17. Update your system packages and install Java by running the following commands:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install fontconfig openjdk-17-jre\njava -version\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter installation, you can verify the version of Java to ensure everything is properly set up. The output should display the installed version, such as openJDK version 17. With Java in place, your system is now ready to host Jenkins.\u003c/p\u003e\u003cp\u003eNext, install Jenkins using its official package repository to ensure you're getting the latest stable release. First, add Jenkins' repository key and configuration to your system, update the package list, and install Jenkins by running these commands:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo wget -O /usr/share/keyrings/jenkins-keyring.asc \\\n  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key\necho \"deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]\" \\\n  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\n  /etc/apt/sources.list.d/jenkins.list \u0026gt; /dev/null\nsudo apt-get update\nsudo apt-get install jenkins\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOnce Jenkins is installed, you need to enable and start the Jenkins service. This ensures Jenkins runs immediately and starts automatically whenever the system boots. Use the following commands to enable and start Jenkins:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl enable jenkins\nsudo systemctl start jenkins\nsudo systemctl status jenkins\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eAfter starting the service, you can access Jenkins through your browser. Open http://[VM_EXTERNAL_IP]:8080\u0026nbsp;and log in using the initial admin password. To retrieve this password, run:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo cat /var/lib/jenkins/secrets/initialAdminPassword\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEnter the password on the setup screen and follow the guided setup wizard. This process will include installing recommended plugins, such as Git, Pipeline, Blue Ocean, and GitHub, which are essential for building your CI/CD pipeline.\u003c/p\u003e\u003cp\u003eIn addition to plugins, configure credentials for accessing your source code repository. Navigate to \u003cstrong\u003eManage Jenkins \u0026gt; Credentials\u003c/strong\u003e, and add secure credentials for platforms like GitHub or GitLab. This setup ensures your pipeline can pull code from your repositories securely.\u003c/p\u003e\u003cp\u003eWith Jenkins configured, you’re ready to create your first pipeline. The pipeline script can define all stages of your CI/CD process, such as fetching code, running tests, and deploying to production.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eSetting Up Git Credentials for Private Repositories\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIf your project is hosted on a private Git platform, you’ll need to provide secure credentials for Jenkins to access the repository. For GitHub users, this involves creating a Personal Access Token (PAT) and adding it to Jenkins' credentials. This ensures seamless integration between Jenkins and your repository without compromising security.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eCreating a Personal Access Token (PAT) on GitHub\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo create a PAT in GitHub:\u003c/p\u003e\u003cp\u003e1. Go to your GitHub account settings.\u003c/p\u003e\u003cp\u003e2. Navigate to Developer settings \u0026gt; Personal Access Tokens \u0026gt; Tokens (classic).\u003c/p\u003e\u003cp\u003e3. Click Generate new token and specify the required permissions. For most CI/CD setups, select scopes like repo (for repository access) and workflow (if managing GitHub Actions).\u003c/p\u003e\u003cp\u003e4. Generate the token and copy it. Make sure to store it securely as you won’t be able to view it again.\u003c/p\u003e\u003ch2\u003eConfiguring Jenkins with GitHub Credentials\u003c/h2\u003e\u003cp\u003eOnce you’ve created your PAT, follow these steps to add it to Jenkins:\u003c/p\u003e\u003cp\u003e1. Log in to the Jenkins Dashboard.\u003c/p\u003e\u003cp\u003e2. Navigate to Manage Jenkins \u0026gt; Manage Credentials.\u003c/p\u003e\u003cp\u003e3. Under (global) credentials, click Add Credentials.\u003c/p\u003e\u003cp\u003e4. In the Kind dropdown, select Username with password.\u003c/p\u003e\u003cp\u003eUsername: Enter your GitHub username (e.g., username).\u003c/p\u003e\u003cp\u003ePassword: Paste the PAT you just created.\u003c/p\u003e\u003cp\u003e5. Give the credential a recognizable ID (e.g., github-creds) and click OK.\u003c/p\u003e\u003ch1\u003eWriting Your First Jenkins Pipeline\u003c/h1\u003e\u003cp\u003eWith Jenkins configured, you’re ready to create your first pipeline. The pipeline script defines all stages of your CI/CD process, including fetching code, running tests, and deploying the application. Let’s walk through an example pipeline written using Jenkins' declarative syntax, which simplifies the process and ensures better readability.\u003c/p\u003e\u003ch4\u003eUnderstanding the Pipeline Script\u003c/h4\u003e\u003cp\u003eBelow is an example pipeline that automates three key stages of the CI/CD process:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Fetching the code from GitHub\u003c/strong\u003e using credentials for a secure connection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Running basic testing commands\u003c/strong\u003e to verify the environment setup.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Deploying the application\u003c/strong\u003e to a server using SSH for remote commands\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003epipeline {\n\u0026nbsp; \u0026nbsp; agent any\n\u0026nbsp; \u0026nbsp;\u0026nbsp;\n\u0026nbsp; \u0026nbsp; environment {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; GITHUB_CREDENTIALS = 'caea020d-a24e-4305-bdc2-d7e51d1c8171'\u0026nbsp; // ID of the GitHub credentials in Jenkins\n\u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp;\u0026nbsp;\n\u0026nbsp; \u0026nbsp; stages {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; stage('Git Checkout') {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; steps {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; script {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Cloning the GitHub repository using the provided credentials\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; git credentialsId: \"${GITHUB_CREDENTIALS}\", url: 'https://github.com/Barbarpotato/API-Registry.git', branch: 'main'\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;\u0026nbsp;\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; stage('Run Testing Commands') {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; steps {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; sh 'hostname'\u0026nbsp; // Outputs the hostname of the Jenkins agent\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; sh 'pwd'\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;// Displays the current working directory in the agent's workspace\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;\u0026nbsp;\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; stage('Deploy to Server') {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; steps {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Using SSH to deploy the application to the target server\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; sshagent(['ssh-key-gateway']) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; sh '''\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ssh -o StrictHostKeyChecking=no darmawanjr88@34.101.205.217 \u0026lt;\u0026lt; EOF\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; cd API-Registry\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; git pull origin main\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; pm2 restart all\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; exit\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; EOF\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; '''\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe provided pipeline script is a clear example of how Jenkins orchestrates the CI/CD process, broken into multiple stages for ease of management. Let's break it down section by section and explain how each part works in a simplified, interactive way.\u003c/p\u003e\u003ch3\u003ePipeline Declaration\u003c/h3\u003e\u003cp\u003eAt the heart of any Jenkins pipeline is the pipeline block, which defines the entire process. Inside this block, the agent any directive tells Jenkins to run the pipeline on any available agent, whether it's a master or a worker node. This flexibility is useful when you have multiple agents configured and don’t want to restrict the execution to a specific one.\u003c/p\u003e\u003ch3\u003eEnvironment Variables\u003c/h3\u003e\u003cp\u003eNext, we have the environment block. This is where we can define variables that are reused across the pipeline. In this case, the variable GITHUB_CREDENTIALS holds the ID of the GitHub credentials that Jenkins uses to securely access the private repository. By defining it here, you ensure it’s easily reusable without hardcoding it into every step. Think of it as a centralized way to manage sensitive data like tokens and credentials, making the pipeline both secure and maintainable.\u003c/p\u003e\u003ch3\u003eStages\u003c/h3\u003e\u003cp\u003eThe pipeline is broken into logical steps called \"stages,\" each representing a part of the CI/CD workflow.\u003c/p\u003e\u003cp\u003e1. Git Checkout\u003c/p\u003e\u003cp\u003eIn the first stage, Jenkins clones the GitHub repository using the git step. The credentialsId points to the pre-configured GitHub credentials stored in Jenkins. This ensures secure and seamless access to the private repository without exposing sensitive information. This step lays the foundation for the entire pipeline, as it fetches the code that the remaining stages will process.\u003c/p\u003e\u003cp\u003e2. Run Testing Commands\u003c/p\u003e\u003cp\u003eThis stage is simple but powerful. It runs shell commands such as hostname and pwd, which output the hostname of the Jenkins agent and the current working directory, respectively. While these commands are placeholders here, you can replace them with actual test scripts. For instance, if you’re running unit tests, you could include a command like npm test or pytest. The purpose of this stage is to ensure the environment is configured correctly and ready for further operations.\u003c/p\u003e\u003cp\u003e3. Deploy to Server\u003c/p\u003e\u003cp\u003eThis stage demonstrates how Jenkins can deploy your application to a production or staging server. Using the sshagent block, Jenkins securely connects to the target server via SSH. It then pulls the latest changes from the repository and restarts the application using PM2, a popular process manager for Node.js. This setup ensures that the application is always up-to-date with the latest code changes, and the restart command ensures a smooth rollout of updates.\u003c/p\u003e\u003cp\u003eContinuing from the previous explanation of setting up a Jenkins pipeline, the next step to truly automate the CI/CD process is to set up a \u003cstrong\u003epush trigger\u003c/strong\u003e using webhooks. This ensures that every time you push changes to your GitHub repository, Jenkins automatically triggers the pipeline, saving you from the hassle of manually starting the build.\u003c/p\u003e\u003cp\u003eLet’s explore how to set up a webhook-based trigger between GitHub and Jenkins in an intuitive and straightforward way.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eWhat Are Webhooks?\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThink of webhooks as a way for GitHub to \"talk\" to Jenkins. Whenever you push code to your repository, GitHub sends a signal (HTTP POST request) to Jenkins, telling it to start the pipeline. This creates an automated, real-time link between your code changes and the build process.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eSetting Up Push Trigger (Webhook)\u003c/strong\u003e\u003c/h3\u003e\u003ch4\u003e\u003cstrong\u003eFirst, Enable GitHub Integration in Jenkins\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eBefore setting up the webhook, you need to make sure Jenkins can communicate with GitHub. To do this, open Jenkins and go to \u003cstrong\u003eManage Jenkins\u003c/strong\u003e \u0026gt; \u003cstrong\u003eManage Plugins\u003c/strong\u003e. From there, search for \u003cstrong\u003eGitHub\u003c/strong\u003e plugins and install them. This will allow Jenkins to recognize GitHub as a source and receive notifications from it.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eConfigure Jenkins to Listen for Webhooks\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eNext, open the Jenkins pipeline job you want to configure. In the job settings, go to \u003cstrong\u003eBuild Triggers\u003c/strong\u003e and enable the option \u003cstrong\u003eGitHub hook trigger for GITScm polling\u003c/strong\u003e. This step is important because it tells Jenkins to \"listen\" for any push events from GitHub, ready to trigger the pipeline whenever changes are detected.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eSet Up a Webhook in GitHub\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eNow, head over to your GitHub repository. Inside the \u003cstrong\u003eSettings\u003c/strong\u003e tab, navigate to \u003cstrong\u003eWebhooks\u003c/strong\u003e and click on \u003cstrong\u003eAdd webhook\u003c/strong\u003e. You'll need to provide Jenkins with a specific endpoint where it can receive notifications from GitHub. The URL format is as follows:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ehttp://\u0026lt;JENKINS_URL\u0026gt;/github-webhook/\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eReplace \u003cstrong\u003e\u003cem\u003eJENKINS_URL \u003c/em\u003e\u003c/strong\u003ewith your Jenkins server’s address. Choose \u003cstrong\u003eapplication/json\u003c/strong\u003e as the content type, and make sure the \u003cstrong\u003ePush events\u003c/strong\u003e option is selected. This ensures that the webhook triggers every time you push changes.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eVerify the Webhook\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eOnce everything is set up, push a commit to your GitHub repository. Then, return to Jenkins and check if the pipeline starts running automatically. If the job kicks off, the webhook is working properly.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eHow It All Comes Together\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhen you push changes to your repository, GitHub sends a webhook to Jenkins. Jenkins then triggers the pipeline to fetch the latest code, run tests, and deploy to your server. This creates a seamless CI/CD process, where every change is automatically tested and deployed.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn this Labs, we’ve taken a deep dive into how Jenkins can supercharge your CI/CD workflows, making it an essential tool for automating your development lifecycle. We started by setting up Jenkins, installing necessary dependencies like Java, and getting the Jenkins service up and running.\u003c/p\u003e\u003cp\u003eFrom there, we explored how to configure Jenkins to work with your GitHub repository, including managing credentials securely. With Jenkins set up and connected to GitHub, we moved on to creating a simple declarative pipeline, allowing Jenkins to automatically fetch the latest code, run tests, and deploy to your server.\u003c/p\u003e\u003cp\u003eWe then enhanced the process by explaining how to set up push triggers using GitHub webhooks. With webhooks in place, Jenkins is able to automatically start the pipeline whenever new code is pushed to the repository, eliminating the need for manual intervention and ensuring continuous integration.\u003c/p\u003e\u003cp\u003eThrough these steps, we’ve created a fully automated CI/CD pipeline that reacts to changes in your code, testing it and deploying it seamlessly. This not only saves time but also minimizes errors, providing faster and more reliable software delivery.\u003c/p\u003e\u003cp\u003eBy mastering Jenkins, you’re empowering yourself to automate complex workflows, improve collaboration, and focus on building great software without worrying about the manual process of integration and deployment.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fjenkins_background.png?alt=media\u0026token=8d8f21c7-f6bd-4157-8343-12090e88d13a","image_alt":"Jenkins Intro image","slug":"Jenkins-Unleashed-Transforming-Your-CICD-Workflow-for-Lightning-Fast-Delivery","index":"6b86b273ff34f"},{"blog_id":"f24fa0d0-6b50-494c-ab4d-49d1b302359f","title":"Building a Robust Microservices Architecture: From gRPC to Kubernetes","short_description":"In the ever-evolving world of software architecture, building a robust and scalable system is key to meeting the demands of modern applications. Recently, I had the opportunity to explore a powerful combination of technologies, starting with gRPC, translating it into HTTP/1.1, and finally deploying the system to a Kubernetes cluster. In this blog, I will take you through the journey, share the challenges I encountered, and explain why each of these steps is important for modern software systems.","timestamp":"2024-12-26 07:34:04","description":"\u003ch2\u003e\u003cstrong\u003eWhy gRPC? Understanding Its Power in Microservices Communication\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs we move toward building distributed systems, one of the key challenges is communication between services. In a typical microservices setup, services need to talk to each other to exchange data and process requests. The two most common approaches for communication are \u003cstrong\u003eRESTful APIs\u003c/strong\u003e and \u003cstrong\u003egRPC\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eSo, why did I choose \u003cstrong\u003egRPC\u003c/strong\u003e for this project?\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhat Is gRPC?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003egRPC (Google Remote Procedure Call) is a high-performance, language-agnostic framework for communication between services. Unlike REST, which relies on text-based protocols (typically JSON over HTTP/1.1), gRPC uses \u003cstrong\u003eProtocol Buffers (protobuf)\u003c/strong\u003e for serialization. This binary protocol is compact, efficient, and designed for high-performance communication, making it ideal for microservices that require fast and reliable data exchange.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhy Use gRPC for Microservices?\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFaster Communication\u003c/strong\u003e: gRPC’s binary protocol is more efficient than text-based protocols, reducing the overhead of parsing and serialization.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCross-Language Support\u003c/strong\u003e: With gRPC, you can define your service in a language-neutral way and implement it in any language that supports gRPC (like Go, Java, Python, and more).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBidirectional Streaming\u003c/strong\u003e: gRPC supports streaming, which makes it a great choice for real-time communication between services.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHowever, as much as gRPC is great for communication between services, it’s not as widely supported by clients as HTTP/1.1, especially for web applications. This brings us to the next step in the process: \u003cstrong\u003etranslating gRPC to HTTP/1.1\u003c/strong\u003e for broader client support.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eTranslating gRPC to HTTP/1.1 for Client Communication\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWhile gRPC is fantastic for internal microservice communication, not all clients can directly communicate with gRPC servers. HTTP/1.1 is still the standard protocol for the majority of web browsers and external client requests. Therefore, I needed to expose the gRPC services through an \u003cstrong\u003eAPI Gateway\u003c/strong\u003e, which would translate incoming HTTP/1.1 requests into gRPC calls.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eWhy an API Gateway?\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eAn \u003cstrong\u003eAPI Gateway\u003c/strong\u003e acts as a reverse proxy that forwards client requests to the appropriate service, handling routing, load balancing, and security concerns. In my case, it also handled translating HTTP requests into gRPC communication.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHTTP to gRPC Translation\u003c/strong\u003e: The API Gateway receives HTTP requests from clients, translates them into gRPC requests, and then forwards them to the respective service. This allows you to expose gRPC services to HTTP clients without changing the core functionality of your services.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCentralized Control\u003c/strong\u003e: The API Gateway helps manage cross-cutting concerns like authentication, authorization, rate limiting, and logging, centralizing these tasks for easier management.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eWhat we are going to build for the sample?\u003c/h2\u003e\u003cp\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc-kubernetes-arch-project.png?alt=media\u0026amp;token=0096deb5-89bb-4cb5-893f-e75a610e13dc\" alt=\"kubernetes+grpc architecture plan\" width=\"720px\"\u003eThe project starts with the need to create a modern, scalable, and efficient architecture for handling multiple services. You aim to build a system where microservices can seamlessly communicate using gRPC for high-performance, low-latency interactions. The choice of gRPC over traditional HTTP APIs stems from its ability to use Protocol Buffers, enabling efficient serialization, lightweight message exchanges, and bi-directional streaming if needed. This makes it ideal for services like login authentication and movie information retrieval, which require quick and reliable data exchange.\u003c/p\u003e\u003cp\u003eIn this project, you plan to create two gRPC services: one for handling user login (grpc-login-service) and another for managing movie-related data (grpc-movie-service). However, since most clients, like web browsers, communicate using HTTP 1.1 or HTTP/2, you introduce an API Gateway as the bridge. This gateway translates HTTP requests into gRPC calls, acting as the central entry point for all client communications. The gateway enables a smoother experience for clients while maintaining the performance benefits of gRPC in the backend.\u003c/p\u003e\u003cp\u003eTo host and scale these services, you decide to deploy them on a Kubernetes cluster. Kubernetes provides a robust platform for container orchestration, ensuring that your services are resilient, scalable, and highly available. Your deployment plan includes pushing your service artifacts (Docker images) to the Google Cloud Artifact Registry and then using a 3-node Kubernetes cluster to deploy these services. Each gRPC service and the API Gateway are containerized and defined using Kubernetes YAML files for deployment and service management.\u003c/p\u003e\u003cp\u003eThis architecture ensures a clean separation of concerns: each service focuses on its domain logic (e.g., login or movies), while the gateway abstracts communication complexities for the clients. It also leverages Kubernetes to provide automated scaling, load balancing, and fault tolerance, making the system ready for production-level traffic.\u003c/p\u003e\u003ch2\u003eDive to API Gateway\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst express = require('express');\nconst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\nconst grpcWeb = require('grpc-web');\n\n// Load .proto files\nconst loginPackageDefinition = protoLoader.loadSync('login-service.proto', {\n\u0026nbsp; \u0026nbsp; keepCase: true,\n\u0026nbsp; \u0026nbsp; longs: String,\n\u0026nbsp; \u0026nbsp; enums: String,\n\u0026nbsp; \u0026nbsp; defaults: true,\n\u0026nbsp; \u0026nbsp; oneofs: true,\n});\n\nconst moviePackageDefinition = protoLoader.loadSync('movie-service.proto', {\n\u0026nbsp; \u0026nbsp; keepCase: true,\n\u0026nbsp; \u0026nbsp; longs: String,\n\u0026nbsp; \u0026nbsp; enums: String,\n\u0026nbsp; \u0026nbsp; defaults: true,\n\u0026nbsp; \u0026nbsp; oneofs: true,\n});\n\nconst loginServiceProto = grpc.loadPackageDefinition(loginPackageDefinition).LoginServiceProto;\nconst movieServiceProto = grpc.loadPackageDefinition(moviePackageDefinition).MovieServiceProto;\n\nconst app = express();\nconst port = 3000;\n\n// Set up your gRPC client\nconst login_client = new loginServiceProto('grpc-login-service:80', grpc.credentials.createInsecure());\nconst movie_client = new movieServiceProto('grpc-movie-service:80', grpc.credentials.createInsecure());\n\n// Middleware to parse JSON bodies\napp.use(express.json());\n\n// API endpoint to proxy to the gRPC service\napp.post('/api/login', (req, res) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; const { username, password } = req.body;\n\n\u0026nbsp; \u0026nbsp; // Call the gRPC method\n\u0026nbsp; \u0026nbsp; login_client.LoginMethod({ username, password }, (err, response) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; if (err) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('gRPC error:', err);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; return res.status(500).send({ error: 'Internal Server Error' });\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.json(response);\n\u0026nbsp; \u0026nbsp; });\n});\n\napp.post('/api/movie', (req, res) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; const { title, description, rating } = req.body;\n\u0026nbsp; \u0026nbsp; // Call the gRPC method\n\u0026nbsp; \u0026nbsp; movie_client.MovieMethod({ title, description, rating }, (err, response) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; if (err) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('gRPC error:', err);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; return res.status(500).send({ error: 'Internal Server Error' });\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.json(response);\n\u0026nbsp; \u0026nbsp; });\n});\n\n// Start the API Gateway server\napp.listen(port, () =\u0026gt; {\n\u0026nbsp; \u0026nbsp; console.log(`API Gateway running at http://localhost:${port}`);\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code sets up a simple gRPC server for handling login requests. It begins by importing the necessary modules: @grpc/grpc-js, which provides the core gRPC functionalities, and @grpc/proto-loader, which is used to parse .proto files containing service definitions. The .proto file, login-service.proto, is loaded and configured using the protoLoader.loadSync method. This configuration ensures that field names are preserved, and specific Protocol Buffers types such as long integers, enumerations, default values, and oneof fields are appropriately handled. The loaded package definition is then passed to grpc.loadPackageDefinition to generate a usable gRPC object, which represents the LoginServiceProto service.\u003c/p\u003e\u003cp\u003eNext, the LoginMethod function is implemented to handle incoming login requests. This function receives the call object, which contains the client’s request data, and a callback function to send a response back to the client. It extracts the username and password from the request, constructs a success message, and sends a structured response containing the message and a status field via the callback. The response indicates that the login was processed successfully.\u003c/p\u003e\u003cp\u003eThe gRPC server is then created using new grpc.Server(). The addService method registers the LoginServiceProto with the server and links it to the LoginMethod implementation. Finally, the server is started by binding it to the address 0.0.0.0 on port 50052 using the bindAsync method. For simplicity, the server uses insecure credentials, making it suitable for local testing but not for production. Once the server is running, a confirmation message is logged to indicate its readiness to handle incoming requests. Overall, this code provides a robust foundation for handling login operations as part of a microservices-based architecture. It enables fast and efficient communication between services through the gRPC protocol.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eDive to GRPC Service\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\n\n\n// Load the .proto file\nconst packageDefinition = protoLoader.loadSync('movie-service.proto', {\n\u0026nbsp; \u0026nbsp; keepCase: true,\n\u0026nbsp; \u0026nbsp; longs: String,\n\u0026nbsp; \u0026nbsp; enums: String,\n\u0026nbsp; \u0026nbsp; defaults: true,\n\u0026nbsp; \u0026nbsp; oneofs: true,\n});\nconst MovieServiceProto = grpc.loadPackageDefinition(packageDefinition).MovieServiceProto;\n\n\n// Implement the ExampleMethod RPC\nfunction MovieMethod(call, callback) {\n\u0026nbsp; \u0026nbsp; const { title, description, rating } = call.request;\n\u0026nbsp; \u0026nbsp; const message = `Thank you for the feedback. The Movie Title is ${title} and the description is ${description} has a rating of ${rating}.`;\n\u0026nbsp; \u0026nbsp; const response = {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; message: message,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; status: 'success',\n\u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; callback(null, response);\n}\n\n\n// Create the gRPC server\nconst server = new grpc.Server();\nserver.addService(MovieServiceProto.service, { MovieMethod: MovieMethod });\n\n\n// Start the server\nconst port = '0.0.0.0:50051';\nserver.bindAsync(port, grpc.ServerCredentials.createInsecure(), () =\u0026gt; {\n\u0026nbsp; \u0026nbsp; console.log(`gRPC server running at ${port}`);\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code defines a gRPC server for handling requests related to movies. The server is built using the @grpc/grpc-js library for gRPC functionality and the @grpc/proto-loader library to load the .proto file that defines the movie service. Here's how it works in detail:\u003c/p\u003e\u003cp\u003eFirst, the .proto file (movie-service.proto) is loaded using protoLoader.loadSync. The protoLoader processes the Protocol Buffers definition into a format compatible with gRPC in Node.js. The options provided during the load process, such as keepCase, longs, and defaults, ensure that the original structure of the Protocol Buffers definition is preserved and mapped appropriately in JavaScript. The loaded package definition is then used to retrieve the MovieServiceProto object, which represents the service described in the .proto file.\u003c/p\u003e\u003cp\u003eNext, the MovieMethod function is implemented as the core logic for handling the gRPC request. This method represents an RPC (Remote Procedure Call) defined in the .proto file. When a client sends a request to this method, it provides details about a movie, such as title, description, and rating. The server responds by constructing a message that acknowledges the feedback and includes the provided details. A response object is created, which contains a message and a success status, and this is sent back to the client using the callback function.\u003c/p\u003e\u003cp\u003eThe gRPC server is then created using new grpc.Server(). The addService method registers the MovieServiceProto.service with the server and maps it to the implementation (MovieMethod). This ensures that whenever a request for the MovieMethod RPC is received, the defined function is executed.\u003c/p\u003e\u003cp\u003eFinally, the server is started on port 50051 using the bindAsync method. The grpc.ServerCredentials.createInsecure() method specifies that the server will run without encryption, suitable for development environments. Once the server is bound to the specified port, it logs a confirmation message to the console, indicating that the gRPC server is running and ready to handle requests.\u003c/p\u003e\u003cp\u003eThis setup is crucial for your project, as it provides the backend logic for handling movie-related data. It showcases how gRPC enables structured communication between services while maintaining high performance and clear data contracts defined in the .proto file.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: There is another grpc service that implemented. and its excluding in this topic. You can create more similiar grpc service like above code.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eCreating Dockerfile\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo containerize your gRPC service and push it to Google Artifact Registry, you'll use a Dockerfile to define the container's build process and interact with the Artifact Registry to store your container image. Here's how you can approach this process:\u003c/p\u003e\u003cp\u003eThe Dockerfile provides instructions for building the container image for your gRPC service. Below is an example Dockerfile for the movie-service:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e# Use the official Node.js image as a base\nFROM node:16-alpine\n\n\n# Set the working directory in the container\nWORKDIR /usr/src/app\n\n\n# Copy the package.json and package-lock.json\nCOPY package*.json ./\n\n\n# Install dependencies\nRUN npm install\n\n\n# Copy the application code to the container\nCOPY . .\n\n\n# Expose the port the gRPC server will run on\nEXPOSE 50051\n\n\n# Start the gRPC service\nCMD [\"node\", \"index.js\"]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eExplanation\u003c/strong\u003e:\u003c/p\u003e\u003cp\u003e1. Base Image: We use the lightweight node:16-alpine image to reduce the container size while still providing all necessary Node.js dependencies.\u003c/p\u003e\u003cp\u003e2. Working Directory: The WORKDIR sets the container's working directory to /usr/src/app.\u003c/p\u003e\u003cp\u003e3. Copying Files: The COPY instructions add your package.json and the rest of your application files to the container.\u003c/p\u003e\u003cp\u003e4. Installing Dependencies: The RUN npm install command ensures that all required dependencies are installed.\u003c/p\u003e\u003cp\u003e5. Exposing Port: The EXPOSE 50051 makes the gRPC service accessible on port 50051.\u003c/p\u003e\u003cp\u003e6. Starting the Service: The CMD defines the command to run your gRPC service when the container starts.\u003c/p\u003e\u003cp\u003e7. Building the Docker Image\u003c/p\u003e\u003cp\u003eAfter creating the Dockerfile, you can build the Docker image for your gRPC service using the following command:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003edocker build -t movie-service .\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis command tags the image as movie-service and uses the current directory (.) as the build context.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: This Dockerfile only represent single project which refers to one of grpc service. You can create another Dockerfile for another service. which not really different with this one.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eTagging and Pushing to Artifact Registry\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis approach allowed Kubernetes (GKE) to seamlessly retrieve and deploy the correct images for each service, ensuring a smooth and reliable deployment process.\u003c/p\u003e\u003cp\u003eTo push the image to Google Artifact Registry, follow these steps:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Authenticate with Google Cloud\u003c/strong\u003e: Run the following command to configure Docker to use Google Cloud credentials:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003egcloud auth configure-docker\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Tag the Image for Artifact Registry\u003c/strong\u003e: Replace \u0026lt;region\u0026gt;, \u0026lt;project-id\u0026gt;, and \u0026lt;repository-name\u0026gt; with your Artifact Registry details. For example:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003edocker tag movie-service \u0026lt;region\u0026gt;-docker.pkg.dev/\u0026lt;project-id\u0026gt;/\u0026lt;repository-name\u0026gt;/movie-service\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Push the Image\u003c/strong\u003e: Push the tagged image to the Artifact Registry:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003edocker push \u0026lt;region\u0026gt;-docker.pkg.dev/\u0026lt;project-id\u0026gt;/\u0026lt;repository-name\u0026gt;/movie-service\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2\u003e\u003cstrong\u003eKubernetes Setup\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo deploy your gRPC services to Kubernetes on Google Kubernetes Engine (GKE), you'll go through several steps, including creating the GKE cluster, setting up YAML files for deployments and services, and deploying the services. Here's a step-by-step explanation:\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e1. Creating a Kubernetes Cluster in GKE\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFirst, you'll create a GKE cluster to host your services.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eStep 1: Enable Required APIs\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eEnsure you have the GKE and Artifact Registry APIs enabled for your project:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003egcloud services enable container.googleapis.com artifactregistry.googleapis.com\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong\u003eStep 2: Create the GKE Cluster\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRun the following command to create a Kubernetes cluster:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003egcloud container clusters create grpc-cluster \\\n    --num-nodes=3 \\\n    --region=us-central1 \\\n    --enable-ip-alias\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e--num-nodes=3: Creates a cluster with 3 nodes.\u003c/p\u003e\u003cp\u003e--region=us-central1: Specifies the cluster region. Adjust this based on your location.\u003c/p\u003e\u003cp\u003e--enable-ip-alias: Enables VPC-native networking.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eStep 3: Connect to the Cluster\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eTo interact with your GKE cluster, fetch its credentials:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003egcloud container clusters get-credentials grpc-cluster --region=us-central1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNow, your local kubectl command is connected to your GKE cluster.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e2. Preparing Kubernetes YAML Files\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eYou need two types of YAML files for each service: a Deployment file and a Service file.\u003c/p\u003e\u003cp\u003eExample: Deployment YAML (movie-service-deployment.yaml)\u003c/p\u003e\u003cp\u003eThis file defines how your gRPC service is deployed.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: movie-service\n  labels:\n    app: movie-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: movie-service\n  template:\n    metadata:\n      labels:\n        app: movie-service\n    spec:\n      containers:\n      - name: movie-service\n        image: us-central1-docker.pkg.dev/\u0026lt;project-id\u0026gt;/\u0026lt;repository-name\u0026gt;/movie-service:latest\n        ports:\n        - containerPort: 50051\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eExplanation:\u003c/p\u003e\u003cp\u003e- replicas: Specifies the number of pod instances for the service.\u003c/p\u003e\u003cp\u003e- image: Points to the Docker image in Google Artifact Registry.\u003c/p\u003e\u003cp\u003e- ports: Exposes port 50051, which the gRPC service listens to.\u003c/p\u003e\u003cp\u003eExample: Service YAML (movie-service-service.yaml)\u003c/p\u003e\u003cp\u003eThis file exposes your gRPC service within the cluster or to the internet.\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eapiVersion: v1\nkind: Service\nmetadata:\n  name: movie-service\nspec:\n  selector:\n    app: movie-service\n  ports:\n  - protocol: TCP\n    port: 50051\n    targetPort: 50051\n  type: ClusterIP\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003eExplanation:\u003c/h4\u003e\u003cp\u003e\u003cstrong\u003etype\u003c/strong\u003e: ClusterIP: Exposes the service within the Kubernetes cluster. Use LoadBalancer if you need external access.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003etargetPort\u003c/strong\u003e: Maps the service port to the container port.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e3. Deploying the Services\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAfter creating the YAML files, apply them to your cluster.\u003c/p\u003e\u003ch4\u003e\u003cstrong\u003eStep 1: Deploy the Movie Service\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eRun the following commands:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ekubectl apply -f movie-service-deployment.yaml\nkubectl apply -f movie-service-service.yaml\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong\u003eStep 2: Verify the Deployment\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eCheck the pods and services to ensure they are running:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ekubectl get pods\nkubectl get svc\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cstrong\u003e4. Deploying Additional Services\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFollow the same process for your login service:\u003c/p\u003e\u003cp\u003e1. Create login-service-deployment.yaml and login-service-service.yaml.\u003c/p\u003e\u003cp\u003e2. Apply the YAML files using kubectl apply.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eConlusion\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eBuilding a modern, scalable system is more than just deploying code—it's a journey into the intricacies of microservices, networking, containerization, and orchestration. From the outset, we delved into the necessity of gRPC, unlocking fast, efficient communication between services. By creating two gRPC services. we saw how to design robust servers capable of processing structured requests and returning meaningful responses. Translating these gRPC methods into HTTP/1.1 via an API Gateway expanded their accessibility, making them usable by any client.\u003c/p\u003e\u003cp\u003eDocker came into play as the backbone of portability and consistency, enabling us to containerize and push our services to Google Artifact Registry. With Kubernetes, we embraced the power of orchestration, deploying services on a GKE cluster, ensuring high availability, load balancing, and seamless scaling. YAML files gave us control over deployments, allowing precise management of replicas, ports, and service types. The API Gateway tied everything together, creating a single point of entry for clients while efficiently routing traffic to the respective gRPC services.\u003c/p\u003e\u003cp\u003eThis project showcases not just technical implementations but also the thought process behind building scalable, maintainable systems. Each step—from writing the first line of code to testing the final deployment—demonstrates the power of modern tools and practices. It's a reminder of how containerization, orchestration, and thoughtful design transform complex challenges into elegant solutions.\u003c/p\u003e\u003cp\u003eThe journey doesn’t stop here. With this foundation, the possibilities are limitless—whether adding more services, optimizing deployments, or exploring advanced Kubernetes features like autoscaling and monitoring. This project serves as a testament to the potential of modern cloud-native development and a stepping stone for future innovation.\u003c/p\u003e\u003cp\u003eSo here’s to embracing complexity, simplifying solutions, and building systems that not only work but inspire. The world of scalable systems awaits—where will you take it next?\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc%20with%20kubernetes.png?alt=media\u0026token=ae61e7f8-2088-416f-924c-512461e18206","image_alt":"Kubernetes+GRPC Background","slug":"Building-a-Robust-Microservices-Architecture-From-gRPC-to-Kubernetes","index":"6b86b273ff34f"},{"blog_id":"3b169c47-8359-4743-9dd2-eebbb68e0c52","title":"Building a Video Streaming Platform with AWS S3, HLS, and Node.js","short_description":"Ever wondered how your favorite streaming platforms deliver smooth, high-quality videos? Streaming video content is a cornerstone of modern web applications. Let’s explore how to build a video streaming service step by step.","timestamp":"2024-12-15 10:42:44","description":"\u003cp\u003eBy the end of this guide, you'll have the skills to:\u003c/p\u003e\u003cp\u003e- Store video chunks securely in AWS S3.\u003c/p\u003e\u003cp\u003e- Stream videos dynamically to users using HLS.\u003c/p\u003e\u003cp\u003e- Enable users to upload videos, automatically process them into HLS chunks, and store them in S3.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eReady to get started? Let's dive in!\u003c/strong\u003e\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eStep 1: Storing Video Chunks in AWS S3\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo stream videos, we first need to upload .ts (MPEG transport stream) files and a .m3u8 playlist to an AWS S3 bucket. Follow these steps to upload your files:\u003c/p\u003e\u003ch3\u003eWhy AWS S3?\u003c/h3\u003e\u003cp\u003eAWS S3 provides a scalable, reliable, and secure storage solution, making it perfect for handling large video files. Using the AWS CLI simplifies the upload process and integrates easily into automation pipelines.\u003c/p\u003e\u003ch3\u003eStep-by-Step Instructions:\u003c/h3\u003e\u003cp\u003e1. Open your terminal.\u003c/p\u003e\u003cp\u003e2. Run the following commands to upload the video chunks and playlist:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eaws s3 cp output0.ts s3://your-bucket-name/path-to-folder/\naws s3 cp playlist.m3u8 s3://your-bucket-name/path-to-folder/\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhile we’re using the AWS CLI here, you could also use the AWS Management Console or SDKs (e.g., AWS SDK for JavaScript) if you prefer a graphical interface or code-based interaction.\u003c/p\u003e\u003cp\u003eOnce uploaded, your files will be securely stored and accessible for streaming. Let’s move to setting up the backend!\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eStep 2: Streaming the Video with Express.js\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eNow that our video chunks are in S3, we need a backend to fetch and serve them to the client. We’ll use Node.js with Express.js to handle this.\u003c/p\u003e\u003ch3\u003eBackend Setup:\u003c/h3\u003e\u003cp\u003e1. Create a new Node.js project and install the necessary packages:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enpm init -y\nnpm install express @aws-sdk/client-s3\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e2. Add the following code to your server.js file: \u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport express from 'express';\nimport { S3Client, GetObjectCommand } from '@aws-sdk/client-s3';\nimport stream from 'stream';\n\nconst app = express();\nconst s3 = new S3Client({ region: 'your-region' });\n\n// Endpoint to serve the HLS playlist\napp.get('/play/playlist.m3u8', async (req, res) =\u0026gt; {\n\u0026nbsp;\u0026nbsp;try {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const command = new GetObjectCommand({\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Bucket: 'your-bucket-name',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Key: 'path-to-folder/playlist.m3u8',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;});\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const s3Response = await s3.send(command);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;s3Response.Body.pipe(res);\n\u0026nbsp;\u0026nbsp;} catch (err) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.error('Error fetching playlist:', err);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;res.status(500).send('Error fetching playlist');\n\u0026nbsp;\u0026nbsp;}\n});\n\n// Endpoint to serve video chunks\napp.get('/play/video/:segment', async (req, res) =\u0026gt; {\n\u0026nbsp;\u0026nbsp;try {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const command = new GetObjectCommand({\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Bucket: 'your-bucket-name',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Key: `path-to-folder/${req.params.segment}`,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;});\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const s3Response = await s3.send(command);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;s3Response.Body.pipe(res);\n\u0026nbsp;\u0026nbsp;} catch (err) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.error('Error fetching video segment:', err);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;res.status(500).send('Error fetching video segment');\n\u0026nbsp;\u0026nbsp;}\n});\n\nconst PORT = 3000;\napp.listen(PORT, () =\u0026gt; {\n\u0026nbsp;\u0026nbsp;console.log(`Server is running on http://localhost:${PORT}`);\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003eWhat’s Happening Here?\u003c/h3\u003e\u003cp\u003e\u003cstrong\u003e- Playlist Endpoint\u003c/strong\u003e: Fetches and streams the .m3u8 file from S3.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Video Chunks Endpoint\u003c/strong\u003e: Dynamically fetches .ts chunks based on the client’s request.\u003c/p\u003e\u003cp\u003eOnce the backend is running, we can serve video content to the client. Let’s bring it all together on the frontend.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eStep 3: Playing the Video on the Client-Side\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo play the HLS video stream in the browser, we’ll use the \u003ccode\u003e\u0026lt;video\u0026gt;\u003c/code\u003e HTML tag and the HLS.js library for compatibility with all modern browsers.\u003c/p\u003e\u003ch3\u003eImplementation:\u003c/h3\u003e\u003cp\u003e1. Create an index.html file with the following content:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003e\u0026lt;!DOCTYPE html\u0026gt;\n\u0026lt;html lang=\"en\"\u0026gt;\n\u0026lt;head\u0026gt;\n\u0026nbsp;\u0026lt;meta charset=\"UTF-8\"\u0026gt;\n\u0026nbsp;\u0026lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u0026gt;\n\u0026nbsp;\u0026lt;title\u0026gt;Video Streaming\u0026lt;/title\u0026gt;\n\u0026nbsp;\u0026lt;script src=\"https://cdn.jsdelivr.net/npm/hls.js@latest\"\u0026gt;\u0026lt;/script\u0026gt;\n\u0026lt;/head\u0026gt;\n\u0026lt;body\u0026gt;\n\u0026nbsp;\u0026lt;h1\u0026gt;Video Streaming Example\u0026lt;/h1\u0026gt;\n\u0026nbsp;\u0026lt;video id=\"videoPlayer\" controls width=\"720\" autoplay\u0026gt;\u0026lt;/video\u0026gt;\n\n\u0026nbsp;\u0026lt;script\u0026gt;\n\u0026nbsp;\u0026nbsp;const video = document.getElementById('videoPlayer');\n\u0026nbsp;\u0026nbsp;const videoSrc = 'http://localhost:3000/play/playlist.m3u8';\n\n\u0026nbsp;\u0026nbsp;if (Hls.isSupported()) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;const hls = new Hls();\n\u0026nbsp;\u0026nbsp;\u0026nbsp;hls.loadSource(videoSrc);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;hls.attachMedia(video);\n\u0026nbsp;\u0026nbsp;} else if (video.canPlayType('application/vnd.apple.mpegurl')) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;video.src = videoSrc;\n\u0026nbsp;\u0026nbsp;} else {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;console.error('This browser does not support HLS streaming');\n\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026lt;/script\u0026gt;\n\u0026lt;/body\u0026gt;\n\u0026lt;/html\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003eKey Points:\u003c/h3\u003e\u003cp\u003e- The browser fetches the \u003ccode\u003e.m3u8\u003c/code\u003e playlist, which directs it to download the \u003ccode\u003e.ts\u003c/code\u003e chunks for smooth playback.\u003c/p\u003e\u003cp\u003e- The HLS.js library ensures compatibility with browsers that don’t natively support HLS.\u003c/p\u003e\u003cp\u003eNow you have a working video player! Let’s take it a step further by allowing users to upload videos.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eStep 4: Allowing Users to Upload Videos\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWhat if users want to upload their own videos? We can process those videos into HLS chunks and store them in S3.\u003c/p\u003e\u003ch3\u003eBackend for Uploading and Processing:\u003c/h3\u003e\u003cp\u003e1. Install the required packages:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enpm install multer fluent-ffmpeg\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e2. Update your server.js file with the following:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport multer from 'multer';\nimport ffmpeg from 'fluent-ffmpeg';\nimport { PutObjectCommand } from '@aws-sdk/client-s3';\nimport fs from 'fs';\n\nconst upload = multer({ dest: 'uploads/' });\n\napp.post('/upload', upload.single('video'), (req, res) =\u0026gt; {\n\u0026nbsp;\u0026nbsp;const inputPath = req.file.path;\n\u0026nbsp;\u0026nbsp;const outputPath = 'processed/';\n\n\u0026nbsp;\u0026nbsp;ffmpeg(inputPath)\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;.output(`${outputPath}playlist.m3u8`)\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;.outputOptions([\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;'-hls_time 10',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;'-hls_list_size 0',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;'-f hls',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;])\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;.on('end', async () =\u0026gt; {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;try {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;// Upload .m3u8 and .ts files to S3\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const filesToUpload = fs.readdirSync(outputPath);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;for (const file of filesToUpload) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;const command = new PutObjectCommand({\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Bucket: 'your-bucket-name',\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Key: `path-to-folder/${file}`,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Body: fs.createReadStream(`${outputPath}${file}`),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;});\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;await s3.send(command);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;res.send('Video uploaded and processed successfully!');\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;} catch (err) {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.error('Error uploading to S3:', err);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;res.status(500).send('Error uploading video.');\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;}\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;})\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;.on('error', (err) =\u0026gt; {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;console.error('Error processing video:', err);\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;res.status(500).send('Error processing video.');\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;})\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;.run();\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003eWorkflow:\u003c/h3\u003e\u003cp\u003e1. Users upload a video via the /upload endpoint.\u003c/p\u003e\u003cp\u003e2. FFmpeg processes the video into HLS chunks.\u003c/p\u003e\u003cp\u003e3. The processed files are automatically uploaded to S3.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eFinal Thoughts\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eCongratulations! You’ve built a fully functional video streaming platform. Here's a recap of what we accomplished:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Secure Storage\u003c/strong\u003e: Used AWS S3 for reliable and scalable storage.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Seamless Streaming\u003c/strong\u003e: Enabled dynamic streaming with HLS.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- User Uploads\u003c/strong\u003e: Automated video processing and storage with FFmpeg and Node.js.\u003c/p\u003e\u003cp\u003eThis architecture is scalable and can serve as the foundation\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FHLS-IMAGE.jpg?alt=media\u0026token=8077c433-3d64-4627-86d5-e9605a6aa9a2","image_alt":"HLS background","slug":"Building-a-Video-Streaming-Platform-with-AWS-S3-HLS-and-Nodejs","index":"6b86b273ff34f"},{"blog_id":"2b8dd94e-d12e-47ea-ba0b-dc2c18c37b68","title":"How to Set Up a Load Balancer in AWS: A Simple Guide to Keep Your Website Fast and Scalable","short_description":"You’ve built an amazing website or app, and the traffic starts pouring in. Users are clicking, scrolling, and browsing like never before. Everything is running smoothly… until it’s not. Suddenly, your server gets overwhelmed, and your site slows down. Worse, it crashes. Yikes!  What do you do?  Enter Load Balancing — your superhero in the world of web traffic. ?  But why do we need it, and how do we set it up on AWS using Nginx? Let's dive in and break it down in simple terms.","timestamp":"2024-12-12 12:48:36","description":"\u003ch1\u003e\u003cstrong\u003eWhy You Need Load Balancing\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine you’re throwing a huge party, and your house is the server. If everyone shows up at once, there’s no room to move, and things get crowded. But if you have a few people guiding guests into different rooms, everyone gets a chance to enjoy the party without overcrowding any single room. This is \u003cstrong\u003eLoad Balancing\u003c/strong\u003e!\u003c/p\u003e\u003cp\u003eLoad balancing is like your traffic manager: it takes the incoming website traffic and evenly distributes it across multiple servers. The result?\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eBetter Performance\u003c/strong\u003e: No server is overloaded.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMore Reliability\u003c/strong\u003e: If one server crashes, the others keep the party going.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEasy Scalability\u003c/strong\u003e: As your site grows, you can add more servers to handle more traffic without a hitch.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this guide, we’re going to set up a simple load balancing system on AWS using \u003cstrong\u003eEC2 instances\u003c/strong\u003e and \u003cstrong\u003eNginx\u003c/strong\u003e (a popular web server). We’ll skip the fancy AWS Elastic Load Balancer (ELB) for now and do it ourselves. ?\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Load Balancing Setup in a Nutshell\u003c/strong\u003e\u003c/h1\u003e\u003ch1\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fload_balancing_structure.png?alt=media\u0026amp;token=92732226-d927-43d0-9d9e-3d82fed692f1\" alt=\"load balancing workflow example for demo\" width=\"720px\"\u003e\u003c/h1\u003e\u003cp\u003eIn a typical web application, you want to ensure that traffic is efficiently managed across your infrastructure. If all users are directed to a single server, that server might get overloaded, causing slow response times or crashes. This is where \u003cstrong\u003eload balancing\u003c/strong\u003e comes into play.\u003c/p\u003e\u003cp\u003eLet’s break down our setup:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- 1 Load Balancer\u003c/strong\u003e: This is like the traffic cop for your web traffic. When users visit your website, their requests first hit the load balancer. The load balancer’s job is to direct these requests to the available backend servers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- 2 Server Instances\u003c/strong\u003e: These are your backend servers that actually process the users’ requests and return content. The load balancer will send incoming requests to either Server 1 or Server 2, ensuring that no single server is overwhelmed. If one server goes down, the load balancer automatically redirects traffic to the other one, ensuring high availability.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhat You’ll Need\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBefore we start, make sure you have:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Three EC2 instances\u003c/strong\u003e on AWS: One for the load balancer and two for the backend servers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Nginx installed\u003c/strong\u003e on all the instances. It’ll handle the distribution of traffic to the backend servers.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eLet’s Get Started! Step by Step\u003c/strong\u003e\u003c/h1\u003e\u003ch3\u003e\u003cstrong\u003eStep 1: Launch EC2 Instances in AWS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eYou’ll need three EC2 instances:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eServer 1 (Backend)\u003c/strong\u003e: This server will handle part of the traffic.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eServer 2 (Backend)\u003c/strong\u003e: Another server to handle a different part of the traffic.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad Balancer\u003c/strong\u003e: This instance will distribute the traffic between the two backend servers.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAll servers should be in the same \u003cstrong\u003eVPC\u003c/strong\u003e (Virtual Private Cloud) for easy communication.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eStep 2: Install Nginx on Each EC2 Instance\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce your EC2 instances are running, it’s time to install Nginx. You can do that by connecting to each instance and running:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update \nsudo apt install nginx\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cstrong\u003eStep 3: Configure Nginx on the Load Balancer\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow comes the fun part — setting up Nginx on the \u003cstrong\u003eLoad Balancer\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Access your load balancer instance\u003c/strong\u003e and open the Nginx configuration file:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /etc/nginx/nginx.conf\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e2. Add the following configuration\u003c/strong\u003e inside the http block:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eupstream backend_servers {\n\u0026nbsp;\u0026nbsp;server 172.31.27.145;\u0026nbsp;# Private IP of Server 1\n\u0026nbsp;\u0026nbsp;server 172.31.27.91;\u0026nbsp;\u0026nbsp;# Private IP of Server 2\n}\n\nserver {\n\u0026nbsp;\u0026nbsp;listen 80;\n\u0026nbsp;\u0026nbsp;server_name \u0026lt;load-balancer-public-IP\u0026gt;;\n\n\u0026nbsp;\u0026nbsp;location / {\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;proxy_pass http://backend_servers;\n\u0026nbsp;\u0026nbsp;}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis is telling Nginx, “Hey, I’ve got two backend servers here (with private IPs), and I want to send all incoming traffic to them.” The load balancer will automatically send traffic to either Server 1 or Server 2.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCheck if the configuration is correct\u003c/strong\u003e:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nginx -t\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003eReload Nginx\u003c/strong\u003e to apply the new settings:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl reload nginx\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3\u003e\u003cstrong\u003eStep 4: Test Your Backend Servers\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eBefore you test the load balancing, make sure both of your backend servers are working properly.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- On Server 1\u003c/strong\u003e, create a simple HTML file that says “Hello from Server 1”:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /usr/share/nginx/html/index.html\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cstrong\u003e- Write: Hello from Server 2 n the file.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- On Server 2, do the same but say “Hello from Server 2”.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e- Test each server directly:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFrom your load balancer instance, you can use curl to check if each server is serving the correct page:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecurl http://172.31.27.145  # Server 1\ncurl http://172.31.27.91   # Server 2\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eYou should see “Hello from Server 1” for Server 1 and “Hello from Server 2” for Server 2.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eStep 5: Test the Load Balancer\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow, let’s put the load balancing to the test. Open your browser and go to the \u003cstrong\u003epublic IP\u003c/strong\u003e of the \u003cstrong\u003eLoad Balancer\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eWhen you hit the load balancer’s IP:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIt should alternate between “Hello from Server 1” and “Hello from Server 2” as Nginx sends traffic to each backend server in turn.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003e\u003cstrong\u003eConclusion: You Did It!\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eCongratulations! ? You’ve just set up a basic load balancing system using Nginx on AWS EC2 instances. By distributing traffic across multiple servers, you’ve made your web app faster, more reliable, and more scalable.\u003c/p\u003e\u003cp\u003eThis is just the beginning — you can easily expand this setup by adding more backend servers or even implementing more advanced load balancing techniques.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhat's Next?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003e1. Add More Servers\u003c/strong\u003e: Scale up by adding more backend servers to handle more traffic.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Secure Your Setup\u003c/strong\u003e: Set up SSL certificates to encrypt traffic.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Monitor Your Servers\u003c/strong\u003e: Use AWS CloudWatch or Nginx logs to monitor the performance of your servers and load balancer.\u003c/p\u003e\u003cp\u003eNow that you understand how load balancing works, you can implement it in your projects to keep your websites and applications fast and responsive, no matter how much traffic you get! ??\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fload_balancing_background.jpg?alt=media\u0026token=a5b062fc-45ad-4770-be8f-ff5b7097c6cc","image_alt":"Load Balancing Background Lab","slug":"how-to-set-up-a-load-balancer-in-aws-a-simple-guide-to-keep-your-website-fast-and-scalable","index":"6b86b273ff34f"},{"blog_id":"986540df-4381-4405-9c40-7ff7b24e6098","title":"Understanding Rate Limiting: A Guide to Staying in Control of Your APIs","short_description":"Imagine you’re hosting a party, and everyone wants to grab snacks from the buffet table at the same time. It’s chaos! Some guests get everything they want, while others leave empty-handed. What if you had a rule where each guest could only take two items at a time? Suddenly, everyone gets a fair share, and your party doesn’t turn into a food fight. That’s rate limiting in a nutshell!  But what exactly is rate limiting, and why is it so important? Let’s dive in and explore this concept together.","timestamp":"2024-12-10 12:37:01","description":"\u003ch1\u003e\u003cstrong\u003eWhat Is Rate Limiting?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAt its core, \u003cstrong\u003erate limiting\u003c/strong\u003e is a control mechanism used in software systems, especially APIs, to restrict how many requests a client can make within a specific timeframe.\u003c/p\u003e\u003cp\u003eThink of it as setting the speed limit on a highway. Without it, cars (or requests) might flood the lanes, causing congestion (or a system crash). Rate limiting ensures everyone gets to their destination (or data) without overwhelming the system.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhy Does Rate Limiting Matter?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine running an online service where thousands (or even millions) of users access your API. What happens if one rogue user floods your system with excessive requests?\u003c/p\u003e\u003cp\u003eWithout rate limiting, here’s what you might face:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1.Server Overload:\u003c/strong\u003e Your system might slow down or crash entirely.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2.Unhappy Users:\u003c/strong\u003e Other users won’t get timely responses, leading to frustration.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3.Increased Costs:\u003c/strong\u003e Handling unnecessary requests eats up resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4.Security Risks:\u003c/strong\u003e It’s an open invitation for \u003cstrong\u003eDDoS (Distributed Denial of Service)\u003c/strong\u003e attacks.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eHow Does Rate Limiting Work?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Frate_limiter_flow.jpg?alt=media\u0026amp;token=0150c280-c4a3-42b2-8422-0a223711a465\" alt=\"Rate limieter workflow example\" width=\"720px\"\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e1. The Client Sends a Request\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eLet’s start with the clients—your users. They might be requesting to fetch data, submit a form, or interact with your app in some way. Every action sends a request to your API server.\u003c/p\u003e\u003cp\u003eNow, without a system in place, too many requests from too many clients could crush the API. This is where the rate limiter steps in.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e2. The Rate Limiter Checks the Gate\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe rate limiter is your vigilant bouncer. Each incoming request is checked against a set of rules. For example:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRule:\u003c/strong\u003e No more than 10 requests per second per client.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRule:\u003c/strong\u003e A maximum of 1,000 requests per day for premium users.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf a request fits within the rules, it gets a thumbs-up. If not, the rate limiter steps in with a polite \"Sorry, you've reached your limit\" (a.k.a., the \u003ccode\u003e429 Too Many Requests\u003c/code\u003e error).\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e3. Redis: The Silent Helper\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow, how does the rate limiter keep track of all this? Enter \u003cstrong\u003eRedis\u003c/strong\u003e, the speedy memory store.\u003c/p\u003e\u003cp\u003eRedis is like a super-efficient notebook that logs each client’s request count. Here’s how it works:\u003c/p\u003e\u003cp\u003eWhen a request comes in, Redis:\u003c/p\u003e\u003cp\u003e-Checks how many requests the client has already made.\u003c/p\u003e\u003cp\u003e-Updates the tally in real-time.\u003c/p\u003e\u003cp\u003eRedis’s speed and scalability make it perfect for handling this kind of workload.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e4. Forwarding to the API Server\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf the request passes the rate limiter’s scrutiny, it’s sent to the \u003cstrong\u003eAPI server\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eThe server processes the request, performs the required action (like retrieving data or updating a record), and sends the response back to the client.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eCommon Rate Limiting Strategies\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eHere are some popular methods to implement rate limiting:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eFixed Window Algorithm\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThink of it as a time bucket. If you allow 100 requests per minute, the count resets every minute.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eExample:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eIf a user sends 99 requests in the last second of a window and 100 in the next second, they technically make 199 requests within two seconds. (Uh-oh!)\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSliding Window Algorithm\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis method smooths things out by tracking requests over a rolling time window. It’s like always looking back 60 seconds from the current moment to count requests.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eToken Bucket\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eImagine each user has a bucket filled with tokens. Each request consumes a token. If the bucket is empty, no more requests are processed until it refills.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eLeaky Bucket\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThis works like a dripping faucet. Even if the user sends requests in bursts, the system processes them at a consistent rate.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhere Is Rate Limiting Used?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eRate limiting isn’t just for APIs—it’s everywhere!\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Social Media Platforms:\u003c/strong\u003e To prevent spamming or abuse (e.g., limiting tweets per minute).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. E-Commerce Sites:\u003c/strong\u003e To stop bots from sniping deals during flash sales.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Gaming Servers:\u003c/strong\u003e To ensure fair play and prevent server overloads.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e4. Banking APIs:\u003c/strong\u003e To protect sensitive systems from fraud or misuse.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eWhy Rate Limiting is Essential\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eRate limiting isn’t just about saying “no.” It’s about balance.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eHere’s what it brings to the table:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Fair Access:\u003c/strong\u003e Every client gets a fair chance to use the API without hogging resources.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Protection:\u003c/strong\u003e Prevents accidental overloads or deliberate attacks (like DDoS) from crashing the system.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e3. Cost Efficiency:\u003c/strong\u003e By controlling traffic, you reduce server strain and save on infrastructure costs.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Big Picture\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eWith rate limiting, APIs can breathe easy, knowing that they’re protected from chaos while serving users efficiently. It’s not just a technical tool—it’s a safeguard for smooth operations.\u003c/p\u003e\u003cp\u003eSo, next time you’re designing an API or interacting with one, remember: there’s a silent hero ensuring everything runs seamlessly. Whether it’s Redis handling the count or the rate limiter enforcing rules, this system is your API’s best friend.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Frate_limiter_bg.png?alt=media\u0026token=0c9fc3ba-7b9d-4fce-8a15-293f8a79d664","image_alt":"Rate limiter cover background","slug":"understanding-rate-limiting-a-guide-to-staying-in-control-of-your-apis","index":"6b86b273ff34f"},{"blog_id":"0af995c6-3bd5-405a-ad71-6ebeaa675d38","title":"Building a Simple CQRS Pattern Architecture","short_description":"In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service.","timestamp":"2024-12-07 09:06:28","description":"\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Query Responsibility Segregation (CQRS)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;pattern is an architectural pattern used to separate the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(commands) and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;(queries) sides of an application. This separation ensures scalability, performance optimization, and flexibility, especially for systems with complex business logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis article explains how to design a\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003esimple CQRS pattern\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and implement it in a practical example.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media\u0026amp;token=6eab7b0b-37d8-49f2-9137-27dadd766c96\" alt=\"CQRS Basic Pattern\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eWhy Do We Need CQRS?\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCQRS is designed to address challenges in systems where the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eread\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;and\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewrite\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;operations have distinct requirements. It is particularly helpful in large, complex applications with high performance, scalability, and maintainability needs. Here’s a breakdown of its importance:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere’s a detailed explanation of\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewhy we need CQRS (Command Query Responsibility Segregation)\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSeparation of Concerns\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn traditional CRUD-based architectures, the same model is often used for both reading and writing data. This can lead to problems such as:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e1.Bloated models trying to handle both reads and writes.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e2.Tight coupling between read and write logic, making it harder to change one without affecting the other.\u003c/span\u003e\u003c/p\u003e\u003ch3\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eOptimization of Reads and Writes\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn many applications, the requirements for\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ereading data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;differ significantly from\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ewriting data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.Writes\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;may require strict validation, transactional consistency, and complex domain logic.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.Reads\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;often focus on speed, scalability, and simplicity, potentially requiring optimized or denormalized views of data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePre-requisites for this lab:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this article we will build simple cqrs architecture with apache kafka, elastic search, and the backend service (Express.js).\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;VM 1: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for handling messaging and event distribution.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;VM 2: Runs\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;for read-optimized data storage and retrieval.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eExpress.js Services\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e:\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;with an endpoint for inserting data into the system.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;A\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eQuery service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to retrieve data from Elasticsearch.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;Mysql Database that connected to Express.js service\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePlanned Architecture\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FCQRS.webp?alt=media\u0026amp;token=58c5dcef-485c-482d-8c6a-459473e51f04\" alt=\"Planned CQRS Architecture for this lab\" width=\"720px\"\u003e\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe architecture depicted in your diagram showcases a practical implementation of the CQRS (Command Query Responsibility Segregation) pattern using separate services and data stores for handling write and read operations. At its core, this design focuses on decoupling the responsibilities of updating and retrieving data, ensuring better scalability, performance, and maintainability.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCommand Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, built with Express.js, serves as the entry point for handling all write operations. Whenever a client sends a request to add or update data, the Command Service writes the data to a MySQL database. This database acts as the system's primary source of truth, ensuring the durability and consistency of all data. Once the data is successfully persisted in MySQL, the Command Service publishes an event to the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Broker\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, implemented with Apache Kafka. The role of Kafka here is to act as an intermediary that reliably propagates changes across the system, enabling asynchronous communication between services.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOn the other side of the architecture, a consumer service listens to the events broadcasted by Kafka. Whenever a new event is received, the consumer retrieves the relevant data from MySQL, transforms it if needed, and indexes it into an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eElasticSearch instance\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. ElasticSearch, being optimized for querying and search operations, ensures that data is structured for fast retrieval. This makes it the perfect choice for systems that need to handle complex queries or search-heavy workloads without compromising performance.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eRead Service\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e, also built with Express.js, provides an API for retrieving data from ElasticSearch. By querying ElasticSearch directly, the Read Service delivers low-latency responses to clients, even under high query loads. This design ensures that the performance of the read operations does not interfere with or degrade the performance of write operations in the Command Service. The use of ElasticSearch also enables advanced search capabilities, such as full-text search, aggregations, and filtering, which are often slow or complex to implement in traditional relational databases.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis architecture embodies the essence of CQRS by segregating the responsibilities of writing and querying data into distinct paths. The Command Service and MySQL handle writes and ensure data consistency, while the Read Service and ElasticSearch are optimized for delivering fast and efficient queries. The inclusion of Kafka as a Message Broker enables asynchronous processing, allowing the system to remain responsive to client requests even when downstream systems take time to process data.\u003c/span\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSetting Up Apache Kafka on Ubuntu Server:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn this guide, I’ll walk you through setting up\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eApache Kafka\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;on an\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eUbuntu Server\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;running on a virtual machine. While you can certainly use Docker and Docker Compose for a Kafka setup, I decided to go the manual route to test the installation process. So, if you’re ready to roll up your sleeves, let’s dive in!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: Install Java\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka runs on the Java Virtual Machine (JVM), so the first step is to install Java. We’ll use OpenJDK 17 for this:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install openjdk-17-jdk -y\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOnce installed, you can verify the version with:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ejava -version\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Download Kafka\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNext, we need to download the Kafka binaries. Use the following command to grab the latest Kafka release:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter downloading, extract the archive:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003etar -xvf kafka_2.13-3.6.0.tgz\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow, let’s move the extracted Kafka directory to\u0026nbsp;/opt for easier access:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo mv kafka_2.13-3.6.0 /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eFinally, navigate to the Kafka directory:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecd /opt/kafka\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Configure Kafka Server Properties\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eBefore we start Kafka, we need to tweak its configuration a bit. Open the\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eserver.properties\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;file with a text editor:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enano config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eHere are a couple of key settings to look for:\u003c/span\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003elog.dirs: This is where Kafka will store its log files. You can set it to a directory of your choice.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ezookeper.connect: Ensure this points to your ZooKeeper instance. If you’re running ZooKeeper locally, the default setting should work.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Start ZooKeeper\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka relies on\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eZooKeeper\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;to manage its metadata, so we’ll need to start ZooKeeper before starting Kafka. Use the following command to get ZooKeeper up and running:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo run ZooKeeper as a background process (so you can keep using your terminal), use this instead:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/zookeeper-server-start.sh config/zookeeper.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Start the Kafka Broker\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eNow that ZooKeeper is running, it’s time to fire up Kafka. Use this command to start the Kafka broker:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eOr, to run Kafka in the background:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-server-start.sh config/server.properties \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Test Your Kafka Setup\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eCongratulations! Your Kafka instance is now up and running. Let’s do a quick test to ensure everything works as expected.\u003c/span\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eCreate a Topic\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eKafka organizes messages into topics. Let’s create a topic named\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eList Topics\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo confirm that the topic was created, list all topics:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-topics.sh --list --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Producer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA producer sends messages to a Kafka topic. Start the producer for\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eType a few messages in the terminal, and they’ll be sent to the topic.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStart a Consumer\u003c/strong\u003e\u003c/li\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eA consumer reads messages from a topic. Start a consumer to read messages from\u0026nbsp;test-topic:\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ebin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003col\u003e\u003cli\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see the messages you typed in the producer terminal appear here!\u003c/span\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 2: Installing Elasticsearch on a Virtual Machine\u003c/strong\u003e\u003c/h2\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 1: SSH into Your Server\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eConnect to your EC2 instance (or VM):\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003essh -i /path/to/your-key.pem ec2-user@your-ec2-public-ip\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 2: Install Java\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eElasticsearch also needs Java:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo apt update\nsudo apt install -y openjdk-11-jdk\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 3: Install Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ewget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt install -y apt-transport-https\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list\n\nsudo apt update\n\nsudo apt install -y elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 4: Enable and Start Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 5: Configure Elasticsearch for External Access\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eEdit the configuration:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo nano /etc/elasticsearch/elasticsearch.yml\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eSet these properties:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003enetwork.host: 0.0.0.0\nhttp.port: 9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRestart Elasticsearch:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003esudo systemctl restart elasticsearch\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eStep 6: Verify Elasticsearch\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eRun:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecurl -X GET http://localhost:9200\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eYou should see a JSON response!\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart:3 Explanation of the Express.js POST Route for Inserting User Data and Sending to Kafka\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport express from 'express';\nimport { Kafka } from 'kafkajs';\nimport mysql from 'mysql2/promise'; // MySQL client for Node.js\n\nconst app = express();\napp.use(express.json()); // Parse JSON request bodies\n\n// Kafka producer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'my-app',\n\u0026nbsp; \u0026nbsp; brokers: ['localhost:9092'], // Replace with your Kafka broker(s)\n});\n\nconst producer = kafka.producer();\n\n// Connect Kafka producer\nasync function connectProducer() {\n\u0026nbsp; \u0026nbsp; await producer.connect();\n}\n\nconnectProducer().catch(console.error);\n\n// MySQL database connection setup\nconst dbConfig = {\n\u0026nbsp; \u0026nbsp; host: 'localhost',\n\u0026nbsp; \u0026nbsp; user: 'root', // Replace with your MySQL username\n\u0026nbsp; \u0026nbsp; password: 'root', // Replace with your MySQL password\n\u0026nbsp; \u0026nbsp; database: 'user', // Replace with your database name\n};\n\nlet connection;\n\nasync function connectDatabase() {\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; connection = await mysql.createConnection(dbConfig);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('Connected to MySQL database');\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error connecting to MySQL:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; process.exit(1);\n\u0026nbsp; \u0026nbsp; }\n}\n\nconnectDatabase();\n\n// POST route to insert user data\napp.post('/users', async (req, res) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; const userData = req.body;\n\n\u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into MySQL database\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const { name, email, password } = userData; // Assuming user data has 'name' and 'email' fields\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const [result] = await connection.execute(\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; 'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [name, email, password]\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; );\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User inserted into database:', result);\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Send the user data to Kafka topic 'user-topic'\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; await producer.send({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; topic: 'users-topic', // Replace with your topic\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; messages: [\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; { value: JSON.stringify(userData) },\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ],\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error processing request:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; res.status(500).json({ message: 'Error processing request' });\n\u0026nbsp; \u0026nbsp; }\n});\n\n// Start Express server\napp.listen(3000, () =\u0026gt; {\n\u0026nbsp; \u0026nbsp; console.log('Server running on http://localhost:3000');\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIn the given code snippet, an Express.js route (/users) is created to handle\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePOST requests\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. This route processes user data by first saving it into a MySQL database and then sending the same data to a Kafka topic. Below is a step-by-step explanation of how this route works:\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e1.\u0026nbsp;Endpoint Definition and Request Handling\u003c/strong\u003e\u003c/h4\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eThe app.post('/users', async (req, res) defines a route that listens for POST requests at the /users endpoint. It uses async/await to handle asynchronous operations such as database insertion and Kafka messaging. The req.body object is used to extract the data sent by the client in the request payload.const userData = req.body;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThe\u0026nbsp;userData object contains the user-provided information, typically in JSON format. For example, it might include fields like\u0026nbsp;name. email and password.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e2.\u0026nbsp;Inserting User Data into MySQL\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eTo store user data, the route uses a prepared SQL statement to prevent\u0026nbsp;\u003c/span\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eSQL injection attacks\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e. The\u0026nbsp;\u003c/span\u003econnection.execute()\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e\u0026nbsp;function interacts with the database, where\u0026nbsp;name, email and passwords fields are inserted into a\u0026nbsp;user table.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econst [result] = await connection.execute(\n    'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n    [name, email, password]\n);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePrepared Statement\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;? placeholders in the SQL query are replaced with actual values (name, email, password) safely.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eDeconstructed User Data\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;name,\u0026nbsp;email, and\u0026nbsp;password fields are extracted from the\u0026nbsp;userData object for better readability and security.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eResult\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;connection.execite()\u0026nbsp;method returns an array, where\u0026nbsp;result contains metadata about the operation, such as the number of rows affected.\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eIf the operation succeeds, a log is generated to confirm that the user data was inserted into the database:\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003econsole.log('User inserted into database:', result);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e3.\u0026nbsp;Sending Data to a Kafka Topic\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter successfully storing the data in MySQL, the route sends the same data to a Kafka topic for further processing. Kafka is often used to handle large-scale distributed messaging and stream processing.\u003c/span\u003e\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eawait producer.send({\n    topic: 'users-topic', // Replace with your topic\n    messages: [\n        { value: JSON.stringify(userData) },\n    ],\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eKafka Producer\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003ep: The producer object is an instance of Kafka's producer client, which is responsible for sending messages to Kafka topics.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eTopic Name\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;topic field specifies the destination Kafka topic (user-topic in this case). This is where the message will be sent for further processing by Kafka consumers.\u003c/span\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003eMessage Payload\u003c/strong\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003e: The\u0026nbsp;message array contains the data to be sent. Each message is an object with a\u0026nbsp;value field, which holds the serialized user data (converted to JSON using\u0026nbsp;JSON.stringfy(userData)).\u003c/span\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis mechanism ensures that user data is available for other systems (e.g., analytics, logging, or notifications) in near real-time.\u003c/span\u003e\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e4.\u0026nbsp;Response to the Client\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eIf both the database insertion and Kafka message-sending steps succeed, the server sends a 201 Created response to the client with a success message:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eres.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThe 201 status code indicates that the request was successfully processed and a new resource was created.\u003c/p\u003e\u003ch4\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003e5.\u0026nbsp;Error Handling\u003c/strong\u003e\u003c/h4\u003e\u003cp\u003eThe try-catch block ensures that errors during either database insertion or Kafka messaging are gracefully handled. If an error occurs, it is logged for debugging purposes, and the client receives a 500 Internal Server Error response:\u003c/p\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003ecatch (error) {\n    console.error('Error processing request:', error);\n    res.status(500).json({ message: 'Error processing request' });\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eThis approach provides transparency to developers and prevents the application from crashing due to unhandled exceptions.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cspan style=\"color: rgb(255, 255, 255);\"\u003eAfter building the express.js service to insert data in mysql database and message broker. we will create the instance that listening on this producer.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 4: Explanation of Kafka Consumer with Elasticsearch Integration\u003c/strong\u003e\u003c/h2\u003e\u003cdiv\u003e\u003cpre\u003e\u003ccode\u003eimport { Kafka } from 'kafkajs';\nimport { Client } from '@elastic/elasticsearch';\n\n// Kafka consumer setup\nconst kafka = new Kafka({\n\u0026nbsp; \u0026nbsp; clientId: 'express-app',\n\u0026nbsp; \u0026nbsp; brokers: ['192.168.128.207:9092'], // Replace with your Kafka broker(s)\n});\n\nconst consumer = kafka.consumer({ groupId: 'user-group' });\n\n// Elasticsearch client setup\nconst esClient = new Client({\n\u0026nbsp; \u0026nbsp; node: 'http://localhost:9200', // Replace with your Elasticsearch URL\n});\n\n// Kafka consumer processing\nasync function consumeMessages() {\n\u0026nbsp; \u0026nbsp; await consumer.connect();\n\u0026nbsp; \u0026nbsp; await consumer.subscribe({ topic: 'users-topic', fromBeginning: true }); // Replace with your topic\n\n\u0026nbsp; \u0026nbsp; await consumer.run({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; eachMessage: async ({ topic, partition, message }) =\u0026gt; {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const userData = JSON.parse(message.value.toString());\n\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; // Insert data into Elasticsearch\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; try {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; const response = await esClient.index({\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; index: 'users', // The Elasticsearch index to use\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; id: message.name,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; document: userData,\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; });\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.log('User data inserted into Elasticsearch:', response);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; } catch (error) {\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; console.error('Error inserting data into Elasticsearch:', error);\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; }\n\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; },\n\u0026nbsp; \u0026nbsp; });\n}\n\nconsumeMessages().catch(console.error);\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis code demonstrates how to integrate Kafka as a messaging system and Elasticsearch as a search and data indexing tool in a Node.js application. The overall flow involves consuming messages from a Kafka topic and then indexing the received data into Elasticsearch for further use, such as querying or searching.\u003c/p\u003e\u003cp\u003eTo start, the script imports two essential libraries: KafkaJS and Elasticsearch client. KafkaJS is a JavaScript library used for interacting with Kafka, which is a distributed streaming platform. The Kafka client allows you to create consumers that can listen to Kafka topics and process the messages in real time. On the other hand, the Elasticsearch client facilitates communication with an Elasticsearch cluster, enabling the ability to store and index documents, which can later be queried or analyzed.\u003c/p\u003e\u003cp\u003eThe Kafka consumer is set up by first initializing the Kafka client with a unique clientId (express-app) and specifying the Kafka brokers. These brokers are the Kafka servers where the consumer will connect. The consumer is created with a groupId, which is user-group in this case. The group ID helps manage message consumption across multiple instances of the consumer. When consumers with the same group ID listen to a Kafka topic, Kafka ensures that each partition of the topic is assigned to only one consumer in the group, effectively balancing the load.\u003c/p\u003e\u003cp\u003eNext, the code sets up the Elasticsearch client by specifying the address of the Elasticsearch node (http://localhost:9200). This client will be used to interact with the Elasticsearch service where the user data will be indexed. Elasticsearch is widely used for its powerful search and analytics capabilities, which can handle large volumes of data and provide fast search results.\u003c/p\u003e\u003cp\u003eOnce both Kafka and Elasticsearch clients are set up, the consumeMessages() function is created to handle the actual logic of consuming messages from Kafka. This function first connects to the Kafka cluster and subscribes to the users-topic. By subscribing to the topic, the consumer listens for new messages that are published to that topic. The fromBeginning: true option ensures that the consumer starts processing messages from the very beginning of the topic’s log, meaning it will consume all the messages from when it first subscribes, not just new messages that arrive after it subscribes.\u003c/p\u003e\u003cp\u003eThe function then uses consumer.run() to begin consuming messages. Each message from the Kafka topic is processed in the eachMessage callback function. Inside this function, the message's value is parsed from a buffer into a JavaScript object (since Kafka messages are typically sent as binary data). The parsed data, which represents user information in this case, is then indexed into Elasticsearch. The esClient.index() method is used to insert this data into the users index in Elasticsearch. A unique identifier for the document is generated using the message's name field. This id ensures that each document in Elasticsearch can be uniquely identified.\u003c/p\u003e\u003cp\u003eIf the data insertion into Elasticsearch is successful, a response from Elasticsearch is logged to the console, confirming that the user data has been indexed. If an error occurs while inserting the data, the error is caught and logged.\u003c/p\u003e\u003cp\u003eFinally, the consumeMessages() function is invoked, and any unhandled errors are caught by console.error() to prevent the application from crashing. This ensures that the consumer will keep running and processing messages as they arrive, continuously feeding new data into Elasticsearch for indexing.\u003c/p\u003e\u003ch2\u003e\u003cstrong style=\"color: rgb(255, 255, 255);\"\u003ePart 5: Wrapup what we have been doing\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThe architecture discussed in this article aligns well with the CQRS (Command Query Responsibility Segregation) pattern, which is a powerful design pattern that separates the logic of reading data (queries) from the logic of writing data (commands). By implementing Kafka and Elasticsearch in conjunction with MySQL and Express.js, we create a robust system that effectively adheres to the principles of CQRS.\u003c/p\u003e\u003cp\u003eIn this architecture, the write operations (commands) are handled by the /users POST route in the Express.js service. When user data is received, it's inserted into the MySQL database and sent to Kafka. Kafka acts as the message bus, decoupling the data-writing process from the read operations and ensuring that data can be asynchronously processed and consumed by different systems or services.\u003c/p\u003e\u003cp\u003eThe read operations (queries) are efficiently handled by Elasticsearch. After the data is consumed from Kafka and indexed into Elasticsearch, it becomes readily available for fast and scalable querying. Elasticsearch's ability to index and search large volumes of data makes it an excellent fit for handling query-based operations in this architecture.\u003c/p\u003e\u003cp\u003eBy using CQRS, we ensure that the system is optimized for both reading and writing operations, enabling high scalability and responsiveness. Kafka, as the message broker, enables asynchronous communication and allows for horizontal scaling in the system. Meanwhile, Elasticsearch ensures that queries on user data are fast, efficient, and scalable.\u003c/p\u003e\u003cp\u003eThis CQRS-based approach also helps with performance optimization, as read and write concerns are handled separately. It allows for the scaling of each part of the system independently, depending on whether there is a higher load on reading or writing. The separation of concerns promotes better maintainability, flexibility, and scalability, making this architecture ideal for modern, high-traffic applications requiring real-time data processing and analytics.\u003c/p\u003e\u003cp\u003eIn conclusion, by integrating Kafka, Elasticsearch, and Express.js with a CQRS approach, this system architecture offers a scalable, maintainable, and highly performant solution for handling real-time data in applications where reading and writing data need to be optimized separately.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs-background-title.png?alt=media\u0026token=dd34dffa-1cc4-4b14-b4e2-f6840555b0e4","image_alt":"Image cover of CQRS Architecture","slug":"building-a-simple-cqrs-pattern-architecture","index":"6b86b273ff34f"},{"blog_id":"e7d3943c-c12a-42c1-9663-2d90449138dc","title":"The concept of splitting a frontend into smaller, manageable pieces.","short_description":"Have you ever worked on a massive frontend application where every change felt risky and deploying updates took ages? If so, micro frontends might be the solution you’ve been looking for","timestamp":"2024-12-04 11:23:45","description":"\u003cp\u003eMicro frontends are an architectural approach that applies the principles of microservices to frontend development. Instead of building a single, monolithic frontend application, the user interface is divided into smaller, independent pieces called \u003cstrong\u003emicro frontends\u003c/strong\u003e, each of which is developed, deployed, and maintained independently.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eThe Problem with Monolithic Frontends\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eA \u003cstrong\u003emonolithic frontend\u003c/strong\u003e refers to a single, large codebase that manages the entire user interface of an application. While this approach works well for small applications, it becomes increasingly challenging to manage and scale as the application and development teams grow. Below are the key problems associated with monolithic frontends:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003e1. Scaling Teams\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCoordination Overhead\u003c/strong\u003e: In a large team, multiple developers work on the same codebase. This can lead to frequent merge conflicts, delayed pull requests, and dependency issues.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimited Parallel Development\u003c/strong\u003e: Because the codebase is tightly coupled, teams cannot work independently on different parts of the application without stepping on each other's toes.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e2. Slower Development Cycles\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSingle Deployment Pipeline\u003c/strong\u003e: In a monolithic frontend, all changes must pass through the same build and deployment pipeline. This means Small changes (e.g., fixing a typo) require deploying the entire application, also A single bug can block the entire release process.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLonger Testing Time\u003c/strong\u003e: The larger the application, the more time and effort it takes to ensure that new changes don’t break existing functionality.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e3. High Risk of Changes\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRipple Effect\u003c/strong\u003e: Since everything is interconnected, even small changes in one part of the application can have unintended consequences elsewhere.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRollback Challenges\u003c/strong\u003e: If something goes wrong after deployment, rolling back requires reverting the entire application, not just the problematic component.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003e4. Tech Stack Lock-In\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDifficult to Adopt New Frameworks\u003c/strong\u003e: In a monolithic frontend, the entire application is built using a single framework or library. Upgrading or switching technologies is a monumental task that may require rewriting the entire application. \u003cstrong\u003eExample\u003c/strong\u003e: Migrating from AngularJS to React would involve significant effort and downtime.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo Flexibility for Teams\u003c/strong\u003e: Teams must stick to the same tech stack, even if certain parts of the application would benefit from newer or more suitable tools.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003e\u003cstrong\u003eHow Micro Frontends Work\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eMicro frontends operate on the principle of \u003cstrong\u003edivide and conquer\u003c/strong\u003e. Instead of managing one gigantic codebase, you divide your application into smaller units.\u003c/p\u003e\u003cp\u003eThe idea behind micro frontends is simple yet powerful:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e1. Divide\u003c/strong\u003e your application into smaller, self-contained pieces.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e2. Conquer\u003c/strong\u003e by developing, testing, and deploying these pieces independently.\u003c/p\u003e\u003cp\u003eInstead of one massive codebase where every change has the potential to disrupt the entire application, you get a collection of smaller, focused units that can evolve at their own pace. Here is the breakdown of how divide and conquer works in microfrontend.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmonolit_vs_microfrontend.png?alt=media\u0026amp;token=24eba3d8-b7f3-4674-b33e-62daf0517afd\" alt=\"Monolithic Vs Microfrontend\" width=\"720px\"\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eIn a traditional monolithic architecture (left side), all components of a web application are tightly coupled and deployed as a single unit. This includes the web application layer (frontend), integration layer (APIs), and service layer (backend).\u003c/p\u003e\u003cp\u003eMicrofrontends (right side) break down a large web application into smaller, independent frontend applications (also called \"micro-applications\" or \"micro-frontends\"). Each microfrontend is responsible for a specific feature or section of the application. This approach promotes modularity, scalability, and faster development cycles.\u003c/p\u003e\u003cp\u003eThe divide and conquer approach is central to microfrontends. It involves:\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eDividing the Application:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIdentify Features:\u003c/strong\u003e First, break down the application into distinct features or sections. For example, in the image, we have \"Cart,\" \"Website,\" and \"Payment\" as separate features.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAssign Teams:\u003c/strong\u003e Each feature is assigned to an independent team. This allows teams to work autonomously, focusing on their specific feature.\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e\u003cstrong\u003eConquering the Features:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eIndependent Development:\u003c/strong\u003e Each team develops its feature as a standalone frontend application. They can use different technologies (React, JAML, etc.) and frameworks as needed.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAPI Integration:\u003c/strong\u003e Teams define clear APIs for their microfrontends to communicate with each other and with the backend services. This ensures loose coupling and flexibility.\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003e\u003cstrong\u003eComposition and Orchestration:\u003c/strong\u003e\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eFrameworks and Tools:\u003c/strong\u003e A framework or library is used to combine the microfrontends into a cohesive user experience. This can be done using techniques like server-side composition (e.g., with a Node.js server) or client-side composition (e.g., with a JavaScript framework like Single-SPA).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRouting and Navigation:\u003c/strong\u003e The framework handles routing and navigation between microfrontends, ensuring a seamless user experience.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eShared Components:\u003c/strong\u003e If necessary, shared components can be developed and used across multiple microfrontends. This promotes consistency and reduces code duplication.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eMicro frontends represent a modern approach to building scalable, resilient, and maintainable frontend applications. By breaking down your application into smaller, independent pieces, you empower your teams to innovate faster and reduce deployment risks.. If you’ve ever felt constrained by the limitations of a monolithic frontend, it might be time to explore the possibilities of micro frontends. They’re not just a technical solution—they’re a way to rethink how we build for the web.\u003c/p\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmicrofrontend.webp?alt=media\u0026token=6274c176-9622-4cd5-93be-9ea9e5912bd9","image_alt":"Microfrontend Image Cover","slug":"the-concept-of-splitting-a-frontend-into-smaller-manageable-pieces","index":"6b86b273ff34f"},{"blog_id":"4a5ae2d7-0f8f-46b9-b49a-4ff130f22292","title":"Why You Need Kubernetes: A Comprehensive Guide","short_description":"In today's fast-paced digital landscape, applications are becoming increasingly complex and distributed. To manage this complexity and ensure high availability, reliability, and scalability, organizations are turning to Kubernetes. This powerful container orchestration platform has revolutionized the way we deploy and manage applications.","timestamp":"2024-11-02 23:48:06","description":"\u003cdiv id=\"content-0\"\u003e\u003cp\u003eIn today's rapidly evolving technological landscape, applications are becoming increasingly complex and distributed. To manage this complexity and ensure high availability, reliability, and scalability, organizations are turning to Kubernetes. This powerful container orchestration platform has revolutionized the way we deploy and manage applications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhy Kubernetes?\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cp\u003e\u003cstrong\u003e1. Simplified Deployment and Management:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAutomated Deployment:\u003c/strong\u003e Kubernetes automates the deployment process, eliminating manual intervention and reducing the risk of human error. With a few configuration changes, you can deploy complex applications to multiple environments with ease.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSelf-Healing:\u003c/strong\u003e Kubernetes can automatically detect and recover from failures, ensuring that your applications remain up and running. If a pod fails, Kubernetes will automatically restart it on a different node.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScalability:\u003c/strong\u003e You can easily scale your applications up or down to meet changing demand, without requiring significant manual effort. Whether it's a sudden traffic spike or a planned scaling event, Kubernetes can handle it seamlessly.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003e\u003cstrong\u003e2. Efficient Resource Utilization:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eResource Allocation:\u003c/strong\u003e Kubernetes efficiently allocates resources (CPU, memory) to your applications, maximizing utilization and minimizing waste. It ensures that your applications get the resources they need, while avoiding overprovisioning.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic Scheduling:\u003c/strong\u003e It intelligently schedules pods onto nodes, optimizing resource allocation across the cluster. This ensures that your applications are always running on the most suitable nodes, regardless of their resource requirements.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e3. Enhanced Reliability and Availability:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHigh Availability:\u003c/strong\u003e Kubernetes ensures high availability by replicating your applications across multiple nodes, providing redundancy and fault tolerance. If one node fails, your application will continue to run on other nodes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad Balancing:\u003c/strong\u003e It automatically distributes traffic across multiple instances of your application, improving performance and reliability. This ensures that no single instance is overwhelmed, and your users get a consistent experience.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003e4. Increased Flexibility and Portability:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContainer-Based:\u003c/strong\u003e Kubernetes is container-based, allowing you to package your applications and their dependencies into portable units. This makes it easy to move your applications between different environments, such as development, testing, and production.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePlatform Agnostic:\u003c/strong\u003e It can run on various infrastructure platforms, including public clouds (AWS, Azure, GCP), private clouds, and on-premises data centers. This gives you the flexibility to choose the best platform for your needs.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eKubernetes Architecture\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkubernetes-architecture.png?alt=media\u0026token=cd32c87e-d584-4aec-a83f-f0c20d7d0f5c'/\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eThis diagram provides a clear overview of the key components in a Kubernetes cluster and how they interact with each other. Let's break down each component:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eControl Plane\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAPI Server:\u003c/strong\u003e The main entry point for all interactions with the cluster. All requests (e.g., creating a new pod, scaling a deployment) are sent to the API Server.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eetcd:\u003c/strong\u003e A distributed, consistent, highly-available key-value store that stores the entire cluster state. All information about pods, services, deployments, and more is stored here.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eController Manager:\u003c/strong\u003e Manages various controllers responsible for ensuring the cluster is in the desired state. Examples of controllers include Deployment Controller, ReplicaSet Controller, and Job Controller.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScheduler:\u003c/strong\u003e Responsible for scheduling pods to available nodes. It considers various factors like resource availability, affinities, and anti-affinities.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eNode\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eKubelet:\u003c/strong\u003e An agent that runs on each node. Kubelet ensures that the containers specified in pod manifests are running on the node.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ekube-proxy:\u003c/strong\u003e A network proxy that implements network rules for services and load balancing.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eComponent Interactions\u003c/strong\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eUser or Tool:\u003c/strong\u003e When you want to create or manage Kubernetes resources (e.g., using kubectl), you interact with the API Server.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAPI Server:\u003c/strong\u003e Receives the request, validates it, and stores it in etcd.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eController Manager:\u003c/strong\u003e Monitors changes in etcd and takes necessary actions. For example, if the number of replicas for a deployment doesn't match the desired state, the controller will create or delete pods.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eScheduler:\u003c/strong\u003e When there's a new pod to be scheduled, the scheduler selects the most suitable node and informs the Kubelet.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKubelet:\u003c/strong\u003e Receives information from the scheduler and starts running the pod's containers.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ekube-proxy:\u003c/strong\u003e Manages networking to ensure traffic is routed to the correct pods.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003e\u003cstrong\u003eDeployments: Scale, Update, Rollback\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eImagine you have a simple web application (e.g., a Node.js app) running in a Kubernetes cluster. The application is exposed via a Kubernetes Service, and you want to manage it using a Deployment. Here’s how you can implement this:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch2\u003e1. \u003cstrong\u003eCreating a Deployment\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eFirst, you'll create a Deployment to manage your application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-web-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-web-app\n  template:\n    metadata:\n      labels:\n        app: my-web-app\n    spec:\n      containers:\n        - name: my-web-app\n          image: myusername/my-web-app:1.0\n          ports:\n            - containerPort: 80\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReplicas\u003c/strong\u003e: This specifies how many pods you want to run.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSelector\u003c/strong\u003e: This defines how to identify the pods managed by this Deployment.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTemplate\u003c/strong\u003e: This describes the pods that will be created.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eDeploying the Application\u003c/strong\u003e: Apply the Deployment with the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl apply -f deployment.yaml\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch2\u003e2. \u003cstrong\u003eScaling the Application\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf you want to handle increased traffic, you can scale your Deployment up or down.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eScaling Up\u003c/strong\u003e: To increase the number of replicas to 5:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl scale deployment my-web-app --replicas=5\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eScaling Down: To decrease the number of replicas back to 3:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl scale deployment my-web-app --replicas=3\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003ch2\u003e3. \u003cstrong\u003eUpdating the Application\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhen you want to update your application (for example, deploying a new version of the image), modify the Deployment:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003espec:\n  template:\n    spec:\n      containers:\n        - name: my-web-app\n          image: myusername/my-web-app:2.0 # Updated version\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cp\u003eApplying the Update: You can update the Deployment by reapplying the configuration:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl apply -f deployment.yaml\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cp\u003eKubernetes will perform a rolling update, gradually replacing the old pods with new ones.\u003c/p\u003e\u003ch2\u003e4. \u003cstrong\u003eChecking the Update Status\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo monitor the status of the update, use:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout status deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003ch2\u003e5. \u003cstrong\u003eRolling Back an Update\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf something goes wrong with the new version, you can roll back to the previous version easily:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout undo deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-28\"\u003e\u003cp\u003eTo check the history of the revisions, you can use:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl rollout history deployment/my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003ch2\u003e6. \u003cstrong\u003eVerifying the Rollback\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAfter rolling back, you can verify that the previous version is running:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-31\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekubectl get deployments\nkubectl describe deployment my-web-app\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003ch2\u003e7. \u003cstrong\u003eCreating a Service\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eA Kubernetes \u003cstrong\u003eService\u003c/strong\u003e is used to expose your application, making it accessible from outside the cluster (or within, depending on your requirements). Here's an example of a Service configuration:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapiVersion: v1\nkind: Service\nmetadata:\n  name: my-web-app-service\nspec:\n  selector:\n    app: my-web-app\n  ports:\n    - protocol: TCP\n      port: 80       # Port on the Service\n      targetPort: 80 # Port on the container\n  type: LoadBalancer\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003ch1\u003e\u003cstrong\u003eIn Conclusion:\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eKubernetes architecture is designed to simplify the management of large-scale containerized applications. By understanding its components and interactions, you can effectively leverage Kubernetes to build reliable and scalable applications.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FKubernetes-logo-1024x576.png?alt=media\u0026token=e6f56ef1-e429-4d8f-9597-2d5a01023cf9","image_alt":"Kubernetes Logo","slug":"why-you-need-kubernetes-a-comprehensive-guide","index":"6b86b273ff34f"},{"blog_id":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb","title":"Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters","short_description":"Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently.","timestamp":"2024-10-07 05:37:09","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eSpark Architecture\u003c/h1\u003e\u003cp\u003eImagine a Spark application as a bustling city. At the heart of this city is the \u003cstrong\u003eDriver Program\u003c/strong\u003e, which acts like the mayor overseeing everything that happens. The driver program is responsible for running your code, coordinating work, and making key decisions about how tasks should be executed. Like a city planner, it organizes and manages the tasks to be performed by the cluster, breaking down large jobs into smaller units of work. These jobs are then divided into \u003cstrong\u003etasks\u003c/strong\u003e that Spark can run in parallel. The beauty of this architecture lies in its ability to scale, allowing multiple tasks to be completed simultaneously on different data partitions. These tasks are dispatched to \u003cstrong\u003eexecutors\u003c/strong\u003e, which are the hard workers of the cluster, doing the heavy lifting.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FApache-Spark-architecture.png?alt=media\u0026token=ae8b882a-a359-4e63-aad6-a9a9f0a06ff8'/\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003eExecutors are independent workers spread across the cluster, each taking responsibility for a portion of the work. They are not just performing tasks but are also responsible for caching data, which can speed up future computations by reusing cached data rather than starting from scratch. Much like how a construction team works efficiently by reusing tools and materials at the job site, executors are optimized to perform computations without redundant effort.\u003c/p\u003e\u003cp\u003eTo help you better visualize this, think of a project that requires breaking down and shipping parts to different locations. Each part can be processed independently and reassembled once all pieces are completed. The driver program ensures these tasks are coordinated correctly, and executors handle each piece of the job.\u003c/p\u003e\u003cp\u003eBut what’s the relationship between \u003cstrong\u003ejobs\u003c/strong\u003e, \u003cstrong\u003etasks\u003c/strong\u003e, and \u003cstrong\u003estages\u003c/strong\u003e? Imagine you're the head of a large construction project, and you need to break down the overall project (the \u003cstrong\u003ejob\u003c/strong\u003e) into manageable tasks for each construction team. A \u003cstrong\u003etask\u003c/strong\u003e in Spark operates on a specific partition of data, much like a construction team focusing on a particular section of the building. Once a group of tasks that don’t depend on any other data is identified, Spark bundles them into \u003cstrong\u003estages\u003c/strong\u003e. A stage represents a set of tasks that can be executed independently without needing data from elsewhere in the project. However, sometimes a task will need information from another part of the dataset, requiring what’s known as a \u003cstrong\u003edata shuffle\u003c/strong\u003e. This shuffle is like coordinating deliveries between different construction teams—an operation that can slow things down due to the necessary data exchange, but essential for the overall completion of the job.\u003c/p\u003e\u003cp\u003eWhat happens when Spark needs to run in different environments? That’s where \u003cstrong\u003ecluster modes\u003c/strong\u003e come in. There are several ways Spark can be deployed, each suited to different use cases. The simplest is \u003cstrong\u003elocal mode\u003c/strong\u003e, ideal for testing on your own machine. Imagine local mode as running your city’s planning department with just one employee—the driver program manages everything on its own, without help from external workers. This is great for testing, but when you need real performance, it’s time to deploy Spark in a cluster.\u003c/p\u003e\u003cp\u003eIn a full cluster setup, Spark supports several modes. \u003cstrong\u003eSpark Standalone\u003c/strong\u003e is the quickest way to set up a cluster environment, ideal for smaller projects or when you want complete control over the infrastructure. For those already working in large-scale environments with Hadoop, \u003cstrong\u003eYARN\u003c/strong\u003e is a natural choice. It integrates seamlessly with the broader Hadoop ecosystem, making it easy to manage resources. For greater flexibility and the ability to handle dynamic workloads, \u003cstrong\u003eApache Mesos\u003c/strong\u003e comes into play, providing a more robust partitioning and scaling system.\u003c/p\u003e\u003cp\u003eBut if you’re looking for a modern, cloud-native approach, \u003cstrong\u003eKubernetes\u003c/strong\u003e offers powerful benefits. Running Spark on Kubernetes is like managing a city that can grow or shrink as needed, using containers to deploy and scale your Spark applications. With Kubernetes, Spark becomes highly portable, making it easy to run your Spark jobs in any cloud environment. You can set up your Spark application inside containers and have them scale automatically based on demand, ensuring smooth processing even as workloads increase.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eMastering the Art of Running Apache Spark Applications\u003c/h1\u003e\u003cp\u003eEver wondered how to launch your Apache Spark application into action? Whether you’re processing mountains of data or just running local tests, Apache Spark’s flexibility makes it incredibly powerful. But let’s be real—understanding how to run a Spark application might seem daunting at first. Fear not! Here’s your step-by-step guide to Spark mastery.\u003c/p\u003e\u003cp\u003eAt the heart of it all is the \u003ccode\u003espark-submit\u003c/code\u003e script. Think of it as Spark’s personal conductor, ensuring your application runs smoothly across a distributed cluster or right on your local machine. With \u003ccode\u003espark-submit\u003c/code\u003e, you’ve got full control: it lets you specify everything from the cluster manager you want to connect to, to how much memory and CPU cores your application needs. You can also include any additional files or libraries your app requires, making sure all the pieces are in place for a flawless run.\u003c/p\u003e\u003cp\u003eNow, let’s talk dependencies. In Spark, making sure the driver and executors have access to the right libraries is crucial. If you’re using Java or Scala, bundling all your code and libraries into a single uber-JAR (or fat JAR) is a common approach. This neat package ensures that everything is shipped out and accessible where it’s needed. For Python applications—aka PySpark—you’ll want to ensure that each node in your cluster has the exact same Python libraries installed. Imagine trying to run a marathon with mismatched shoes—it won’t end well, right? Same idea with your Spark dependencies.\u003c/p\u003e\u003cp\u003eIf you’re in the mood for a more hands-on, experimental approach, then the \u003cstrong\u003eSpark Shell\u003c/strong\u003e is your playground. This interactive tool lets you dive right into Spark with either Scala or Python, without needing to write and submit an entire application. When you fire up the Spark Shell, it automatically sets up everything for you—giving you instant access to Spark’s APIs. You can run quick computations, play around with datasets, and see the results in real-time, making it perfect for debugging or just satisfying your curiosity.\u003c/p\u003e\u003cp\u003eSo, to sum it all up: running an Apache Spark application is as easy as using \u003ccode\u003espark-submit\u003c/code\u003e to launch your code, bundling your dependencies into an uber-JAR (or ensuring Python libraries are ready), and—if you're feeling adventurous—jumping into the Spark Shell for some interactive magic. Spark truly puts the power of distributed computing at your fingertips.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe now trying to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose. In this lab, you will:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInstall a Spark Master and Worker using Docker Compose\u003c/li\u003e\u003cli\u003eCreate a python script containing a spark job\u003c/li\u003e\u003cli\u003eSubmit the job to the cluster directly from python (Note: you’ll learn how to submit a job from the command line in the Kubernetes Lab)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch2\u003eInstall a Apache Spark cluster using Docker Compose\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003egit clone https://github.com/big-data-europe/docker-spark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003echange to that directory and attempt to docker-compose up\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecd docker-spark\ndocker-compose up\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eAfter quite some time you should see the following message:\u003c/p\u003e\u003cp\u003e\u003cem\u003eSuccessfully registered with master spark://\u0026lt;server address\u0026gt;:7077\u003c/em\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch2\u003eCreate Code\u003c/h2\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, IntegerType, StringType\nsc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))\nsc.setLogLevel(\"INFO\")\nspark = SparkSession.builder.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\ndf = spark.createDataFrame(\n    [\n        (1, \"foo\"),\n        (2, \"bar\"),\n    ],\n    StructType(\n        [\n            StructField(\"id\", IntegerType(), False),\n            StructField(\"txt\", StringType(), False),\n        ]\n    ),\n)\nprint(df.dtypes)\ndf.show()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch2\u003e\u003cstrong\u003eExecute code / submit Spark job\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow we execute the python file we saved earlier.\u003c/p\u003e\u003cp\u003eIn the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.Now we execute the python file we saved earlier.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erm -r ~/.cache/pip/selfcheck/\npip3 install --upgrade pip\npip install --upgrade distro-info\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cp\u003ePlease enter the following commands in the terminal to download the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ewget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz \n\u0026\u0026 tar xf spark-3.3.3-bin-hadoop3.tgz \u0026\u0026 rm -rf spark-3.3.3-bin-hadoop3.tgz\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eRun the following commands to set up the\u0026nbsp;\u0026nbsp;which is preinstalled in the environment and\u0026nbsp;\u0026nbsp;which you just downloaded.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eexport JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\nexport SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003eInstall the required packages to set up the spark environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install pyspark\npython3 -m pip install findspark\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cp\u003eType in the following command in the terminal to execute the Python script.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epython3 submit.py\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cp\u003ego to port 8080 to see the admin UI of the Spark master\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-28\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fdemo-spark-submit.png?alt=media\u0026token=b8fe9a3e-6bcf-42f4-82a7-2b1842b11e74'/\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cp\u003e\u003cstrong\u003eLook at that!\u003c/strong\u003e You can now see all your registered workers (we’ve got one for now) and the jobs you’ve submitted (just one at the moment) through Spark's slick interface. Want to dig even deeper? You can access the worker’s UI by heading to port 8081 and see what’s happening under the hood!\u003c/p\u003e\u003cp\u003eIn this hands-on lab, you've set up your very own experimental Apache Spark cluster using Docker Compose. How cool is that? Now, you can easily submit Spark jobs directly from your Python code like a pro!\u0026nbsp;\u003c/p\u003e\u003cp\u003eBut wait, there’s more! In our next adventure—the Kubernetes lab—you’ll unlock the power of submitting Spark jobs right from the command line.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fspark.png?alt=media\u0026token=7a343f5c-0174-4a31-99e1-4943f9e135af","image_alt":"Apache Spark","slug":"mastering-apache-spark-an-engaging-dive-into-its-architecture-and-clusters","index":"6b86b273ff34f"},{"blog_id":"9f224db5-a76d-4a6f-9f7b-32bfbdf5696f","title":"MapReduce: The Magic Behind Processing Big Data in Hadoop","short_description":"Ever wondered how companies handle mountains of data efficiently? Enter MapReduce—Hadoop’s superhero when it comes to processing large datasets. Instead of one machine trying to handle everything, MapReduce breaks the work into smaller chunks and distributes it across many machines, making the process faster and more reliable.","timestamp":"2024-09-29 05:50:56","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eMapReduce: Powering Big Data Processing in Hadoop\u003c/h1\u003e\u003cp\u003eIn the world of Big Data, \u003cstrong\u003eMapReduce\u003c/strong\u003e stands as one of the key mechanisms for efficiently processing enormous datasets across distributed systems. It enables parallel processing of data across multiple computers within a cluster, making data handling at scale fast and reliable.\u003c/p\u003e\u003cp\u003eLet’s make it easier to grasp and dive into how MapReduce works in a more engaging, reader-friendly way.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eWhat Exactly is MapReduce?\u003c/h2\u003e\u003cp\u003eImagine you have a huge task that needs to be done, but you can split it up among several friends. Each of them works on a piece of the task, and when they’re done, someone else comes along to gather all the pieces, combine them, and finish the job. That’s essentially how MapReduce works in Hadoop.\u003c/p\u003e\u003cp\u003eIn simple terms:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eMap\u003c/strong\u003e: The data gets divided into smaller, manageable pieces, and each piece is processed independently. The goal here is to transform data into key-value pairs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReduce\u003c/strong\u003e: After the data is organized, the pieces with the same key are grouped together. Then, the reduce function steps in to summarize or aggregate the data to produce the final output.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis parallel processing is what makes MapReduce so powerful in handling large datasets efficiently\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch2\u003eSetting the Right Number of Mappers and Reducers\u003c/h2\u003e\u003cp\u003eWhile Hadoop does a lot of the work for you, it’s useful to know how mappers and reducers are assigned, because it can directly affect performance.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eHow Many Mappers?\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eAutomatically\u003c/strong\u003e: Hadoop typically assigns one mapper per block of data in HDFS (Hadoop Distributed File System). Each block is handled by one mapper.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eManually\u003c/strong\u003e: If needed, you can adjust this manually by configuring the \u003ccode\u003emapreduce.job.maps\u003c/code\u003e parameter in Hadoop settings. For example, if you want smaller or larger chunks of data per mapper, this is where you can tweak it.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e\u003cstrong\u003eHow Many Reducers?\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eManual Configuration\u003c/strong\u003e: Reducers are usually set by you, the developer, when writing the MapReduce job. You specify how many reducers are needed with the \u003ccode\u003emapreduce.job.reduces\u003c/code\u003e setting.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDynamic Adjustment\u003c/strong\u003e: Sometimes, Hadoop can adjust the number of reducers based on the data size, but setting it manually is common for optimizing performance.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eMore mappers or reducers can mean more parallel processing, but it’s important to find the right balance to avoid bottlenecks during the shuffle and sort phase (when data is grouped before reduction).\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch2\u003eThe MapReduce Workflow in Action\u003c/h2\u003e\u003cp\u003eLet’s look at a real-life example of how MapReduce works, step-by-step, using Hadoop. Let’s say you want to process a file called \u003ccode\u003ejar.txt\u003c/code\u003e. Here’s how it works:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eInput Data to HDFS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eYou (the client) upload the file \u003ccode\u003ejar.txt\u003c/code\u003e to Hadoop’s distributed file system (HDFS). Simple enough, but this is where the magic starts.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eJobTracker \u0026amp; NameNode – The Commanders\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe \u003cstrong\u003eJobTracker\u003c/strong\u003e (which coordinates all MapReduce jobs) reaches out to the \u003cstrong\u003eNameNode\u003c/strong\u003e (the master of HDFS) to find out where the file is stored. NameNode provides the metadata, explaining how the file will be divided into blocks and stored across DataNodes.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eData Replication for Safety\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFor reliability, Hadoop replicates each block of data across multiple DataNodes. So, your file doesn’t just exist in one place—it’s copied to ensure that, even if one node goes down, your data is still accessible.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eTaskTracker \u0026amp; Block Assignment\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eEach block of data is assigned to a \u003cstrong\u003eTaskTracker\u003c/strong\u003e running on a DataNode. These TaskTrackers are responsible for managing the mappers that will process the blocks. It’s like sending out your team of workers to handle different pieces of the puzzle.\u003c/p\u003e\u003ch3\u003e5. \u003cstrong\u003eMap Task – Processing Begins\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eThe TaskTrackers execute the map phase by processing the data blocks in parallel. Each mapper works on its own piece of data, transforming it into key-value pairs.\u003c/p\u003e\u003ch3\u003e6. \u003cstrong\u003eIntermediate Results Stored Locally\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce the map phase is complete, the intermediate results are saved locally on each DataNode. These results aren’t final yet—they still need to be combined.\u003c/p\u003e\u003ch3\u003e7. \u003cstrong\u003eReduce Task – Bringing It All Together\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNow, the \u003cstrong\u003eReduce\u003c/strong\u003e phase begins. TaskTrackers running the reduce task gather the intermediate results from all mappers. They group the data by key and aggregate it to produce the final output.\u003c/p\u003e\u003ch3\u003e8. \u003cstrong\u003eFinal Output Stored in HDFS\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eOnce the reducers finish their job, the final output is written back to HDFS. This means your processed data is now available in the distributed file system, ready for use.\u003c/p\u003e\u003ch3\u003e9. \u003cstrong\u003eJob Completion – Success!\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFinally, the JobTracker informs you that the job is complete, and you can access the results in HDFS. Mission accomplished!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch2\u003eWhy Should You Care About MapReduce?\u003c/h2\u003e\u003cp\u003eMapReduce simplifies the complex task of processing huge datasets by breaking it into smaller chunks and handling everything in parallel. It’s incredibly efficient and, thanks to data replication, fault-tolerant. Even if a node fails, your job won’t crash—the data is safe, and the job continues on other nodes.\u003c/p\u003e\u003cp\u003eSo, the next time you’re dealing with a massive dataset, just remember—MapReduce is like having an army of helpers, all working together to get the job done fast!\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmap_reduce.webp?alt=media\u0026token=9675cfce-94c9-4495-9f54-d6f651de19ff","image_alt":"Map reduce","slug":"mapreduce-the-magic-behind-processing-big-data-in-hadoop","index":"6b86b273ff34f"},{"blog_id":"2418e501-daff-473e-b29c-81ba9d65d596","title":"Understanding Hadoop Distributed File System (HDFS)","short_description":"HDFS, or Hadoop Distributed File System, is the backbone of Hadoop. It’s specially built to handle huge volumes of data by spreading it across multiple machines, making it perfect for big data tasks.","timestamp":"2024-09-29 05:29:46","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eHow HDFS Stands Out from Traditional File Systems\u003c/h1\u003e\u003cp\u003eHave you ever wondered how Hadoop handles those massive datasets? It’s not quite like how your typical computer saves files using NTFS or FAT32. Let's break it down!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is HDFS?\u003c/h1\u003e\u003cp\u003eHDFS, or Hadoop Distributed File System, is the backbone of Hadoop. It’s specially built to handle huge volumes of data by spreading it across multiple machines, making it perfect for big data tasks. If you’ve got terabytes or even petabytes of information, HDFS can manage it efficiently.\u003c/p\u003e\u003cp\u003eUnlike regular file systems like \u003cstrong\u003eNTFS\u003c/strong\u003e (which you might find on your Windows laptop), HDFS isn't just about saving files locally. Instead, it breaks your files into chunks (called blocks), and then spreads them out over several machines. This allows Hadoop to process multiple pieces of data at the same time. Pretty neat, right?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fblock_and_replication.jpg?alt=media\u0026token=7c465fa0-f2ed-444d-814a-1ab9c3e7c944'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eThe Magic of HDFS: Why Blocks and Replication Matter\u003c/h1\u003e\u003cp\u003eHere’s where HDFS gets really clever: it doesn’t just save your data in one place. It cuts files into large blocks, then duplicates (or replicates) those blocks across several machines. Why? So that if one machine crashes, your data isn’t lost. It’s all about fault tolerance.\u003c/p\u003e\u003cp\u003eThink of it like this: Imagine you’ve got a giant puzzle, but instead of keeping all the pieces in one box, you spread them across three different boxes. That way, even if you lose one box, you’ve still got enough pieces in the other two to complete the picture. That’s how replication in HDFS works. Each block is typically copied three times (though you can change that number if you need more or fewer backups).\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eComparing HDFS to NTFS\u003c/h1\u003e\u003cp\u003eNow, let’s compare this to NTFS. NTFS is a file system that works great for smaller files and doesn’t need to worry about distributing data over a network. When you save a file on your laptop, NTFS does its job locally—no fancy distribution happening here. If your hard drive fails, well, you better hope you’ve got a backup.\u003c/p\u003e\u003cp\u003eOn the other hand, HDFS shines in situations where you're dealing with Big Data. Imagine trying to process a massive log file for an entire year across multiple servers. That’s when HDFS steps in to spread the workload across dozens or even hundreds of machines, making everything faster and more reliable.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch2\u003eHow Clusters Work in HDFS\u003c/h2\u003e\u003cp\u003eHDFS operates on a \u003cstrong\u003emaster-slave architecture\u003c/strong\u003e, which, despite the name, is simpler than it sounds. Picture a group project where one person (the master) assigns tasks, and the rest (the slaves) carry them out. In Hadoop’s case, the \u003cstrong\u003eNameNode\u003c/strong\u003e is the master that keeps track of which machines (or \u003cstrong\u003eDataNodes\u003c/strong\u003e) are storing which blocks of data.\u003c/p\u003e\u003cp\u003eBut here's the cool part: even if one of the DataNodes goes offline, you won't lose your data. Thanks to replication, other copies of the data are safe on different nodes. The NameNode makes sure that if something goes wrong, the system automatically reassigns tasks to healthy machines. So, your project continues without missing a beat.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003ch2\u003eBlocks and Replication: The Heart of HDFS\u003c/h2\u003e\u003cp\u003eWhen you upload a file to HDFS, it doesn’t store the file in one piece. Instead, it breaks it into blocks—big ones, usually around 128 MB or 256 MB each (way bigger than what you'd see in NTFS or FAT32). These blocks are then distributed across multiple machines, and each block is typically replicated three times, just in case something goes wrong.\u003c/p\u003e\u003cp\u003eLet’s say you’ve uploaded a 1 GB file. HDFS would split that file into eight blocks, each around 128 MB. Then, it spreads these blocks out, saving them on different machines. And, for good measure, it replicates each block three times, storing those copies on different nodes. So even if one machine dies, the system still has two copies of every block.\u003c/p\u003e\u003cp\u003eThis is what makes HDFS so powerful—by distributing both the data and the risk of failure, it allows Hadoop to manage enormous datasets without constantly worrying about losing data due to hardware issues.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch2\u003eWhat Happens When You Write Data to HDFS?\u003c/h2\u003e\u003cp\u003eHere’s how the process goes when you write data to HDFS:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRequest\u003c/strong\u003e: You, or an application, send a request to HDFS to save some data.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCommunication\u003c/strong\u003e: HDFS, through the \u003cstrong\u003eNameNode\u003c/strong\u003e, checks where your data should be stored. It’s like asking the master project manager where to put each piece of the puzzle.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBreaking into Blocks\u003c/strong\u003e: The data is then broken down into blocks and sent to different DataNodes, which store those blocks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eReplication\u003c/strong\u003e: As each block gets stored, it's also replicated across other nodes. If one copy goes missing, the system can recreate it from another.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eConfirmation\u003c/strong\u003e: Once all blocks are safely stored and replicated, HDFS sends a confirmation, saying, “All good! Your data’s safely stored.”\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eThis process happens behind the scenes, but it's crucial to keeping your data safe and your big data applications running smoothly.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch2\u003eWhat Happens When a DataNode Fails in HDFS?\u003c/h2\u003e\u003cp\u003eImagine you're managing a vast library, and each book is split into multiple copies stored across different branches to ensure none of the information is lost. Now, what if one of the branches burns down? You wouldn't panic, right? Why? Because the other branches still hold copies. This is pretty much how Hadoop handles DataNode failures.\u003c/p\u003e\u003cp\u003eDataNodes store chunks of data, and it's inevitable that sometimes, things will go wrong. Maybe a server breaks down, or there's a network issue. But here’s the cool part: Hadoop has it all covered.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003eHow Does Hadoop Know a DataNode Failed?\u003c/h3\u003e\u003cp\u003eThink of it like a regular check-in system. Every few seconds, each DataNode sends a simple signal to the \u003cstrong\u003eNameNode\u003c/strong\u003e (the master controller) saying, “Hey, I’m here, and I’m working!” These signals are called \u003cstrong\u003eheartbeats\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eBut what if the NameNode doesn’t hear from a DataNode? If it misses several heartbeats in a row, the NameNode starts to get suspicious. After a certain time, it officially declares, “This DataNode is down!”\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003eWhat Happens Next?\u003c/h3\u003e\u003cp\u003eNow, this is where Hadoop shows off its resilience. The moment a DataNode fails, the NameNode updates its metadata. Basically, it marks all the data blocks that were on that failed node as unavailable.\u003c/p\u003e\u003cp\u003eBut remember—Hadoop doesn’t store just one copy of your data. It replicates blocks across multiple DataNodes. So, even though one DataNode has failed, the same data is safely stored on other DataNodes. The NameNode immediately gets to work, ensuring that all data remains fully backed up. It checks if any data blocks have dropped below the desired number of replicas (usually 3), and if so, it orders other healthy DataNodes to make fresh copies.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch3\u003eBringing a DataNode Back to Life\u003c/h3\u003e\u003cp\u003eLet’s say the failed DataNode eventually comes back online. It’s not like Hadoop just forgets about it. First, the NameNode checks whether the data stored there is still relevant. If the system has already made new replicas of those blocks, it might decide to remove the old, now-duplicated data to free up space.\u003c/p\u003e\u003cp\u003eOnce the DataNode starts sending its heartbeats again, it gets reintegrated into the system, and life goes on as if nothing happened. It’s like having a team member who took a break and is now ready to get back to work!\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eIn a nutshell, when a DataNode fails, Hadoop's architecture ensures there's no reason to worry. The NameNode quickly detects the failure, handles it behind the scenes by replicating data to other nodes, and everything continues running smoothly without missing a beat! Pretty smart, right?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch2\u003eWrapping Up\u003c/h2\u003e\u003cp\u003eHDFS might seem complex at first, but once you break it down, it’s really all about efficiency and reliability. By splitting files into blocks, distributing them across multiple machines, and replicating them for safety, HDFS ensures that your data is always accessible—even when things go wrong. So, if you're working with big datasets, HDFS is the perfect solution to keep everything running like clockwork.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fhdfs.jpg?alt=media\u0026token=b609034b-d178-4287-8ed3-4efb0dfa3249","image_alt":"HDFS","slug":"understanding-hadoop-distributed-file-system-hdfs","index":"6b86b273ff34f"},{"blog_id":"7a76b44a-8bc3-451e-95f0-3ccb1bc23e34","title":"Create Dashboard app with Electron.js","short_description":"In the world of application development, Electron.js has become a popular framework for building desktop applications using web technologies. In this blog, we will discuss how to create a desktop application template with Electron.js using React and Vite as the frontend framework, and IndexedDB as the default client-side database.","timestamp":"2024-09-20 14:06:27","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is Electron.js?\u003c/h1\u003e\u003cp\u003eElectron.js is an open-source framework that allows developers to build cross-platform desktop applications (Windows, macOS, and Linux) using web technologies like HTML, CSS, and JavaScript. Electron combines Chromium and Node.js so that developers can build desktop applications using the same technology they use for web development. I will discussed about the Modash project that i created too. So other people can use this template and ready to be launch as a producition app. This Modash project using the weird stack such as electron.js, React, Vite and IndexDb. i will tell you why i use this stack.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eWhy Use React and Vite?\u003c/h1\u003e\u003cp\u003eReact is a very popular JavaScript library for building user interfaces (UIs). By using React, we can create dynamic and interactive UI components and Vite is a build tool that provides speed and efficiency in frontend project development. Vite offers a fast development experience with hot module replacement (HMR) and a very fast build process.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eWhy IndexedDB?\u003c/h1\u003e\u003cp\u003eIndexedDB is a web-based database storage solution that provides larger storage compared to LocalStorage and SessionStorage. IndexedDB is ideal for applications that require large data storage on the client side, such as desktop applications built with Electron.js. we can use other database instead of this indexDb in my template. but for a default i used this indexDB to manage my account login. this is not  a good practice because indexDb can be access trough the Web interface and directly clear the dabatase data. but this can be prevent in prouction by blocking the user to inspect the devtools. Here is the code to prevent user inspect and ruined their production app:\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e if (process.env.NODE_ENV === \"development\") {\n    win.loadURL(\"http://localhost:6969/\");\n    win.webContents.openDevTools();\n  } else {\n    win.loadFile(path.join(__dirname, \"./dist/index.html\"));\n\n    // Remove the menu (including DevTools shortcuts)\n    win.setMenu(null);\n\n    // Disable context menu (right-click) that might have \"Inspect Element\"\n    win.webContents.on('context-menu', (e) =\u0026gt; {\n      e.preventDefault();\n    });\n\n    // Optionally, prevent any manual attempts to open DevTools\n    win.webContents.on('before-input-event', (event, input) =\u0026gt; {\n      if (input.key === 'I' \u0026\u0026 input.control \u0026\u0026 input.shift) {\n        event.preventDefault();\n      }\n    });\n\n    // Make sure DevTools are closed\n    win.webContents.on('did-finish-load', () =\u0026gt; {\n      win.webContents.closeDevTools();\n    });\n  }\n};\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eThis code snippet is part of an Electron application setup and handles different behaviors based on whether the application is running in development or production mode. In development mode, it loads the application from a local server (\u003ccode\u003ehttp://localhost:6969/\u003c/code\u003e) and opens the Developer Tools for debugging purposes. On the other hand, in production mode, it loads the application from a local HTML file, disables the menu, prevents the context menu from appearing (which might include the \"Inspect Element\" option), and disables the shortcut (\u003ccode\u003eCtrl+Shift+I\u003c/code\u003e) for opening the Developer Tools. Additionally, it ensures that the Developer Tools are closed after the application finishes loading, contributing to a cleaner user interface and enhanced security.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003eModash\u003c/h1\u003e\u003cp\u003eThis template is used to build the Dashboard application such as inventory list. sales, etc. the default template is using the React router dom for handling and manage the page. the starter kit for css framework i used is a Chakra-UI. You can visit this site to read Chakra-UI docs. \u003ca href=\"https://v2.chakra-ui.com/\" target=\"_blank\"\u003ehttps://v2.chakra-ui.com/\u003c/a\u003e. in this template i already built Home page. and the settings page. You can tweak and modify freely as you need if you want to build the dashboard app using this template. Here is the link of my github repository \u003ca href=\"https://github.com/Barbarpotato/Modash\" target=\"_blank\"\u003ehttps://github.com/Barbarpotato/Modash\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eElectron Problem with Production\u003c/h1\u003e\u003cp\u003eSometimes Electron.js has some trouble to delivered as a production app if you used the electron.js and integrating it wirh React environment. i have face some problems in my journey such as error when production if i sed react-router-dom, etc. to prevent react-router-dom crash during a production, you can use the \u0026lt;HashRouter\u0026gt; instead of \u0026lt;Browser Router\u0026gt;. this will solve the issue in electron production app. Here is the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport { HashRouter } from 'react-router-dom'\nimport { ChakraProvider } from '@chakra-ui/react'\nimport theme from './theme/theme.js'\nimport App from './App.jsx'\nimport './index.css'\nimport { AuthProvider } from './hooks/useAuth.jsx'\n\nReactDOM.createRoot(document.getElementById('root')).render(\n  \u0026lt;ChakraProvider theme={theme}\u0026gt;\n    \u0026lt;HashRouter\u0026gt;\n      \u0026lt;AuthProvider\u0026gt;\n        \u0026lt;App /\u0026gt;\n      \u0026lt;/AuthProvider\u0026gt;\n    \u0026lt;/HashRouter\u0026gt;\n  \u0026lt;/ChakraProvider\u0026gt;\n)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003e\u003ccode\u003eHashRouter\u003c/code\u003e is a type of router provided by React Router DOM, a popular routing library for React applications. The \u003ccode\u003eHashRouter\u003c/code\u003e uses the hash portion of the URL (the part after the \u003ccode\u003e#\u003c/code\u003e symbol) to keep the UI in sync with the URL. This makes it suitable for applications that need to be deployed to servers that don't handle dynamic requests, such as GitHub Pages.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003eProduction Mode\u003c/h1\u003e\u003cp\u003eMy Template is very ready about hearing the produciton mode. Dont worry about that. if you not optimis about my template during in production environment. You can clone my repository and attempt to make it as a production app. Go to a terminal and type \u003cstrong\u003enpm run prod. \u003c/strong\u003eThis will make 2 folders name dist and out folder. where the dist folder is the build package for the react and the out folder is the application that you are going to install in your device. Here is the package.json file you can look and learn about the template environment:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e{\n  \"name\": \"modash-desktop-app\",\n  \"productName\": \"Modash\",\n  \"description\": \"Modification Dashboard Dekstop App for inventory management purpose\",\n  \"private\": true,\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"main\": \"electron.cjs\",\n  \"scripts\": {\n    \"dev\": \"cross-env NODE_ENV=development concurrently \\\"npm:serve\\\" \\\"npm:electron\\\"\",\n    \"prod\": \"npm run build \u0026\u0026 npm run electron-build\",\n    \"serve\": \"vite\",\n    \"build\": \"cross-env NODE_ENV=production vite build\",\n    \"preview\": \"vite preview\",\n    \"electron\": \"wait-on tcp:6969 \u0026\u0026 electron .\",\n    \"electron-build\": \"cross-env NODE_ENV=production electron-builder\"\n  },\n  \"build\": {\n    \"appId\": \"electron-react-vite\",\n    \"mac\": {\n      \"icon\": \"public/download.ico\"\n    },\n    \"win\": {\n      \"target\": [\n        \"nsis\"\n      ],\n      \"icon\": \"public/download.ico\"\n    },\n    \"nsis\": {\n      \"oneClick\": false,\n      \"allowToChangeInstallationDirectory\": true,\n      \"installerIcon\": \"public/download.ico\",\n      \"uninstallerIcon\": \"public/download.ico\",\n      \"uninstallDisplayName\": \"electron-react-vite\"\n    },\n    \"directories\": {\n      \"output\": \"out\"\n    },\n    \"files\": [\n      \"dist/**/*\",\n      \"electron.cjs\",\n      \"electron/**\"\n    ]\n  },\n  \"dependencies\": {\n    \"@chakra-ui/react\": \"^2.8.2\",\n    \"@emotion/react\": \"^11.11.4\",\n    \"@emotion/styled\": \"^11.11.5\",\n    \"framer-motion\": \"^11.2.6\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"react-icons\": \"^5.2.1\",\n    \"react-router-dom\": \"^6.23.1\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"^18.0.28\",\n    \"@types/react-dom\": \"^18.0.11\",\n    \"@vitejs/plugin-react\": \"^4.0.0\",\n    \"builder\": \"^5.0.0\",\n    \"concurrently\": \"^8.0.1\",\n    \"cross-env\": \"^7.0.3\",\n    \"electron\": \"^24.3.1\",\n    \"electron-builder\": \"^24.13.3\",\n    \"eslint\": \"^8.38.0\",\n    \"eslint-plugin-react\": \"^7.32.2\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"eslint-plugin-react-refresh\": \"^0.3.4\",\n    \"vite\": \"^4.3.2\",\n    \"wait-on\": \"^7.0.1\"\n  }\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FModash-Display.png?alt=media\u0026token=3b3d3eef-37e4-4c37-91a1-80394b91069e'/\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eYou need to install your production applicaiton and once installed you can look the first page will display the login page. You can modify this style as you like. but the default style is just like it. Enjoy Use this template and you can build this with fast. and promises production environment ?.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Felectron.png?alt=media\u0026token=46d654ee-c016-41d2-ab1d-38d5653638a6","image_alt":"Electron.js","slug":"create-dashboard-app-with-electronjs","index":"6b86b273ff34f"},{"blog_id":"bc2c576f-29ce-4513-ab68-b1c336eb9e90","title":"Introduction to Elastic Search","short_description":"Elasticsearch is a powerful and scalable search engine built on Apache Lucene, commonly used for full-text search, data analysis, and log management.","timestamp":"2024-09-20 14:05:37","description":"\u003cdiv id=\"content-2\"\u003e\u003cp\u003eElasticsearch is a powerful search and analytics engine that is often used in conjunction with various architectural patterns, including CQRS (Command Query Responsibility Segregation). Here’s an overview of why Elasticsearch can be particularly useful in the context of CQRS:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eWhy Use Elasticsearch with CQRS Architecture?\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003e\u003cstrong\u003e1. Separation of Concerns\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eIn CQRS, the system is divided into two parts:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCommand Side\u003c/strong\u003e: Handles writes and updates to the system.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eQuery Side\u003c/strong\u003e: Handles reads and queries from the system.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eElasticsearch fits naturally into the Query Side of CQRS. It’s designed for fast searches and complex queries, which aligns perfectly with the need for efficient read operations in CQRS. By using Elasticsearch, you can offload complex search and analytics queries from your primary database, allowing it to focus on handling writes and updates.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e2. Scalability\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch is built to handle large volumes of data and high query loads. This makes it an excellent choice for the Query Side in CQRS where you might need to execute complex searches, aggregations, and filtering on large datasets. Elasticsearch’s distributed nature allows it to scale horizontally, handling increasing amounts of read traffic without a significant performance hit.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e3. High Performance\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch is optimized for search and analytics, providing low-latency responses even for complex queries. It uses inverted indexing to quickly retrieve relevant documents, which is beneficial for applications requiring fast, real-time search capabilities. This performance advantage is critical in the Query Side of CQRS, where read operations need to be efficient and responsive.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003e4. Flexibility in Querying\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eElasticsearch supports a wide range of query types, including full-text search, filtering, faceting, and aggregations. This flexibility allows you to perform complex and nuanced queries that are often needed for analytics and reporting. In CQRS, where the Query Side might need to provide various views and insights from the data, Elasticsearch can cater to these diverse querying needs.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media\u0026token=6eab7b0b-37d8-49f2-9137-27dadd766c96'/\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003e\u003cstrong\u003eElasticsearch Configuration\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eBy default, Elasticsearch comes with a feature called X-Pack, which is an additional plugin provided by Elastic. However, since X-Pack is not open-source, you can disable it if it's not needed. All Elasticsearch configurations can be found in the \u003ccode\u003econfig/elasticsearch.yml\u003c/code\u003e file. Elasticsearch uses YAML format for its configurations, making it easy to manage.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003e\u003cstrong\u003eRunning Elasticsearch\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTo run Elasticsearch, simply open a terminal and execute the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e./bin/elasticsearch\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eOnce it's running, Elasticsearch will be accessible on port 9200 according to the \u003ccode\u003ehttp.port\u003c/code\u003e setting in the configuration file. To stop the Elasticsearch application, you can use the \u003ccode\u003eCtrl + C\u003c/code\u003e key combination.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePUT /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cp\u003eThe rules for an index name are: it must be lowercase, cannot contain special characters except for \u003ccode\u003e-\u003c/code\u003e, \u003ccode\u003e+\u003c/code\u003e, and \u003ccode\u003e_\u003c/code\u003e (and these cannot be at the beginning), and it cannot exceed 255 bytes.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch2\u003e\u003cstrong\u003eElasticsearch Client\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eElasticsearch communicates using RESTful API, which means we can use HTTP to interact with it. This makes Elasticsearch very flexible and easy to learn, as well as easy to integrate with other applications.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eFlexible Schema\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eUnlike relational databases, Elasticsearch allows you to insert data into an index without needing to define the schema first. This schema is very flexible, but once it's established, you cannot change the data type of existing fields; you can only add new attributes. For example, if you create an \u003ccode\u003eage\u003c/code\u003e attribute with a \u003ccode\u003enumber\u003c/code\u003e type, you cannot change it to a \u003ccode\u003estring\u003c/code\u003e type later on.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003ePrimary Key in Elasticsearch\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWhen creating a document in Elasticsearch, you are required to include a primary key or ID. Unlike relational databases, in Elasticsearch, the primary key must use the \u003ccode\u003e_id\u003c/code\u003e field and can only consist of a single field with a string type.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eInteracting with Elasticsearch Using API\u003c/strong\u003e\u003c/h2\u003e\u003ch3\u003eCreating an Index\u003c/h3\u003e\u003cp\u003eIn Elasticsearch, there is no concept of a database like in RDBMS. You can directly create an index (similar to a table in a database). A common practice is to use the application name as a prefix for the index name, such as \u003ccode\u003emyapp_users\u003c/code\u003e. This prevents index name conflicts when Elasticsearch is used for multiple applications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePUT /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThe rules for an index name are: it must be lowercase, cannot contain special characters except for \u003ccode\u003e-\u003c/code\u003e, \u003ccode\u003e+\u003c/code\u003e, and \u003ccode\u003e_\u003c/code\u003e (and these cannot be at the beginning), and it cannot exceed 255 bytes.\u003c/p\u003e\u003ch3\u003eDeleting an Index\u003c/h3\u003e\u003cp\u003eTo delete an index, you can use the DELETE HTTP method. Deleting an index will automatically remove all data within it.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eDELETE /index_name\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch3\u003eAdding Data with Create API\u003c/h3\u003e\u003cp\u003eTo add data to Elasticsearch, you can use the Create API. This API is safe, meaning if the document with the specified \u003ccode\u003e_id\u003c/code\u003e doesn't exist, it will be saved as a new document. However, if it already exists, a conflict error will occur.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_create/id\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch3\u003eRetrieving Data with Get API\u003c/h3\u003e\u003cp\u003eAfter saving data, you can retrieve it using the Get API. This API returns the data along with its metadata, such as \u003ccode\u003e_id\u003c/code\u003e, index name, document version, etc.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eGET /index_name/_doc/id\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003ch3\u003eMulti Get API\u003c/h3\u003e\u003cp\u003eElasticsearch also provides a Multi Get API to retrieve multiple documents at once. This is useful when you need to fetch data from multiple indexes in a single API call.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /_mget\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003ch3\u003eSearching Data with Search API\u003c/h3\u003e\u003cp\u003eTo search for documents without using \u003ccode\u003e_id\u003c/code\u003e, you can use the Search API. This is a very powerful and complex API that allows you to perform highly specific queries.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_search\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003ch3\u003ePagination and Sorting\u003c/h3\u003e\u003cp\u003eThe Search API also supports pagination with the \u003ccode\u003efrom\u003c/code\u003e parameter to specify the starting document and the \u003ccode\u003esize\u003c/code\u003e parameter to specify the number of documents in the response. Additionally, you can sort the search results using the \u003ccode\u003esort\u003c/code\u003e parameter.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ePOST /index_name/_search\n{\n  \"from\": 0,\n  \"size\": 10,\n  \"sort\": [\n    { \"field_name\": \"asc\" }\n  ]\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eElasticsearch is a powerful and flexible tool for search and data analysis. With a basic understanding of configuration, running the application, and interacting with it using APIs, you can start leveraging Elasticsearch for various use cases such as full-text search, log management, and more. In the next article, we'll dive deeper into optimization and advanced features in Elasticsearch.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FElastic-Search.webp?alt=media\u0026token=419c2bf2-cb3e-4582-a48f-eec9da861459","image_alt":"Intro Elastic search","slug":"introduction-to-elastic-search","index":"6b86b273ff34f"},{"blog_id":"4e309d73-dc31-47b8-816a-901fd092368d","title":"Integrate Mysql to Flask","short_description":"This lab will cover the installation and usage of the flask_mysqldb library. It will include instructions on installing the library, initializing the database in app.py, and providing examples of how to post data and read data from the database.","timestamp":"2024-09-20 11:57:42","description":"\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eIn this article, we will explore how to integrate MySQL with a Flask application using the\u0026nbsp;\u003ccode\u003eflask_mysqldb\u003c/code\u003e\u0026nbsp;library. This integration allows us to efficiently manage database operations within our Flask web applications. In this lab we will asumme that we are already know to installed the flask application. and we will skip ahead and jump to the sql connection.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eThere is a Prerequisites to running this operations:\u003c/p\u003e\u003cp\u003eBefore starting, ensure you have the following installed:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePython (3.x recommended)\u003c/li\u003e\u003cli\u003eFlask\u003c/li\u003e\u003cli\u003eMySQL Server\u003c/li\u003e\u003cli\u003e\u003ccode\u003eflask_mysqldb\u003c/code\u003e\u0026nbsp;library (\u003ccode\u003epip install flask-mysqldb\u003c/code\u003e)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf we already fulfilled the prerequisites, we will going to install the mysql to the flask application:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install flask-mysqldb\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003eOnce the installation is completed, we will initiate all the MySql configuration. in this lab we will going to initiate the configurations file trough the \u003cstrong\u003e\u003cu\u003eapp.py\u003c/u\u003e\u003c/strong\u003e file. This is not recommended if the application continue grows to large scale. but for this demonstration, we are going to do that.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch1\u003eConfigure the MYSQL Connections\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom flask import Flask, request, jsonify\nfrom flask_mysqldb import MySQL\n\napp = Flask(__name__)\napp.config['MYSQL_HOST'] = 'localhost'\napp.config['MYSQL_USER'] = 'username'\napp.config['MYSQL_PASSWORD'] = 'password'\napp.config['MYSQL_DB'] = 'database_name'\n\nmysql = MySQL(app)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003ein the app.py, im importing the flask_mysqldb that we are already installed so we can use it in the app.py file later. after import the module. initiate the mysql configuration just like above. fill the neccessary field such as the password for db, the host user, etc. depends non your localhost mysql server.\u003c/p\u003e\u003cp\u003eit will be good if we try to running the flask server application and there is no anomaly when we are running it. when there is nothing error when we are running the flask server. that means that out mysql connection configurations form the flask is successful. and we will continue to do some basic mysql operations in this flask server.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003ch1\u003eCREATE OPERATIONS\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.route('/insert', methods=['POST'])\ndef insert():\n    cur = mysql.connection.cursor()\n    data = request.get_json()\n    name = data['name']\n    age = data['age']\n    cur.execute(\"INSERT INTO users (name, age) VALUES (%s, %s)\", (name, age))\n    mysql.connection.commit()\n    cur.close()\n    return jsonify({'message': 'Data inserted successfully'})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eThis code snippet defines a route in a Flask application (\u003ccode\u003e/insert\u003c/code\u003e) that handles POST requests. When a POST request is made to this endpoint, the function \u003ccode\u003einsert()\u003c/code\u003e is executed.\u003c/p\u003e\u003cp\u003eInside \u003ccode\u003einsert()\u003c/code\u003e, it establishes a connection to the MySQL database using \u003ccode\u003eflask_mysqldb\u003c/code\u003e, extracts JSON data containing \u003ccode\u003ename\u003c/code\u003e and \u003ccode\u003eage\u003c/code\u003e from the request body, and inserts this data into the \u003ccode\u003eusers\u003c/code\u003e table using an SQL \u003ccode\u003eINSERT\u003c/code\u003e statement.\u003c/p\u003e\u003cp\u003eAfter committing the transaction to the database and closing the cursor, it returns a JSON response confirming successful data insertion. This functionality demonstrates how to integrate MySQL database operations seamlessly into a Flask web application for handling data input.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003ch1\u003eREAD OPERATIONS\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e@app.route('/users', methods=['GET'])\ndef users():\n    cur = mysql.connection.cursor()\n    cur.execute(\"SELECT * FROM users\")\n    results = cur.fetchall()\n    cur.close()\n    users_list = []\n    for row in results:\n        user = {\n            'id': row[0],\n            'name': row[1],\n            'age': row[2]\n        }\n        users_list.append(user)\n    return jsonify({'users': users_list})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eLet brak down the code piece by piece!\u003c/p\u003e\u003cp\u003eSo This code snippet defines a route in a Flask application (\u003ccode\u003e/users\u003c/code\u003e) that handles GET requests. When a GET request is made to this endpoint, the function\u0026nbsp;\u003ccode\u003eusers()\u003c/code\u003e\u0026nbsp;is executed.\u003c/p\u003e\u003cp\u003eInside\u0026nbsp;\u003ccode\u003eusers()\u003c/code\u003e, it establishes a connection to the MySQL database using\u0026nbsp;\u003ccode\u003eflask_mysqldb\u003c/code\u003e, executes an SQL\u0026nbsp;\u003ccode\u003eSELECT\u003c/code\u003e\u0026nbsp;query to fetch all records from the\u0026nbsp;\u003ccode\u003eusers\u003c/code\u003e\u0026nbsp;table, and fetches all results using\u0026nbsp;\u003ccode\u003ecur.fetchall()\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eAfter retrieving the results, it closes the database cursor to release resources. Then, it iterates through the fetched results to construct a list of dictionaries (\u003ccode\u003eusers_list\u003c/code\u003e), where each dictionary represents a user with keys\u0026nbsp;\u003ccode\u003eid\u003c/code\u003e,\u0026nbsp;\u003ccode\u003ename\u003c/code\u003e, and\u0026nbsp;\u003ccode\u003eage\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eFinally, it returns a JSON response containing the list of users in the format\u0026nbsp;\u003ccode\u003e{'users': users_list}\u003c/code\u003e. This functionality demonstrates how to retrieve and format data from a MySQL database using Flask, making it accessible via an API endpoint. Very Simple is in it?\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003eBy following these steps, you can effectively integrate MySQL into your Flask applications using \u003ccode\u003eflask_mysqldb\u003c/code\u003e. This setup enables you to perform essential database operations seamlessly within your web development projects.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmysql_flask.png?alt=media\u0026token=9890d7a0-79cc-4ed1-beae-0b916b9f72e1","image_alt":"SQL + Flask","slug":"integrate-mysql-to-flask","index":"6b86b273ff34f"},{"blog_id":"e27f5bbc-f1be-4013-89da-312a09e6f3c0","title":"Search Engine Optimization","short_description":"SEO stands for “search engine optimization.” In simple terms, SEO means the process of improving your website to increase its visibility in Google, Microsoft Bing, and other search engines whenever people search for: Products you sell. Services you provide.","timestamp":"2024-09-20 11:37:18","description":"\u003ch1\u003eOptimizing Your Content for Top-notch SEO\u003c/h1\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\u003cp\u003eSEO (Search Engine Optimization) is a crucial practice in the digital world to enhance the visibility and ranking of your web pages on search engines like Google. By understanding the fundamental principles of SEO, you can increase the likelihood of your content appearing in search results, attract more visitors, and enhance your online presence.\u003c/p\u003e\u003ch2\u003e1. Understanding Keywords\u003c/h2\u003e\u003cp\u003eThe key to SEO lies in keywords or keyword phrases that potential visitors use when searching for information on search engines. Conduct keyword research to identify terms relevant to your business or topic. Utilize tools like Google Keyword Planner to determine keywords with high search volume.\u003c/p\u003e\u003ch2\u003e2. High-Quality Content\u003c/h2\u003e\u003cp\u003eSearch engines prioritize content that provides value to users. Ensure your content is informative, relevant, and meets the needs of your audience. Use engaging writing styles and avoid duplicate content. High-quality content not only captivates readers but is also favored by search engines.\u003c/p\u003e\u003ch2\u003e3. SEO-Friendly URL Structure\u003c/h2\u003e\u003cp\u003eIt's crucial to have URLs that are easily understood by both humans and search engines. Use a clear and descriptive URL structure that is concise and relevant to the content. For example: \u003ccode\u003ewww.examplewebsite.com/seo-article\u003c/code\u003e.\u003c/p\u003e\u003ch2\u003e4. Heading and Subheading Usage\u003c/h2\u003e\u003cp\u003eBreak down your content into easily digestible sections using headings (H1, H2, H3, etc.) and subheadings. This helps search engines understand the structure and hierarchy of your content. Make sure to include keywords in headings to provide additional signals to search engines.\u003c/p\u003e\u003ch2\u003e5. Image Optimization\u003c/h2\u003e\u003cp\u003eImages can enhance the visual appeal of your content but also require SEO optimization. Ensure each image has a descriptive alt attribute containing relevant keywords. Image file sizes should also be minimized to speed up page loading times.\u003c/p\u003e\u003ch2\u003e6. Quality Backlinks\u003c/h2\u003e\u003cp\u003eGetting backlinks from high-quality websites can boost your authority and SEO ranking. Build partnerships with other websites in your industry and engage in mutually beneficial link exchanges.\u003c/p\u003e\u003ch2\u003eConclusion\u003c/h2\u003e\u003cp\u003eOptimizing your content for SEO requires attention to detail and a commitment to providing added value to users. By implementing proper SEO practices, you can improve search engine rankings, attract more traffic, and strengthen your online presence.\u003c/p\u003e","image":"https://itbox.id/wp-content/uploads/2023/02/SEO-1_11zon.jpg","image_alt":"SEO Cover Image","slug":"search-engine-optimization","index":"6b86b273ff34f"},{"blog_id":"14d690d3-2201-43e4-b936-e06564ab67e2","title":"React PWA Fundamental","short_description":"A Progressive Web App is a type of web application that combines the best features of web and mobile applications. PWAs can be accessed through a web browser but can also be installed on a user's device like traditional mobile apps.","timestamp":"2024-09-20 11:37:12","description":"\u003ch1\u003eReact PWA Fundamental\u003c/h1\u003e\u003cp\u003eA Progressive Web App is a type of web application that combines the best features of web and mobile applications. PWAs can be accessed through a web browser but can also be installed on a user's device like traditional mobile apps. Some key features of PWAs include:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe ability to work offline\u003c/li\u003e\u003cli\u003eFast performance\u003c/li\u003e\u003cli\u003eResponsive display on various devices\u003c/li\u003e\u003cli\u003eAccess through an icon on the device's home screen\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ein this case we will be focused on how to make our react applicaion running on offline, get the requested react module from cache instead from the server. we will be learn about how to cache the customs api fetch that can be store to our react application.\u003c/p\u003e\u003ch2\u003eWhat Are Service Workers?\u003c/h2\u003e\u003cp\u003eService Workers are a crucial part of PWAs. They are JavaScript files that run in the background and enable features like offline caching, push notifications, and background sync. Service Workers act as intermediaries between your web application and the network, allowing you to control how your PWA behaves in various scenarios.\u003c/p\u003e\u003ch2\u003eVITE React PWA\u003c/h2\u003e\u003cp\u003eIn Vite React Project, there is some special configuration needed to applied the Progressive Web App. Below is the step-by-step to configure the PWA.\u003c/p\u003e\u003ch3\u003eInstalling vite-plugin-pwa\u003c/h3\u003e\u003cp\u003eFirst we need to install the vite-plugin-pwa plugin, just add it to your project as a dev dependency:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install -D vite-plugin-pwa\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNote: to running the implementation of PWA, we need to to build our react vite project, then running the react vite project trough npm run preview. There is some extra configuration to make implementation of PWA running in development mode.\u003c/p\u003e\u003ch3\u003eConfiguring vite-plugin-pwa\u003c/h3\u003e\u003cp\u003eEdit your vite.config.js / vite.config.ts file and add the vite-plugin-pwa:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { defineConfig } from 'vite'\nimport { VitePWA } from 'vite-plugin-pwa'\nimport react from '@vitejs/plugin-react'\n\n// https://vitejs.dev/config/\nexport default defineConfig({\n  plugins: [\n    react(),\n    VitePWA({\n      registerType: 'autoUpdate',\n      workbox: {\n        globPatterns: ['**/*.{js,css,html,ico,png,svg}']\n      }\n    })\n  ],\n})\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eCache External Resources\u003c/h3\u003e\u003cp\u003eIf you have some additional resource like font and css, you must include them into the service workerpre-cache and so your application will work when offline. But in this scenario, we will trying to use some free-api named: https://jsonplaceholder.typicode.com. to fetch the data from it and then stored it to the cache browser. so it can be rendered to a front-end page without the network traffic. The implementation is very easy. we need to addd some property in workbox object named :runtimeCaching. Below the example of how to use it:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e runtimeCaching: [\n          {\n            urlPattern: ({ url }) =\u0026gt; {\n              return url.pathname.match('/posts/1')\n            },\n            handler: 'CacheFirst',\n            options: {\n              cacheName: 'api-cache',\n              cacheableResponse: {\n                statuses: [0, 200]\n              }\n            }\n          }\n        ]\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003eResult Excercise\u003c/h2\u003e\u003cp\u003eBelow is the result example of how the PWA can running the application without the network traffic and requested from the server.\u003c/p\u003e\u003cimg src='https://raw.githubusercontent.com/Barbarpotato/React-PWA-Fundamental/main/git-images/Result.png?token=GHSAT0AAAAAABT2NYLCZWUIB25VOK65BHRIZI6WNAA'/\u003e\u003ch2\u003eService Worker without PWA capabilities\u003c/h2\u003e\u003cp\u003eSometimes you don't need the full blown PWA functionality like offline cache and manifest file, but need simple custom Service Worker.\u003c/p\u003e\u003ch3\u003eSetup the Service Worker\u003c/h3\u003e\u003cp\u003eYou can first check the browsers are supporting the service worker by create the script like below:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e  \u0026lt;script\u0026gt;\n    if ('serviceWorker' in navigator) {\n      window.addEventListener('load', () =\u0026gt; {\n        navigator.serviceWorker.register('/src/serviceWorker.js').then((reg) =\u0026gt; {\n          console.log('Worker Registered!')\n        }).catch(err =\u0026gt; {\n          console.log('Error in service Worker', err)\n        })\n      })\n    }\n  \u0026lt;/script\u0026gt;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis this code is responsible for registering a service worker for your web application.\u003c/p\u003e\u003ch2\u003eOffline Caching\u003c/h2\u003e\u003cp\u003eIf the services worker is available (it whill show the Worker Registered in your broswer console). Now let's create the serviceWorker.js file in public directory. We can squeeze the serviceWorker file by create some eventlistener that installed some assests from server to Cache Storage. So the client is not calling the resource from the server anymore instead calling from the client browser cache data.\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eself.addEventListener('install', (event) =\u0026gt; {\n    event.waitUntil(\n        caches.open('PWA-Cache').then((caches) =\u0026gt; {\n            console.log('Opened Cache')\n            return caches.addAll([\n                './assets/react.svg',\n                '/vite.svg'\n            ])\n        })\n    )\n})\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe purpose of this install event handler is to cache these specified assets when the service worker is first installed. Once the assets are cached, they can be served from the cache even if the user is offline, providing offline access to these resources. This is a fundamental step in building Progressive Web Apps (PWAs) that work seamlessly offline.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eAfter installing all assest from the server to the client. we need to tell the browser that whenever we fetch the data, we need to check the browser cache data first before we calling the server resource. if the client request it is same as the data from a data cache browser, then just use the cache browser data. Below is the example of how the explanation above implemented in javascript:\u003c/p\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eself.addEventListener('fetch', (event) =\u0026gt; {\n    event.respondWith(\n        caches.match(event.request).then((response) =\u0026gt; {\n            if (response) {\n                // Cache hit, return the response\n                return response;\n            }\n            // Not found in cache, fetch from the network\n            return fetch(event.request);\n        })\n    );\n});\u003c/code\u003e\u003c/pre\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Freact-pwa.png?alt=media\u0026token=2b264f65-ddf9-4b4a-afa8-efb39cb13c3d","image_alt":"Progressive Web App","slug":"react-pwa-fundamental","index":"6b86b273ff34f"},{"blog_id":"9c490813-d5d6-4e27-8cc8-31f115046c0e","title":"GraphQL Fundamental","short_description":"GraphQL is a powerful query language for your API, and it provides a more efficient, powerful, and flexible alternative to the traditional REST API.","timestamp":"2024-09-20 11:37:06","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is GraphQL?\u003c/h1\u003e\u003cp\u003eGraphQL is a query language for your API, and it's designed to request and deliver exactly the data that a client needs. Unlike REST, where the server determines what data is returned, GraphQL puts the control in the hands of the client. This allows for more efficient data retrieval, reduces over-fetching and under-fetching, and enables clients to request multiple resources in a single query.\u003c/p\u003e\u003ch1\u003eHow Does GraphQL Work?\u003c/h1\u003e\u003cp\u003eGraphQL is based on a strong schema that defines the types and operations available in your API. Clients can request data by specifying what they need, and the server responds with only the requested data in a structured format (typically JSON). The server resolves the query by matching it to the available types and fields in the schema.\u003c/p\u003e\u003ch1\u003eSetting Up GraphQL with Node.js, Express, and graphql-express\u003c/h1\u003e\u003cp\u003eThis Content will guide you through the process of setting up a GraphQL server on the backend using Node.js, Express, and the graphql-express package. GraphQL is a powerful query language that allows you to request and deliver data with precision, and this setup will enable you to create a flexible API.\u003c/p\u003e\u003ch1\u003ePrerequisites\u003c/h1\u003e\u003cp\u003eBefore you get started, ensure you have the following prerequisites:\u003c/p\u003e\u003col\u003e\u003cli\u003eNode.js installed on your machine.\u003c/li\u003e\u003cli\u003eA basic understanding of JavaScript and Express.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eBasic Installation \u0026amp; Usage\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cul\u003e\u003cli\u003eCreate a New Node.js Project: If you don't already have a Node.js project, create a new directory for your project and run npm init to initialize a new Node.js project.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cul\u003e\u003cli\u003eInstall Dependencies: You'll need to install the required packages using npm. Run the following command to install Express, GraphQL, and graphql-express:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install express graphql express-graphql\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cul\u003e\u003cli\u003eCreate a Server File: Create a new JavaScript file (e.g., app.js) in your project directory.\u003c/li\u003e\u003cli\u003eImport Dependencies: In server.js, import the necessary packages and set up the Express app.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst express = require('express');\nconst { graphqlHTTP } = require('express-graphql');\nconst { buildSchema } = require('graphql');\n\nconst app = express();\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cp\u003eCreate Schema Folder and create schema.js file. In Schema file there is some component that you need to understand:\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eCreate GraphQL Object Type\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eObject Type is a fundamental building block used to define the structure of the data that can be queried from a GraphQL API. It represents a type of object that can be retrieved or manipulated through the API. Object Types play a crucial role in modeling the data and defining the shape of the response that clients can request. Below is the example of how to make Object Type of GraphQL:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst BookType = new GraphQLObjectType({\n    name: 'book',\n    fields: () =\u0026gt; ({\n        id: { type: GraphQLString },\n        name: { type: GraphQLString },\n        genre: { type: GraphQLString }\n    })\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch2\u003eCreate Entry Point to the GraphQL Schema\u003c/h2\u003e\u003cp\u003eQuery is an operation type in GraphQL used to read or retrieve data from the server. They are created using the RootQuery Object Type and include fields that can be accessed by clients. Below the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst RootQuery = new GraphQLObjectType({\n    name: 'RootQueryType',\n    fields: {\n        book: {\n            type: BookType,\n            args: { id: { type: GraphQLString } },\n            resolve(parent, args) {\n                // code to get data from db\n                return books.filter(object =\u0026gt; object.id === args.id)[0]\n            }\n        }\n    }\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThis code is defining a RootQuery type in GraphQL, which serves as the entry point for querying data in the schema.\u003c/p\u003e\u003col\u003e\u003cli\u003econst RootQuery = new GraphQLObjectType({ ... }): This line creates a new GraphQL Object Type called RootQuery. The RootQuery type is special in GraphQL, as it is the starting point for all read (query) operations. It defines the fields that clients can query from the root of the schema.\u003c/li\u003e\u003cli\u003ename: 'RootQueryType': This sets the name for the RootQueryType. In this case, it's named \"RootQueryType.\"\u003c/li\u003e\u003cli\u003efields: { ... }: Here, you define the available fields within the RootQuery. Each field represents a possible query that clients can make.\u003c/li\u003e\u003cli\u003ebook: { ... }: This defines a field called \"book\" within the RootQuery. Clients can use this field to query information about books.\u003c/li\u003e\u003cli\u003etype: BookType: The type field specifies the data type that will be returned by the \"book\" query. In this case, it's set to the BookType, indicating that when a client queries \"book,\" they will receive data structured according to the BookType.\u003c/li\u003e\u003cli\u003eargs: { id: { type: GraphQLString } }: The args field specifies the arguments that can be provided with the \"book\" query. In this case, there's one argument named \"id,\" which is of type GraphQLString. It means that clients need to provide an \"id\" when querying for a book.\u003c/li\u003e\u003cli\u003eresolve(parent, args) { ... }: The resolve function is where you specify how to fetch the actual data when a client makes a query for \"book.\"\u003c/li\u003e\u003cli\u003ereturn books.filter(object =\u0026gt; object.id === args.id)[0]: Within the resolve function, you see code to fetch the data. In this case, it's looking through an array called \"books\" to find a book with an \"id\" that matches the one provided by the client. The filter method is used to find the matching book, and [0] is added to return the first matching result. This result will be returned to the client in the shape of a BookType.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eFinally, the code exports a new GraphQLSchema instance with the RootQuery as the query root. This makes the book query available for use in your GraphQL API.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003emodule.exports = new GraphQLSchema({\nquery: RootQuery\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cul\u003e\u003cli\u003eSet Up GraphQL Middleware to your app.js: Use the graphqlHTTP middleware to create a GraphQL endpoint for your Express app.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.use('/graphql', graphqlHTTP({\n    schema: schema,\n    graphiql: true\n}))\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eby using the graphiql property to your middleware, the backend service will provide the graphql development interface that can be access for demo and simulate accessing the different variant of our http request.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cul\u003e\u003cli\u003eStart the Server: Start the Express server on a port of your choice.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eapp.listen(4000, () =\u0026gt; {\n    console.log(`Now Listening on Post 4000`)\n})\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cul\u003e\u003cli\u003eTesting Your Queries in GrapiQL: You can access the /graphql endpoint in your browser, and the interface will be like this:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/graphiql-example1.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003ch2\u003eList Type\u003c/h2\u003e\u003cp\u003eSo far we have already built the relationship between the book and who is the author from the book. Now we want to build the relationship between the author and the book. we want to know if some author are called trough the request, we want to know what books they are created. in other words we call it\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eone to many\u003c/code\u003e\u0026nbsp;relationship if we are on the relational database environment, which is one author can have many books they created. Below is the example of how to implement it in GraphQL:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e// we added additional object for the one to many relationship display purposes.\nconst books = [\n    { name: 'Name of thw Wind', genre: 'Fantasy', id: '1', authorId: '1' },\n    { name: 'The Final Empire', genre: 'Fantasy', id: '2', authorId: '2' },\n    { name: 'The Long Earth', genre: 'Sci-Fi', id: '3', authorId: '3' },\n    { name: 'The Hero Of Ages', genre: 'Fantasy', id: '4', authorId: '2' },\n    { name: 'The Colourof Magic', genre: 'Fantasy', id: '5', authorId: '3' },\n    { name: 'The Loght Fantastic', genre: 'Fantasy', id: '6', authorId: '3' }\n];\n\nconst authors = [\n    { name: 'Patrick Bateman', age: 29, id: '1' },\n    { name: 'Bruce Wayne', age: 33, id: '2' },\n    { name: 'Peter Parker', age: 25, id: '3' }\n]\n\nconst AuthorType = new GraphQLObjectType({\n    name: 'author',\n    fields: () =\u0026gt; ({\n        id: { type: GraphQLID },\n        name: { type: GraphQLString },\n        age: { type: GraphQLInt },\n        book: {\n            type: new GraphQLList(BookType),\n            resolve(parent, args) {\n                return books.filter(object =\u0026gt; object.authorId === parent.id)\n            }\n        }\n    })\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cul\u003e\u003cli\u003eIn above code, dont forget that we are going to return multiple object from the book fields, which we are need the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eGraphQLList\u003c/code\u003e\u0026nbsp;imported from the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003egraphql\u003c/code\u003e\u0026nbsp;instance.\u003c/li\u003e\u003cli\u003ewe are returning the processed data from the authorType. This data will be processed again in resolve function book field.\u003c/li\u003e\u003cli\u003ethe result query if we success build this one to many relations:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/type-list.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003ch2\u003eAll Objects\u003c/h2\u003e\u003cp\u003eFor some cases, we need to return all list of books, or all list of author that we want to the client. To do this we just added some field in the\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eRootQuery\u003c/code\u003e\u0026nbsp;of our Schema:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst RootQuery = new GraphQLObjectType({\n    name: 'RootQueryType',\n    fields: {\n        ...,\n        ...,\n        books: {\n            type: new GraphQLList(BookType),\n            resolve(_parent, _args) {\n                return books\n            }\n        },\n        authors: {\n            type: new GraphQLList(AuthorType),\n            resolve(_parent, _args) {\n                return authors\n            }\n        }\n    }\n});\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003cp\u003eThe result output from the graphiql will be like this:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003cimg src='https://github.com/Barbarpotato/GraphQL-Fundamental/raw/main/images/all-objects.png'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgraphql.png?alt=media\u0026token=bdb93458-f903-44e3-bb96-0086a7f78a6e","image_alt":"Graph QL - Darma Labs","slug":"graphql-fundamental","index":"6b86b273ff34f"},{"blog_id":"978cd40e-18da-47a6-a4fe-917bec69dea1","title":"React Lazy Load","short_description":"React Lazy Load is a technique used to load React components lazily, i.e., only when they are needed. This helps reduce the initial bundle size and improve the performance of your React application. You can easily implement lazy loading in your React project using React.lazy() along with Suspense.","timestamp":"2024-09-20 11:36:59","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eReal-World Case\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cimg style='width:720px;' src='https://raw.githubusercontent.com/Barbarpotato/React-Lazy-Load/main/git-image/Scenario.png'/\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cp\u003eWhen building a dashboard application. This application has high complexity and requires a large size to be loaded by application users in the future. For example, the dashboard application has several different features, depending on the position of each application user. for example, users with the admin position have a different features in the application compared to users with other positions, for example Sales. When Sales enters a dashboard application. Of course, the features contained in admin will not appear for users with sales positions, BUT users with sales positions will still load the features contained in admin even though they are not used. Vice Versa, This affects the performance, speed and efficiency of an application running.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eSolution\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eTo asnwer the prbolem above, we will be use\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eLazy Load\u003c/code\u003e\u0026nbsp;technique, you can optimize the initial loading time and improve the performance of your React application by dynamically loading components as needed.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eUsage\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eHere are the steps on how to use React Lazy Load in your project:\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eCreate the routes for the web page.\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eWe have 3 different pags for this case, Main page which can be access for all the role position (Admin, Sales). we have the sales component which can be access by sales person, and the last page is the admin page, where it can only be access by the admin person.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { Routes, Route } from 'react-router-dom'\nimport Sales from './pages/Sales'\nimport Home from './pages/Home'\nimport Admin from './pages/Admin'\nimport './App.css'\n\nfunction App() {\n\n  return (\n    \u0026lt;Routes\u0026gt;\n      \u0026lt;Route index element={\u0026lt;Home /\u0026gt;} /\u0026gt;\n      \u0026lt;Route path='/sales' element={\u0026lt;Sales /\u0026gt;} /\u0026gt;\n      \u0026lt;Route path='/admin' element={\u0026lt;Admin /\u0026gt;} /\u0026gt;\n    \u0026lt;/Routes\u0026gt;\n  )\n}\n\nexport default App\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eIf we dont implement the lazy load technique\u0026nbsp;\u0026nbsp;our react application\u0026nbsp;it will be load\u0026nbsp;\u0026nbsp;three pages\u0026nbsp;which are Main, Sales\u0026nbsp;\u0026nbsp;the Admin page. If we implement the lazy load, whenever we access the Sales Page, it will be load the Main\u0026nbsp;\u0026nbsp;the Sales Page, Vice Versa\u003c/p\u003e\u003ch2\u003eApplied the Lazy Load to your React App\u003c/h2\u003e\u003cp\u003eAfter build the different page\u0026nbsp;\u0026nbsp;route, we can implement the lazy load by using the lazy\u0026nbsp;\u0026nbsp;Suspense\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport { Routes, Route } from 'react-router-dom'\nimport { lazy, Suspense } from 'react'\nimport Home from './pages/Home'\nimport './App.css'\n\nconst Admin = lazy(() =\u0026gt; import('./pages/Admin'))\nconst Sales = lazy(() =\u0026gt; import('./pages/Sales'))\n\nfunction App() {\n\n  return (\n    \u0026lt;Routes\u0026gt;\n\n      \u0026lt;Route index element={\u0026lt;Home /\u0026gt;} /\u0026gt;\n\n      \u0026lt;Route path='/sales' element={\n        \u0026lt;Suspense fallback={\u0026lt;div\u0026gt;Loading Content...\u0026lt;/div\u0026gt;}\u0026gt;\n          \u0026lt;Sales /\u0026gt;\n        \u0026lt;/Suspense\u0026gt;} /\u0026gt;\n\n      \u0026lt;Route path='/admin' element={\n        \u0026lt;Suspense fallback={\u0026lt;div\u0026gt;Loading Content...\u0026lt;/div\u0026gt;}\u0026gt;\n          \u0026lt;Admin /\u0026gt;\n        \u0026lt;/Suspense\u0026gt;} /\u0026gt;\n\n    \u0026lt;/Routes \u0026gt;\n  )\n}\n\nexport default App\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch2\u003e\u003cstrong\u003eImplementation Result\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eNow, if you go to the Dev Console -\u0026gt; Source. You need to check the page source file. and then check the src folder -\u0026gt; pages . We cannot see our Admin and Sales Component. If we accessing the admin route or the sales route, it will appear in the source dev console. which means that our lazy load implement successfully.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg style='width:720px;' src='https://raw.githubusercontent.com/Barbarpotato/React-Lazy-Load/main/git-image/lazy-load.png'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Flazy-load.png?alt=media\u0026token=94d1a55f-c578-455a-9623-b561272a9f99","image_alt":"Lazy Load React - Darma","slug":"react-lazy-load","index":"6b86b273ff34f"},{"blog_id":"c4973cc3-3c29-4fcb-9651-5d921b02eed2","title":"ETL using shell scripts","short_description":"In this lab, we perform ETL (Extract, Transform, Load) using shell scripts. Learn to extract data from delimited files, transform text data with cut and tr, and load it into a PostgreSQL database. This hands-on approach will gain essential skills for automating data management and integration.","timestamp":"2024-09-20 11:36:39","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is ETL?\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eETL\u003c/strong\u003e stands for \u003cstrong\u003eExtract, Transform, Load\u003c/strong\u003e. It is a crucial process in data management and integration, typically used in data warehousing and data analytics. Here's a brief overview:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExtract\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eThis step involves retrieving data from various sources such as databases, files, APIs, or other systems. The goal is to gather raw data for further processing.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eTransform\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eOnce the data is extracted, it often needs to be cleaned, formatted, and transformed to fit the desired structure or schema. This step may include filtering, sorting, aggregating, and converting data to ensure consistency and compatibility.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLoad\u003c/strong\u003e:\u003c/li\u003e\u003cli class=\"ql-indent-1\"\u003eThe final step is to load the transformed data into a target system, such as a database, data warehouse, or data lake. This makes the data available for querying, analysis, and reporting.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003ch1\u003eETL using Shell Scripts\u003c/h1\u003e\u003cp\u003eIn this lab we will covering about read and extract data from various delimited thing using shell command. We'll understand how to handle file input, parse the data, and prepare it for further processing. Master the techniques for transforming text data using powerful shell utilities like tr command. and finally load the data to the database. in this lab we will use PostgreSQL. \u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003e\u003cstrong\u003eExercise 1 - Extracting data using 'cut' command\u003c/strong\u003e\u003c/h1\u003e\u003ch2\u003eExtracting characters\u003c/h2\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"database\" | cut -c1-4\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eYou should get the string ‘data’ as output.\u003c/p\u003e\u003cp\u003eThe command below shows how to extract 5th to 8th characters.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"database\" | cut -c5-8\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eYou should get the string ‘base’ as output.\u003c/p\u003e\u003cp\u003eNon-contiguous characters can be extracted using the comma.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch2\u003eExtracting fields/columns\u003c/h2\u003e\u003cp\u003eWe can extract a specific column/field from a delimited text file, by mentioning\u003c/p\u003e\u003cul\u003e\u003cli\u003ethe delimiter using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003e-d\u003c/code\u003e\u0026nbsp;option, or\u003c/li\u003e\u003cli\u003ethe field number using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003e-f\u003c/code\u003e\u0026nbsp;option.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe /etc/passwd is a “:” delimited file.\u003c/p\u003e\u003cp\u003eThe command below extracts usernames (the first field) from /etc/passwd.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1 /etc/passwd\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cimg  style='width:600px; height:100%'src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_1.png?alt=media\u0026token=7a3e427e-002d-4d12-875f-8f8160cc984d'/\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003ch1\u003e\u003cstrong\u003eExercise 2 - Transforming data using 'tr'\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch2\u003eTranslate from one character set to another\u003c/h2\u003e\u003cp\u003eThe command below translates all lower case alphabets to upper case.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"Shell Scripting\" | tr \"[a-z]\" \"[A-Z]\" echo \"Shell Scripting\" | tr \"[:lower:]\" \"[:upper:]\" echo \"Shell Scripting\" | tr \"[A-Z]\" \"[a-z]\"\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch1\u003ePrepared the Database for the practice\u003c/h1\u003e\u003cp\u003eIn this exercise we will create a table called\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eusers\u003c/code\u003e\u0026nbsp;in the PostgreSQL database using PostgresSQL CLI. This table will hold the user account information.\u003c/p\u003e\u003cp\u003eThe table\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eusers\u003c/code\u003e\u0026nbsp;will have the following columns: uname, uid, home\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecreate table users(username varchar(50),userid int,homedirectory varchar(100));\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch1\u003eCreate the Shell Script File\u003c/h1\u003etouch csv2db.sh. Then Open the file in the editor. Copy and paste the following lines into the newly created file.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# This script # Extracts data from /etc/passwd file into a CSV file. # The csv data file contains the user name, user id and # home directory of each user account defined in /etc/passwd # Transforms the text delimiter from \":\" to \",\". # Loads the data from the CSV file into a table in PostgreSQL database.\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003eWe need to add lines of code to the script that will xtract user name (field 1), user id (field 3), and home directory path (field 6) from /etc/passwd file using the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003ecut\u003c/code\u003e\u0026nbsp;command. \u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1,3,6 /etc/passwd\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003ethen run the script. -\u0026gt; bash csv2db.sh\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cp\u003elet us change the last line of command so it not directly printed trough the terminal instead we will save the output to .txt file\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ecut -d\":\" -f1,3,6 /etc/passwd \u0026gt; extracted-data.txt\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-24\"\u003e\u003cimg style='width:720px; heigth:100%;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_2.png?alt=media\u0026token=3b31b28f-40bf-4f3e-952a-4d0607d133c6'/\u003e\u003c/div\u003e\u003cdiv id=\"content-25\"\u003e\u003cp\u003eThe extracted columns are separated by the original “:” delimiter. You need to convert this into a “,” delimited file. Add the below lines at the end of the script and save the file.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-26\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003etr \":\" \",\" \u0026lt; extracted-data.txt \u0026gt; transformed-data.csv\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-27\"\u003e\u003cp\u003eRun the script and then it automatically save the .csv file to your system. you can read it from the terimnal by calling this command: cat transformed-data.csv\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-29\"\u003e\u003ch3\u003eTo load data from a shell script, you will use the\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003epsql\u003c/code\u003e\u0026nbsp;client utility in a non-interactive manner. This is done by sending the database commands through a command pipeline to\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003epsql\u003c/code\u003e\u0026nbsp;with the help of\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eecho\u003c/code\u003e\u0026nbsp;command.\u003c/h3\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-30\"\u003e\u003cp\u003ePostgreSQL command to copy data from a CSV file to a table is\u0026nbsp;\u003ccode style=\"background-color: transparent; color: rgb(255, 153, 0);\"\u003eCOPY\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eThe basic structure of the command which we will use in our script is\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-31\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eCOPY table_name FROM 'filename' DELIMITERS 'delimiter_character' FORMAT;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho \"\\c template1;\\COPY users FROM '/home/project/transformed-data.csv' DELIMITERS ',' CSV;\" | psql --username=postgres --host=localhost\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eecho '\\c template1; \\\\SELECT * from users;' | psql --username=postgres --host=localhost\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-35\"\u003e\u003cimg style='width:720px; height:100%;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_script_img_3.png?alt=media\u0026token=ff23b79e-4971-4f41-9060-58b47f009358'/\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fetl_using_shell_scripts.png?alt=media\u0026token=e81bec9a-d988-429f-954c-eab34e3d0e11","image_alt":"ETL Using bash Script","slug":"etl-using-shell-scripts","index":"6b86b273ff34f"},{"blog_id":"0e1dd5fa-3510-411f-9dc9-85938efe6af9","title":"Working with Time \u0026 Space Complexity ","short_description":"In this reading, We will explore a worked example of a piece of code written in Python, along with how you would evaluate it using Big-O notation.","timestamp":"2024-09-20 11:35:52","description":"\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eBig-O notation\u003c/h2\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eEvaluating an application's performance ensures that the code written is good and fit for purpose. The question is how do we evaluate efficiency? When we measure electricity, we use kilowatt-hours, which means how many kilowatts an appliance will use if it runs for an hour. The appliance will not always run for an hour, and it may have different requirements depending on the setting used, it is more of a general rule-of-thumb for evaluating cost.\u003c/p\u003e\u003cp\u003eWhen evaluating coding solutions, Big-O notation is used. So, Big-O notation is the kilowatt hour of code evaluation. It can be applied to measuring how much time a piece of code will take or how much space it will use in memory. Not all processors will run at the same speed, so instead of timing an application, you count the number of instructions an application initiates.\u003c/p\u003e\u003cp\u003eWhich measurement reflects the quickest possible execution of some code? Let's explore which measurement reflects the quickest possible execution of some code.\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;You use a constant time algorithm that takes O(1) (O-of-one) time to compute. This determines that it will only take one computation to complete a task. An example of this is to print an item from an array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# An array with 5 numbers \narray = [0,1,2,3,4]\n\n# retrieve the number found at index location 3 \nprint(array[3]) \u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eIn this instance, no matter how many values exist in the array, the approach has a Big-O of one. This means that running this code is considered O(1).\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n)\u003c/code\u003e\u0026nbsp;Next, let's explore an example of O(n). Taking the same array, an if statement is written that looks for the number 5. To establish that 5 is not there, it has to check every item in the array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# An array with 5 numbers \narray = [0,1,2,3,4] \n\nif 5 in array:\n    print(\"five is alive\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# an array with 10 numbers \narray = [0,1,2,3,4,6,7,8,9,10]\n\nif 5 in array:\n    print(\"five is still alive\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(log n)\u003c/code\u003e\u0026nbsp;This search is less intensive than O(n) but more work than O(1). O(log n) is a logarithmic search and it will increase as new inputs are added but these inputs only offer marginal increases. An excellent example of this in action is a binary search. Binary search is covered in more detail later in the course.\u003c/p\u003e\u003cp\u003eNow, imagine playing a guessing game with the following prompts: too high, too low, or correct. You are given a range of 100 to 1. You may decide to approach the problem systematically. First, you guess 50 – too high. So, you guess 25 – which is too high. You may choose then to go 12 or 13. What is happening here is that you are halving the search space with each guess.\u003c/p\u003e\u003cp\u003eSo, while the input to this function was 100 using a binary search approach, you should come upon the answer in under 5 or 6 guesses. This solution would have a time complexity of O(log n). Even if n (the range of numbers entered) is ten times bigger. It will not take ten times as many guesses.\u003c/p\u003e\u003cp\u003eHere is a breakdown of those steps on the array.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003earray = [0,1,2,3,4,6,7,8,9,10]\n\nprint(\"##Step One\")\nprint(\"Array\")\nprint(array)\nmidpoint = int(len(array)/2)\nprint(\"the midpoint at step one is: \" , array[midpoint])\n\nprint()\n\nprint(\"##Step Two\")\narray = array[:midpoint] # 6 is the midpoint of the array \nprint(\"Array\")\nprint(array)\n# running this shows the numbers left to check \n# is 5 \u0026lt; 3 \n# no \n# so discard the left hand side \n\n# so the array is halved again \nmidpoint=int(len(array)/2)\nprint(\"the midpoint is: \",  array[midpoint])\n\nprint()\nprint(\"##Step Three\") \narray = array[midpoint:] # so the array is halved at the midpoint\nprint(array)# check for the midpoint \nmidpoint=int(len(array)/2)\nprint(\"the midpoint is: \" , array[midpoint])\n# is 4 \u0026lt; 5 \n# yes look to the right\n\nprint()\nprint(\"##Step Four\") \nprint(array[midpoint:]) \n# check for the midpoint \narray = array[midpoint:] # so the array is halved at the midpoint\nmidpoint=int(len(array)/2)\n\n\n\nprint()\nprint(\"##Step Five\") \narray = array[midpoint:] \nprint(array)\nprint(\"only one value to check and it is not 5\")\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cp\u003eYou will notice that to determine if 5 is present, it took 5 steps. That is a big-O score of O(5). You can see that this is bigger than O(1) but smaller than O(n). Now, what happens when the array is extended to 100? When looking for a number in an array of 10, it took 5 guesses. Looking at an array of 100 will not take 50 guesses; it will take no more than 10. Equally, if the list is extended to 1000, the guesses will only go up to 15-20.\u003c/p\u003e\u003cp\u003eFrom this, we can see that it is not O(1) because the answer is not immediate. It is not big-O(n) because the number of guesses does not go up with the size n of the array. So here, one says that the complexity is O(log(n)).\u003c/p\u003e\u003cp\u003eTo gain greater insight into how the log values are only a gradual rise, look at a log table up to 100,000,000. This lens shows that O(log n) incurs only a minimal processing cost. Running a binary search on an array with any n values will, in a worst-case scenario, always make the number of computations found in the log values column.\u003c/p\u003e\u003cp\u003e\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n^2)\u003c/code\u003e\u0026nbsp;is heavy on computation. This is quadratic complexity, meaning that the work is doubled for every element in the array. An excellent way to visualize this is to consider that you have a variety of arrays. In keeping with the earlier example, let's explore the following code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enew_array=[] # an array to hold all of the results \n# array with five numbers \narray = [0,1,2,3,4]\nfor i in range(len(array)): # the array has five values, so this is n=5 \n    for j in range(len(array)): # still the same array so n = 5 \n        new_array.append(i*j) # every computation made is stored here \n\nprint(len(new_array)) #how big is this new array ? \u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cp\u003eThe first loop wi ll equal the number of elements input, n. The second loop will also look at the number of input elements, n. So, the overall complexity of running this approach can be said to be n*n which is n^2 (n-squared). To find out how many computations were made, you have to print out the number of times n was used in the loop as below.\u003c/p\u003e\u003cp\u003eIf you know that the array has 25 elements, then you understand the principles of calculating Big-O notation. To further test your knowledge, how many computations would be required if n = 6? Meaning the array had 6 values? The answer is 6 x 6 so 36.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch1\u003eWorking with Space Complexity\u003c/h1\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eSome Algorithms like\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003ehash tables\u003c/code\u003e\u0026nbsp;provide very fast lookups in\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(10) time\u003c/code\u003e. However, to work efficiently, they must have a lookup every element stored. This results in a space complexity of O(n). The big O-notation for space complexity is the same as for the time O(1), O(log log n), O(log n) and so on. In all these notations\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003en\u003c/code\u003e\u0026nbsp;refers to the size of the input. This is often measured in bytes.\u003c/p\u003e\u003cp\u003eDifferent languages have different memory costs associated with them. In java for instance, an integer requires 4 bytes of memory. A blank array will consume 12 bytes for the header object and an additional 4 bytes for padding. Thus, if n refers to an array of integers size 4, then the total memory requirement is 32 bytes of memory.\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003e(4 * 4) + 12 + 4 = 32\u003c/code\u003e\u003c/p\u003e\u003cp\u003eWhen discussing space complexity, you have to consider what the increase in input size has on the overall usage. Space Compexity is the total of auxiliary space + input space The space complexity of a problem can be broken into two sections namely auxiliary and input space:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAuxiliary space is the space required to hold all data required for the solution. It refers to the temporary space needed to compute a given solution.\u003c/li\u003e\u003cli\u003eInput space refers to the space required to add data to the function, algorithm, application or system that you are evaluating.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst a = 1;\nconst b = 10;\nconst c = 100;\nconst sum = (a, b, c) =\u0026gt; a + b + c;\nconst d = sum(a, b, c);\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cp\u003eIn the above function sum, we can resolve the space complexity required by the memory requirement. Given that all arguments in the above example are integers and the return value is an integer of set size, we know this will be constant.\u003c/p\u003e\u003cp\u003eGiven the Number type in JavaScript is 64-bit (8 bytes), we can resolve that the memory requirement for a, b and c is (24). Therefore, the function sum has\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;constant space complexity given that we know the constant requirement of 24 bytes of data space for this function.\u003c/p\u003e\u003cp\u003eLooking at an example in C that requires a dynamic amount of memory in an array:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eint sum(int a[], int n) {\n    int x = 0;      // 4 bytes for x\n    for(int i = 0; i \u0026lt; n; i++)  // 4 bytes for i\n    {\n        x  = x + a[i];\n    }\n    return(x);\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003cp\u003eFirstly, we know that int types in C require 4 bytes of space. Here we note that 4*n bytes of space is required for the array a[] of integers. The remaining variables x, n, i and the return value each require a constant 4 bytes each of memory give that they are integers.\u003c/p\u003e\u003cp\u003eThis gives us a total memory requirement of (4n + 12). This itself is\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(n)\u003c/code\u003e\u0026nbsp;linear space complexity since the memory requires linearly increases with input value n.\u003c/p\u003e\u003cp\u003eWhat is important to note with this example is that the auxiliary space required for the above sum function is actually\u0026nbsp;\u003ccode style=\"background-color: var(--bgColor-neutral-muted, var(--color-neutral-muted));\"\u003eO(1)\u003c/code\u003e\u0026nbsp;constant given that the auxiliary variables are only x and i which totals a (8) memory requirement (constant).\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcoding_interview.png?alt=media\u0026token=1e7a8927-e196-4b59-a0bd-202c9d190484","image_alt":"Coding interview Preparation","slug":"working-with-time--space-complexity-","index":"6b86b273ff34f"},{"blog_id":"59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29","title":"Create a DAG for Apache Airflow with Python Operator","short_description":"In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI.","timestamp":"2024-09-20 11:35:46","description":"\u003cdiv id=\"content-0\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow is an open-source platform designed for orchestrating and managing complex workflows and data pipelines. It allows users to programmatically author, schedule, and monitor workflows through a rich web-based user interface. Airflow uses directed acyclic graphs (DAGs) to represent workflows, making it highly flexible and scalable for a wide range of use cases. With built-in support for dynamic pipeline generation, Airflow enables users to create workflows that adapt to changes in data and business logic. Additionally, its extensible architecture allows integration\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_architecture.png?alt=media\u0026token=2ff31468-d678-4134-b46c-4f0da4f4ff93'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eApache Airflow's architecture is designed to be modular and scalable, consisting of several key components: the Scheduler, the Executor, the Web Server, and the Metadata Database. The Scheduler handles the scheduling of tasks, ensuring they run at the correct times and in the correct order. The Executor runs the tasks, which can be distributed across multiple workers for scalability. The Web Server provides a user-friendly interface for managing and monitoring workflows, while the Metadata Database stores the state of tasks and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eWorkflows in Airflow are defined as Directed Acyclic Graphs (DAGs), where each node represents a task, and edges define the dependencies between tasks. This structure allows for clear visualization of the workflow and its dependencies. Tasks within a DAG can range from simple data processing steps to complex data transfer operations, and they can be scheduled to run at specific intervals or triggered by external events. Airflow supports a wide range of operators for different tasks, including BashOperator for running bash scripts, PythonOperator for executing Python code, and many more for interacting with various data systems and services. This flexibility and the ability to create dynamic, code-driven workflows make Airflow a powerful tool for orchestrating data pipelines and workflows.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we will learn how to install apache airflow and create some basic DAG to perform ETL task. First we want to make sure our apache airflow in in our local machine for demonstration. We will install using docker and docker compose for it. Lets create new folder and named it as we want. In this case im going to create folder name apache_airflow. Then lets\u0026nbsp;create a Dockerfile inside that folder and fill the file with this code:\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eFROM apache/airflow:latest\nUSER root\nRUN apt-get update \u0026\u0026 \\\n    apt-get -y install git \u0026\u0026 \\\n    apt-get clean\n\nUSER airflow\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cp class=\"ql-align-justify\"\u003eOverall, this Dockerfile snippet customizes the Airflow Docker image by adding Git, which can be useful for tasks such as cloning repositories or managing code directly within the Airflow environment. Lets run this docker file. If the build image is successful, we then jump to the next step where we are going to create a new file docker compose to use our image and create a volums for our data. Below is the example of how we build our compose to run apache airflow:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3'\nservices:\n  apacheairflow:\n    image: apacheairflow:latest\n\n    volumes:\n      - ./airflow:/opt/airflow\n\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp class=\"ql-align-justify\"\u003eAfter this, we can access our apache airflow in our local machine by accessing trough \u003ca href=\"http://localhost:8080/\" target=\"_blank\"\u003ehttp://localhost:8080\u003c/a\u003e. Dont forget to build up our compose file. By running docker-compose up -d. We can type our username Admin and we wil get our password from our folder generated from /airflow/stand_alone_admin_password.txt.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can create our DAG file in our /airflow folder. Create new folder name dags and fill the python file inside of it. The name is our wishes.\u0026nbsp;Then we can create a simple task from it. But we need to import some thing like:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# Import the libraries\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models import DAG\n# Operators; you need this to write tasks!\nfrom airflow.operators.python import PythonOperator\n\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp class=\"ql-align-justify\"\u003eNow we can define our function, DAG Argument, DAG Definitions, Define the task, and create a task pipeline from this file.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eIn this case. I just want to grab some data from the api and save it to json file.Lets create our functions first called extract here is the code for the extract data from the api random jokes:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef extract():\n    response = requests.get('https://official-joke-api.appspot.com/random_joke')\n    joke = response.json()\n\n    # Define the path to the JSON file\n    file_path = '/opt/airflow/jokes/joke.json'\n    \n    try:\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Read existing jokes from the JSON file if it exists\n        if os.path.exists(file_path):\n            with open(file_path, 'r') as f:\n                # Try to load the existing jokes, handle if file is empty\n                content = f.read()\n                jokes = json.loads(content) if content else []\n        else:\n            jokes = []\n\n        # Append the new joke to the list\n        jokes.append(joke)\n        \n        # Write the updated list of jokes to the JSON file\n        with open(file_path, 'w') as f:\n            json.dump(jokes, f, indent=4)\n        \n        print(f\"Joke saved to {file_path}: {joke['setup']} - {joke['punchline']}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eAfter that we can create our DAG Arguments and DAG Definitions:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e# You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Darmawan',\n    'start_date': days_ago(0),\n    'email': ['darmawanjr88@gmail.com'],\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\ndag = DAG(\n    'jokes-callable-dag',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cp\u003eThen we can define the task named execute_extract to call the extract function\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eAnd build the the task pipeline for now we just have a single task when we have multiple task we can call our defined task like a sequence call like task_1 \u0026gt;\u0026gt; task_2 \u0026gt;\u0026gt; task_3 and so on. Now we can go to the web UI and triggering the DAG that we build.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cimg src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_1.png?alt=media\u0026token=1b05157a-20dc-4748-bb40-ff9bbaeb23be'/\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cp\u003eHorray we dit it! We can create a simple task from the apache airflow. We can make another complex task from our idea and our problems that we faced from our life.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_python.jpg?alt=media\u0026token=f6c31e13-c63f-49c5-b0ad-b54ec8509560","image_alt":"Apache airflow with Python","slug":"create-a-dag-for-apache-airflow-with-python-operator","index":"6b86b273ff34f"},{"blog_id":"cfb72d72-4754-48f1-b815-52edb986d17b","title":"Introduction to Apache Kafka","short_description":"In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration","timestamp":"2024-09-20 11:35:12","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eApache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eThink of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003eKafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eArchitecture\u003c/strong\u003e\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media\u0026token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp class=\"ql-align-justify\"\u003eThe architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:\u003c/p\u003e\u003col\u003e\u003cli class=\"ql-align-justify\"\u003eProducers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eTopics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eBrokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003ePartitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eConsumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.\u003c/li\u003e\u003cli class=\"ql-align-justify\"\u003eKafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eCore Component\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eHere's a breakdown of the core components of Kafka:\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.\u003c/p\u003e\u003cp class=\"ql-align-justify\"\u003e5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1 class=\"ql-align-justify\"\u003e\u003cstrong\u003eInstallation\u003c/strong\u003e\u003c/h1\u003e\u003cp class=\"ql-align-justify\"\u003eIn this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eversion: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eWe can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; bash\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cp\u003eNow we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003eOnce we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ekafka-console-producer --topic test --bootstrap-server localhost:9092\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eNow our producer topic is available in this terminal. we can type a few messages and hit\u0026nbsp;\u003ccode\u003eEnter\u003c/code\u003e\u0026nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edocker exec -it \u0026lt;kafka-container-id\u0026gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cp\u003eNow we should see the messages we produced in the previous step. Here is the output of what he have been doing so far\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media\u0026token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media\u0026token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eWe've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka.png?alt=media\u0026token=50fe3034-c769-4d0b-b54c-5c0ad0bbd0e5","image_alt":"Apache Kafka","slug":"introduction-to-apache-kafka","index":"6b86b273ff34f"},{"blog_id":"8be4d7fd-b2ea-4b0f-ad3b-a6064553e06b","title":"Build Kafka Python Client","short_description":"In this lab we will bring back the apache kafka to work with the application environment. In this case we are going to use the apache kafka to a simple python application.","timestamp":"2024-09-20 11:35:05","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003ePreviously we already install the apache kafka using the docker compose, create a simple workflow of how the apache is running. You can visit this site to learn the apache kafka installation and the basic workflow if you are not familir with these topic (\u003ca href=\"https://barbarpotato.github.io/#/lab/d1c5319b-3019-4576-9368-d3757bf35c6a\" target=\"_blank\" style=\"color: inherit; background-color: transparent;\"\u003eIntroduction to Apache Kafka\u003c/a\u003e). To create a Kafka Python client that interacts with your Kafka instance, you can use the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library, which is a high-performance Kafka client built on the librdkafka C library.\u003c/p\u003e\u003ch3\u003e\u003cstrong\u003eInstall the Kafka Python Client\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFirst, you need to install the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library. You can do this using pip. Note: You can create virtual environment of your project folder if you dont want to install this library to your global library package. by using the command: python -m venv \u0026lt;the name of your virtual environment\u0026gt;. then activating the venv using this command: source \u0026lt;your_venv_name\u0026gt;/Scripts/activate.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003epip install confluent-kafka\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch3\u003e\u003cstrong\u003eProduce Message\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCreate a Python script to produce messages to the Kafka topic\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom confluent_kafka import Producer\nimport socket\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\np = Producer({'bootstrap.servers': 'localhost:9092', 'client.id': socket.gethostname()})\n\ntopic = 'test'\n\nfor i in range(10):\n    p.produce(topic, key=str(i), value=f\"Message {i}\", callback=delivery_report)\n    p.poll(0)\n\np.flush()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch3\u003e\u003cstrong\u003eConsume Messages\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eCreate another Python script to consume messages from the Kafka topic.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003efrom confluent_kafka import Consumer, KafkaError\n\nc = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'mygroup',\n    'auto.offset.reset': 'earliest'\n})\n\nc.subscribe(['test'])\n\nwhile True:\n    msg = c.poll(1.0)\n\n    if msg is None:\n        continue\n    if msg.error():\n        if msg.error().code() == KafkaError._PARTITION_EOF:\n            print(f'%% {msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}')\n        elif msg.error():\n            raise KafkaException(msg.error())\n    else:\n        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n\nc.close()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003ethe consumer script initializes a Kafka consumer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eConsumer\u003c/code\u003e\u0026nbsp;class, subscribes to the specified topic(s) with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003esubscribe\u003c/code\u003e\u0026nbsp;method, and retrieves messages from the Kafka topic using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method. Finally, the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eclose\u003c/code\u003e\u0026nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client. Now you can save the file.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eTo run the producer and consumer, open two terminal windows or tabs. In the first terminal, run the producer by executing\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epython producer.py\u003c/code\u003e, and in the second terminal, run the consumer by executing\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epython consumer.py\u003c/code\u003e. The producer script will send 10 messages to the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003etest\u003c/code\u003e\u0026nbsp;topic, while the consumer script will receive and print these messages. The producer script initializes a Kafka producer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eProducer\u003c/code\u003e\u0026nbsp;class and sends messages to a specified topic with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eproduce\u003c/code\u003e\u0026nbsp;method. A\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003edelivery_report\u003c/code\u003e\u0026nbsp;callback function confirms message delivery, and the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method serves delivery reports, which should be called regularly. The\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eflush\u003c/code\u003e\u0026nbsp;method waits for all messages in the producer queue to be delivered. On the other hand, the consumer script initializes a Kafka consumer using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eConsumer\u003c/code\u003e\u0026nbsp;class, subscribes to the specified topic(s) with the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003esubscribe\u003c/code\u003e\u0026nbsp;method, and retrieves messages from the Kafka topic using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003epoll\u003c/code\u003e\u0026nbsp;method. Finally, the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003eclose\u003c/code\u003e\u0026nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client.\u003c/p\u003e\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\u003cp\u003eHere is the example output about what we are doing:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_1.png?alt=media\u0026token=4c079070-9f99-436f-b81c-8b9d0dbada30'/\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_2.png?alt=media\u0026token=7a151ac1-e386-4aef-a99a-f2d72fe611fd'/\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn conclusion, we successfully set up Apache Kafka using Docker, enabling a reliable environment for message streaming. Following this, we implemented a Kafka Python client using the\u0026nbsp;\u003ccode style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\"\u003econfluent-kafka\u003c/code\u003e\u0026nbsp;library, which provides high-performance Kafka producer and consumer capabilities. The producer script was designed to initialize a Kafka producer, send messages to a specified topic, and confirm message delivery through a callback function, while regularly polling for delivery reports and ensuring all messages are delivered with a flush operation. The consumer script was crafted to initialize a Kafka consumer, subscribe to the desired topic(s), retrieve messages, and close the consumer after committing final offsets. By running the producer and consumer scripts in separate terminal windows, you verified the setup by successfully sending and receiving messages. This end-to-end setup demonstrates a functional Kafka environment and a practical Python client for producing and consuming messages, laying a strong foundation for building more complex message-driven applications.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python.png?alt=media\u0026token=10c2db3d-b180-4bad-af2a-90aee92c47e1","image_alt":"Kafka with Pyhton","slug":"build-kafka-python-client","index":"6b86b273ff34f"},{"blog_id":"82db10ae-f820-45d6-b985-9ac22fa7046f","title":"Create GRPC With MongoDB \u0026 Node.js","short_description":"We will explore about the grpc, which is the better version of the basic HTTP1.1 and make communications more faster between client-server","timestamp":"2024-09-20 11:34:55","description":"\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is GRPC?\u003c/h1\u003e\u003cp\u003egRPC (Google Remote Procedure Call) is a high-performance, open-source framework developed by Google that enables efficient communication between distributed systems using remote procedure calls (RPCs). It leverages HTTP/2 for transport, Protocol Buffers (protobuf) for serialization, and supports features like multiplexing, streaming, and low-latency communication, making it ideal for modern microservices architectures and real-time applications. gRPC is increasingly used in this era due to its ability to handle complex, high-throughput workloads while ensuring consistent, language-agnostic communication between services, which is crucial for building scalable, resilient, and performant systems.\u003c/p\u003e\u003cp\u003egRPC is highly relevant today due to its efficient communication using Protocol Buffers for serialization, which reduces message sizes and improves performance. It leverages HTTP/2 for lower latency and higher throughput, supports multiple programming languages, and provides strongly typed contracts for consistency. With built-in support for streaming, SSL/TLS for security, and automatic code generation, gRPC enhances development speed and ensures secure, real-time communication. Its compatibility with cloud-native platforms and microservices architectures further makes it a valuable tool for modern distributed systems.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc_architecture.png?alt=media\u0026token=fff5674e-8a0b-4e46-805d-feed5b3cf0f5'/\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eThe basic architecture of gRPC involves several key components that facilitate communication between a client and server. Here's a breakdown of the architecture:\u003c/p\u003e\u003ch3\u003e1.\u0026nbsp;\u003cstrong\u003eProtocol Buffers (Protobufs):\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDefinition Language\u003c/strong\u003e: Protobufs are used to define the service methods and message types. These definitions are stored in\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;files.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSerialization\u003c/strong\u003e: Protobufs serialize data into a compact binary format, which is efficient for transmission.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e2.\u0026nbsp;\u003cstrong\u003eService Definition:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eService Interface\u003c/strong\u003e: In the\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file, services are defined with methods that can be called remotely by clients. Each method specifies the request and response message types.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e3.\u0026nbsp;\u003cstrong\u003eCode Generation:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eClient and Server Stubs\u003c/strong\u003e: The\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file is used to generate client and server code in various languages. These stubs handle the serialization and deserialization of messages and the underlying gRPC protocol details.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e4.\u0026nbsp;\u003cstrong\u003eClient-Side:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStub\u003c/strong\u003e: The client uses a generated stub to invoke methods on the server. The stub provides a local representation of the remote service.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eChannel\u003c/strong\u003e: The client creates a communication channel to the server, which manages the connection and handles the low-level details of the communication.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e5.\u0026nbsp;\u003cstrong\u003eServer-Side:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImplementation\u003c/strong\u003e: The server implements the service methods defined in the\u0026nbsp;\u003ccode\u003e.proto\u003c/code\u003e\u0026nbsp;file. These methods process incoming requests and generate responses.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eServer\u003c/strong\u003e: The server listens for incoming requests, dispatches them to the appropriate service method implementations, and sends back responses.\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003e6.\u0026nbsp;\u003cstrong\u003eCommunication:\u003c/strong\u003e\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHTTP/2\u003c/strong\u003e: gRPC uses HTTP/2 as the transport protocol, providing features like multiplexing, flow control, and header compression.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eStreams\u003c/strong\u003e: gRPC supports different types of RPCs, including unary (single request/response), server streaming, client streaming, and bidirectional streaming.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eBasic gRPC Workflow:\u003c/h1\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eDefine the Service\u003c/strong\u003e: Create a \u003ccode\u003e.proto\u003c/code\u003e file with service and message definitions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGenerate Code\u003c/strong\u003e: Use the \u003ccode\u003eprotoc\u003c/code\u003e compiler to generate client and server code from the \u003ccode\u003e.proto\u003c/code\u003e file.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eImplement the Server\u003c/strong\u003e: Write the server-side code to implement the service methods.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCreate the Client\u003c/strong\u003e: Write the client-side code to call the service methods using the generated stub.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEstablish Communication\u003c/strong\u003e: The client and server communicate over a channel using HTTP/2, with data serialized and deserialized using Protobufs.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch3\u003eIn gRPC, the client can indeed send data to the server. gRPC supports several types of communication patterns, including:\u003c/h3\u003e\u003ch3\u003e1. Unary RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a single request to the server and receives a single response.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc SayHello (HelloRequest) returns (HelloReply) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003e2. Server Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a single request to the server and receives a stream of responses.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc ListFeatures (Rectangle) returns (stream Feature) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003e3. Client Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eThe client sends a stream of requests to the server and receives a single response.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc RecordRoute (stream Point) returns (RouteSummary) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch3\u003e4. Bidirectional Streaming RPC:\u003c/h3\u003e\u003cul\u003e\u003cli\u003eBoth the client and server send a stream of messages to each other.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003erpc RouteChat (stream RouteNote) returns (stream RouteNote) {}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch1\u003eExample\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003eLet's try to do the key list that we define in above. in this example we are going to implement the unary RPC for the sake of the simplicity. and in this example we are going to implement the grpc within the node.js with the helper of our beloved database called MongoDb. for this example what we are going to achieve is the client side can search the book data from the mongodb database. To ensure that the client can get the data successfully, the client side need to get interact with our grpc server. in a nutshell, the server side grpc will serve the client side. To create a gRPC server in Node.js, we need to install the required packages:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install @grpc/grpc-js @grpc/proto-loader\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003cp\u003eafter the installation is complete, we then create a proto file. in this file we define the service and the message that we are going to build. here is the example code:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-22\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003esyntax = \"proto3\";\n\nservice Book {\n  rpc GetBook (GetBookRequest) returns (BookReply) {}\n}\n\nmessage GetBookRequest {\n  string Title = 1;\n}\n\nmessage BookReply {\n  string Title = 1;\n  string Author = 2;\n  int32 Published = 3;\n  string Language = 4;\n  string Id = 5;\n  int32 Sales = 6;\n}\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-23\"\u003e\u003cp\u003ethe proto file is ready. now we can load it up trough our core script to running our node server. dont forget that we are using the mongodb database. so the first step is to setup the connection between the mongodb and the server, and then we build up the grpc service in a top of mongodb service.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-32\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\nconst path = require('path');\nconst { MongoClient } = require('mongodb');\n\n// Load the protobuf file\nconst PROTO_PATH = path.resolve(__dirname, 'service.proto');\nconst packageDefinition = protoLoader.loadSync(PROTO_PATH);\nconst proto = grpc.loadPackageDefinition(packageDefinition);\n\n// MongoDB connection URL and Database name\nconst url = 'mongodb://localhost:27017';\nconst dbName = 'Book';\n\nlet db;\nlet client;\n\n// Connect to MongoDB and start gRPC server\nasync function main() {\n    const client = new MongoClient(url);\n\n    try {\n        await client.connect();\n        console.log('Connected to MongoDB');\n        db = client.db(dbName);\n\n        // Start gRPC server after MongoDB connection\n        startGrpcServer();\n    } catch (e) {\n        console.error('Failed to connect to MongoDB', e);\n    } finally {\n        // Ensure MongoClient is closed if needed\n        // Uncomment if you need to close the client here:\n        // await client.close();\n    }\n}\n\n\n// gRPC service method\nfunction GetBook(call, callback) {\n    console.log('Received request:', call.request);\n    const { Title } = call.request;\n    const collection = db.collection('list');\n\n    console.log('Performing findOne query with Book:', Title);\n    collection.findOne({ Book: Title }, { maxTimeMS: 5000 })\n        .then(result =\u0026gt; {\n            console.log('findOne query completed');\n            if (result) {\n                const response = {\n                    Title: result.Book,\n                    Author: result['Author(s)'], // Adjusted field names\n                    Published: result['First published'],\n                    Language: result['Original language'],\n                    Id: result._id.toString(),\n                    Sales: result['Approximate sales in millions']\n                };\n                console.log('Sending response:', response);\n                callback(null, response);\n            } else {\n                console.log('Book not found');\n                callback(null, { Title: \"Book not found\" });\n            }\n        })\n        .catch(err =\u0026gt; {\n            console.error('Error fetching data:', err);\n            callback(err, null);\n        });\n}\n\n\n// Start the gRPC server\nfunction startGrpcServer() {\n    const server = new grpc.Server();\n    server.addService(proto.Book.service, { GetBook: GetBook });\n    const port = '50051';\n    server.bindAsync(`0.0.0.0:${port}`, grpc.ServerCredentials.createInsecure(), (error, port) =\u0026gt; {\n        if (error) {\n            console.error(`Failed to bind server: ${error.message}`);\n            process.exit(1); // Exit process if server fails to start\n        }\n        console.log(`Server running at http://0.0.0.0:${port}`);\n    });\n}\n\n// Run the main function\nmain();\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-33\"\u003e\u003cp\u003eWe can now run the server. by using the command node \u0026lt;the name of your server code file\u0026gt;\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-34\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc-step-1.png?alt=media\u0026token=cef277a6-d5e2-45d9-80f2-6871e3715089'/\u003e\u003c/div\u003e\u003cdiv id=\"content-35\"\u003e\u003cp\u003eThe setup and implementation of the server.js is completed. now we move to the code of the client side. which is more simple. here is the code to call the available service from the server side that we implement previously. where in here we can access the GetBook service. and try to search the field named book with value\u0026nbsp;Where the Crawdads Sing\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-36\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003econst grpc = require('@grpc/grpc-js');\nconst protoLoader = require('@grpc/proto-loader');\nconst path = require('path');\n\n// Load the protobuf file\nconst PROTO_PATH = path.resolve(__dirname, 'service.proto');\nconst packageDefinition = protoLoader.loadSync(PROTO_PATH);\nconst proto = grpc.loadPackageDefinition(packageDefinition);\n\n// Create a new gRPC client\nconst client = new proto.Book('localhost:50051', grpc.credentials.createInsecure());\n\n// Call the service\nclient.GetBook({ Title: 'Where the Crawdads Sing' }, (error, response) =\u0026gt; {\n    if (error) {\n        console.error(`Error: ${error.message}`);\n    } else {\n        console.log(`Response: ${JSON.stringify(response)}`);\n    }\n});\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-37\"\u003e\u003cp\u003eWe can run this client side. and it the result of the book that we search is delivered to the client side.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-38\"\u003e\u003cimg style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc-step-2.png?alt=media\u0026token=85d657cc-6260-492d-a70e-62d26b038da2'/\u003e\u003c/div\u003e\u003cdiv id=\"content-40\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003egRPC is a powerful framework for building efficient, high-performance communication between distributed systems. By leveraging HTTP/2 and Protocol Buffers (protobuf), gRPC enables low-latency, scalable interactions ideal for microservices and real-time applications. In this example, we demonstrated how to set up a gRPC server in Node.js that interacts with a MongoDB database. We defined a service using Protocol Buffers, implemented the server to handle requests, and created a client to query book data. This approach allows for efficient data retrieval and robust communication between client and server, showcasing gRPC's capabilities in modern software development. By understanding gRPC's architecture and workflow, and implementing it in a real-world scenario, you can build scalable, high-performance systems that handle complex workloads with ease.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc_background.png?alt=media\u0026token=3df4f4c7-ea5f-4170-ae16-6656671bfd4a","image_alt":"GRPC Background","slug":"create-grpc-with-mongodb--nodejs","index":"6b86b273ff34f"},{"blog_id":"40c0ec5c-b8f4-4965-8bd6-be9f65e867fb","title":"Test-Driven Development with Python","short_description":"In the fast-paced world of software development, delivering high-quality code consistently is crucial. One of the methodologies that can help achieve this goal is Test-Driven Development (TDD). ","timestamp":"2024-09-20 11:34:49","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003e\u003cstrong\u003eIntroduction\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eIn the fast-paced world of software development, delivering high-quality code consistently is crucial. One of the methodologies that can help achieve this goal is Test-Driven Development (TDD). TDD is not just a testing technique; it’s a design methodology that can profoundly impact how you write and maintain code. This blog post will explore what TDD is, its benefits, and how you can start incorporating it into your development workflow.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch1\u003e\u003cstrong\u003eWhat is Test-Driven Development (TDD)?\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eTest-Driven Development is a software development process where you write tests before writing the actual code. The cycle typically follows three steps, known as the \u003cstrong\u003eRed-Green-Refactor\u003c/strong\u003e cycle:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRed\u003c/strong\u003e: Write a test for a new functionality. Since the functionality is not yet implemented, the test will fail (hence the \"red\" phase).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGreen\u003c/strong\u003e: Write the minimum amount of code required to pass the test. At this stage, you aim to make the test pass, even if the solution isn’t perfect.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRefactor\u003c/strong\u003e: Improve the code without changing its functionality. This might involve cleaning up code, optimizing performance, or improving readability.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003cp\u003eThis cycle is repeated for every piece of functionality you want to add to your application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003ch1\u003e\u003cstrong\u003eBenefits of TDD\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eBetter Code Quality\u003c/strong\u003e: Test-Driven Development (TDD) promotes writing only the code needed to pass predefined tests, which helps prevent overengineering. This focus on minimalism means developers are less likely to introduce unnecessary complexity. Moreover, the tests themselves act as a safety net, identifying bugs early in the development process. By catching issues before they escalate, TDD ensures that the code remains clean, efficient, and easy to understand.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImproved Design\u003c/strong\u003e: TDD forces developers to think about the design and functionality of their code before they start writing it. This upfront consideration leads to better-structured, more modular, and decoupled code. Since the tests are written first, they guide the design, ensuring that each piece of the codebase is independently testable and reusable. This modularity not only makes the code easier to maintain but also simplifies future enhancements and refactoring.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFaster Debugging\u003c/strong\u003e: One of the standout benefits of TDD is its ability to streamline the debugging process. When a test fails, it provides an immediate indication of where the problem lies, allowing developers to pinpoint the issue quickly. Instead of spending hours sifting through code to find the root cause of a bug, developers can focus on the specific area that triggered the test failure, significantly reducing debugging time.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDocumentation\u003c/strong\u003e: TDD naturally results in a comprehensive suite of tests that serve as living documentation for the codebase. These tests provide concrete examples of how functions and classes are expected to behave, making it easier for other developers (or even your future self) to understand the code’s purpose and usage. Unlike traditional documentation, which can become outdated, these tests are constantly updated as the code evolves, ensuring they remain relevant and accurate.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eIncreased Confidence\u003c/strong\u003e: With a robust suite of tests in place, developers can refactor or extend their code with greater confidence. The tests act as a safeguard, ensuring that any unintended side effects are caught immediately. This level of assurance is particularly valuable when making significant changes to the codebase, as it minimizes the risk of introducing new bugs and helps maintain the overall stability of the application.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003e\u003cstrong\u003eCommon Misconceptions About TDD\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eTDD is just about testing\u003c/strong\u003e: While the name might suggest that TDD is primarily focused on testing, it’s actually more about guiding the design of your code. The tests you write before the implementation help shape the structure and functionality of the code. TDD drives the development process, ensuring that the code is built to meet the specific requirements laid out in the tests, which results in a more thoughtful and deliberate design.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTDD slows down development\u003c/strong\u003e: At first glance, TDD may seem to slow down development, as it requires writing tests before the actual code. However, this initial investment pays off in the long run. By catching bugs early and reducing the time spent on debugging, TDD often leads to faster overall development. Additionally, the improved code quality and design that result from TDD can reduce the need for extensive refactoring later in the project, further accelerating the development process.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTDD is only for large projects\u003c/strong\u003e: Another common misconception is that TDD is only beneficial for large, complex projects. In reality, TDD can be applied to projects of any size. Even in small projects, TDD helps ensure that the codebase remains maintainable and free of bugs. By adopting TDD from the start, developers can build a strong foundation that makes it easier to scale the project as it grows. The principles of TDD—writing clean, testable code—are universally applicable, regardless of the project's scope.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cp\u003eHere's a simple example of a Test-Driven Development (TDD) workflow using \u003ccode\u003eunittest\u003c/code\u003e in Python. This example demonstrates the TDD cycle: \u003cstrong\u003eRed-Green-Refactor\u003c/strong\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch1\u003e\u003cstrong\u003eScenario\u003c/strong\u003e:\u003c/h1\u003e\u003cp\u003eYou want to create a function that returns the factorial of a number.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 1: Write a Failing Test (Red)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFirst, you write a test for the functionality you want to implement, even though the function doesn’t exist yet. This test will naturally fail because the function isn’t defined.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport unittest\n\nclass TestFactorialFunction(unittest.TestCase):\n    def test_factorial_of_5(self):\n        result = factorial(5)\n        self.assertEqual(result, 120)\n\nif __name__ == '__main__':\n    unittest.main()\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cp\u003e\u003cstrong\u003eExplanation\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThis test checks if the \u003ccode\u003efactorial\u003c/code\u003e function correctly returns 120 when called with the argument \u003ccode\u003e5\u003c/code\u003e.\u003c/li\u003e\u003cli\u003eSince the \u003ccode\u003efactorial\u003c/code\u003e function doesn't exist, running this test will produce an error.\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 2: Implement the Minimum Code to Pass the Test (Green)\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eNext, you implement the \u003ccode\u003efactorial\u003c/code\u003e function, just enough to pass the test.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cp\u003eNow, if you run the test, it should pass:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e$ python test_factorial.py\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e.\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nOK\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-17\"\u003e\u003ch3\u003e\u003cstrong\u003eStep 3: Refactor the Code\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eFinally, you can refactor the code if necessary to improve its design or efficiency. In this case, the code is already quite clean, so no major refactoring is needed. However, if there were redundant code or opportunities to optimize, you would do that in this step.\u003c/p\u003e\u003cp\u003eYou could also add more tests to cover additional cases, such as the factorial of \u003ccode\u003e0\u003c/code\u003e or negative numbers, ensuring your function is robust:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-18\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eclass TestFactorialFunction(unittest.TestCase):\n    def test_factorial_of_5(self):\n        result = factorial(5)\n        self.assertEqual(result, 120)\n\n    def test_factorial_of_0(self):\n        result = factorial(0)\n        self.assertEqual(result, 1)\n\n    def test_factorial_of_1(self):\n        result = factorial(1)\n        self.assertEqual(result, 1)\n\n    def test_factorial_of_negative(self):\n        with self.assertRaises(ValueError):\n            factorial(-1)\n\nif __name__ == '__main__':\n    unittest.main()\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-19\"\u003e\u003cp\u003e\u003cstrong\u003eExplanation\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThese additional tests handle edge cases, such as when \u003ccode\u003en\u003c/code\u003e is \u003ccode\u003e0\u003c/code\u003e, \u003ccode\u003e1\u003c/code\u003e, or negative.\u003c/li\u003e\u003cli\u003eThe \u003ccode\u003etest_factorial_of_negative\u003c/code\u003e test checks that the function raises a \u003ccode\u003eValueError\u003c/code\u003e when given a negative input.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf the function doesn't handle these cases yet, you would modify it:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-20\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003edef factorial(n):\n    if n \u0026lt; 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    elif n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n - 1)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-21\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eFrom this simple scenario, we know that the Test-Driven Development is more than just a testing technique—it’s a powerful design methodology that can lead to cleaner, more maintainable, and bug-free code. By adopting TDD, you can improve your development process, catch bugs early, and build software that stands the test of time. If you haven’t tried TDD yet, now is the perfect time to start. Happy coding!\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Ftest-driven-development-TDD.webp?alt=media\u0026token=c42519d2-3fd0-4451-af98-f2626ba059ba","image_alt":"tdd life cycle","slug":"test-driven-development-with-python","index":"6b86b273ff34f"},{"blog_id":"b731f0f7-895d-46df-9377-cb5bf9008e0a","title":"Build Redis in Next.js","short_description":"Redis, which stands for Remote Dictionary Server, is an in-memory data structure store that is commonly used as a database, cache, and message broker. ","timestamp":"2024-09-20 11:34:41","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eWhat is Redis?\u003c/h1\u003e\u003cp\u003eRedis is a key-value store that operates entirely in memory, making it significantly faster compared to traditional databases that store data on disk. Redis is known as a \u003cstrong\u003eNoSQL\u003c/strong\u003e database and is often used in applications that require high-speed data access.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch2\u003eAdvantages of Redis\u003c/h2\u003e\u003cp\u003eRedis offers several advantages that make it an appealing choice for many developers:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis stores data in memory rather than on disk. This means read and write operations occur in milliseconds, making Redis ideal for applications that require very fast response times.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSupport for Various Data Structures\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis doesn’t just store strings but also supports various data structures such as hashes, lists, sets, and sorted sets. This provides flexibility in how data can be organized and accessed.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eData Persistence\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eAlthough Redis is an in-memory database, it has options to save data to disk, either periodically or based on direct commands. This ensures that data is not lost when Redis is shut down or crashes.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eReplication\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis supports master-slave replication, where data can be replicated to multiple slave servers. This not only increases data availability but also allows Redis to scale the read load.\u003c/p\u003e\u003ch3\u003e5. \u003cstrong\u003ePub/Sub Mechanism\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis features a publish/subscribe mechanism that allows systems to communicate in real-time by subscribing to certain channels and receiving messages whenever they are published to those channels. This is particularly useful for building real-time applications such as chat systems and notifications.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch1\u003eUsing Redis in Applications\u003c/h1\u003e\u003cp\u003eRedis is often used in various scenarios, some of which include:\u003c/p\u003e\u003ch3\u003e1. \u003cstrong\u003eCache\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis is frequently used as a cache to store database query results or frequently accessed data, reducing the load on the main database and improving application response times.\u003c/p\u003e\u003ch3\u003e2. \u003cstrong\u003eSession Store\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eMany web applications use Redis to store user sessions. Using Redis allows sessions to be quickly accessed and deleted when no longer needed.\u003c/p\u003e\u003ch3\u003e3. \u003cstrong\u003eQueue Management\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eRedis is used to manage job queues in distributed systems, such as background jobs, task scheduling, and more.\u003c/p\u003e\u003ch3\u003e4. \u003cstrong\u003eReal-time Analytics\u003c/strong\u003e\u003c/h3\u003e\u003cp\u003eDue to its speed, Redis is often used for real-time data processing, such as counting website visitors or monitoring application performance.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003ch1\u003eInstallation\u003c/h1\u003e\u003cp\u003eIn this lab, we are going to use the Redis services provided by vercel platform. To integrate Redis from Vercel into your Next.js project, you can follow these steps:\u003c/p\u003e\u003col\u003e\u003cli\u003eGo to the Vercel Redis dashboard and create a new Redis instance.\u003c/li\u003e\u003cli\u003eConfigure the Redis instance with the desired settings.\u003c/li\u003e\u003cli\u003eAfter creating the Redis instance, you will receive a connection URL, which typically looks like \u003ccode\u003erediss://:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u003c/code\u003e.\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003ch3\u003eInstall Redis Client for Next.js\u003c/h3\u003e\u003cp\u003eYou will need a Redis client to connect to the Redis instance in your Next.js project. You can use the \u003ccode\u003eioredis\u003c/code\u003e library, a popular Redis client for Node.js.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003enpm install ioredis\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003ch3\u003eConfigure Environment Variables\u003c/h3\u003e\u003cp\u003eAdd the Redis connection URL to your environment variables. Create a \u003ccode\u003e.env.local\u003c/code\u003e file in the root of your Next.js project:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-9\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eREDIS_URL=rediss://:\u0026lt;password\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch3\u003eConnect to Redis in Next.js\u003c/h3\u003e\u003cp\u003eCreate a utility function to handle Redis connections. You can create a file like \u003ccode\u003eredis.js\u003c/code\u003e inside the \u003ccode\u003elib\u003c/code\u003e folder (or any other suitable location in your project):\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003e// lib/redis.js\nimport Redis from 'ioredis';\n\nlet client;\n\nif (!client) {\n  client = new Redis(process.env.REDIS_URL);\n}\n\nexport default client;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003ch3\u003eUse Redis in API Routes or Server Components\u003c/h3\u003e\u003cp\u003eYou can now use this Redis client in your API routes or server components. Here is an example of an API route using Redis:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport redis from \"@/app/libs/redis\";\n\nconst getHandler = async (req, res) =\u0026gt; {\n\n    try {\n\n        // Get the value from Redis\n        const value = await redis.get(\"Darwin AI: Active\");\n\n        // Send the response of a darwin service status\n        if (value) {\n            res.status(200).json({ \"Darwin\": \"Active\" });\n        } else {\n            res.status(200).json({ \"Darwin\": \"Inactive\" });\n        }\n    } catch (error) {\n        res.status(500).json({ error });\n    }\n}\n\nconst postHandler = async (req, res) =\u0026gt; {\n\n    try {\n        const current_timestamp = Date.now();\n\n        // Set a value in Redis. make it expired after 10 minutes\n        await redis.set(\"Darwin AI: Active\", current_timestamp, 'EX', 600);\n\n        // Get the value from Redis\n        const value = await redis.get(\"Darwin AI: Active\");\n\n        // Send the response of a darwin service status\n        if (value) {\n            res.status(200).json({ \"Darwin\": \"Active\" });\n        } else {\n            res.status(200).json({ \"Darwin\": \"Inactive\" });\n        }\n    } catch (error) {\n        res.status(500).json({ error });\n    }\n}\n\n\nconst handler = async (req, res) =\u0026gt; {\n    if (req.method === 'GET') {\n        return getHandler(req, res);\n    }\n    else if (req.method === 'POST') {\n        return postHandler(req, res);\n    }\n    else if (req.method === 'DELETE') {\n        return deleteHandler(req, res);\n    }\n    else {\n        return res.status(405).end(`Method ${req.method} Not Allowed`);\n    }\n};\n\nexport default handler;\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eRedis is a powerful and versatile tool in the world of application development. With its high speed, support for various data structures, and advanced features like persistence and replication, Redis can be used to significantly enhance application performance. In today’s fast-paced world, Redis offers an efficient solution to many challenges faced by developers.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fnext_redis.png?alt=media\u0026token=c1f3ad9d-f034-4581-91ab-40d65814e491","image_alt":"redis in next.js","slug":"build-redis-in-nextjs","index":"6b86b273ff34f"},{"blog_id":"1fab12ad-ebb2-435c-9dc7-17e4015e9b95","title":"Performance Testing with k6","short_description":"k6 is a command-line based tool specifically designed for load testing and performance testing. It allows you to simulate multiple users accessing your application, which helps identify potential issues when traffic spikes.","timestamp":"2024-09-20 11:34:02","description":"\u003cdiv id=\"content-0\"\u003e\u003ch1\u003eUnderstanding k6: A Load and Performance Testing Tool\u003c/h1\u003e\u003cp\u003eWhen developing an application, ensuring it can handle high traffic is crucial. This is where load testing and performance testing come into play, and \u003cstrong\u003ek6\u003c/strong\u003e is a powerful tool designed for this purpose. By using k6, you can assess whether your application can handle heavy traffic and perform optimally under various conditions.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-1\"\u003e\u003ch1\u003eWhat is k6?\u003c/h1\u003e\u003cp\u003ek6 is a command-line based tool specifically designed for load testing and performance testing. It allows you to simulate multiple users accessing your application, which helps identify potential issues when traffic spikes.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eKey Points About k6:\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTerminal-Based:\u003c/strong\u003e k6 operates directly through the terminal.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eJavaScript Testing:\u003c/strong\u003e Tests are written in JavaScript, making it accessible for web developers.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eWhat k6 Does Not Do\u003c/h2\u003e\u003cp\u003eWhile k6 is a robust tool, it has its limitations:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eNo Browser Interaction:\u003c/strong\u003e k6 does not run through a browser.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo Node.js Dependency:\u003c/strong\u003e Unlike many JavaScript tools, k6 doesn't rely on Node.js but is built using Golang.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eNo npm Modules Support:\u003c/strong\u003e If you require additional modules, you can't install them via \u003ccode\u003enpm install\u003c/code\u003e. Instead, you must include the files directly in your project.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eLimitations of k6\u003c/h2\u003e\u003cp\u003ek6 uses Golang's Goja library to execute JavaScript, which means it doesn’t support some of the features available in Node.js. This limitation requires a different approach when setting up your testing environment.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-2\"\u003e\u003ch2\u003eInstallation\u003c/h2\u003e\u003cp\u003eYou can easily install k6 by following the instructions on their \u003ca href=\"https://k6.io/docs/get-started/installation/\" target=\"_blank\"\u003eofficial installation page\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-3\"\u003e\u003ch1\u003eInitial Project Setup\u003c/h1\u003e\u003cp\u003eTo get started with k6, you can set up a Node.js project:\u003c/p\u003e\u003cul\u003e\u003cli\u003eInitialize the project with \u003ccode\u003enpm init\u003c/code\u003e and set the type to \u003ccode\u003emodule\u003c/code\u003e.\u003c/li\u003e\u003cli\u003eInstall k6 as a module in your project.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003ek6 Library\u003c/h2\u003e\u003cp\u003ek6 comes with its own library that simplifies the process of creating tests. However, remember that the npm package for k6 doesn't contain actual JavaScript code—it’s mostly metadata. The k6 tool itself is built on Golang, so it doesn’t run on Node.js.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-4\"\u003e\u003cp\u003eIf you inspect the k6 module, you’ll find that it’s essentially an empty package with just a \u003ccode\u003epackage.json\u003c/code\u003e format.\u003c/p\u003e\u003ch3\u003eWriting k6 Scripts\u003c/h3\u003e\u003cp\u003eScripts in k6 are written in JavaScript and contain the code for the performance test along with the test scenarios you wish to execute. You can create a script manually or use a command to generate a basic script:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-5\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 new src/ping.js\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-6\"\u003e\u003cp\u003eThis command will create a new script at the specified location with a basic setup for performance testing.\t\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-7\"\u003e\u003ch3\u003eRunning a Script\u003c/h3\u003e\u003cp\u003eTo execute the performance test, use the following command:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-8\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 run lokasi/file.js\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-10\"\u003e\u003ch1\u003eChecking the code\u003c/h1\u003e\u003c/div\u003e\u003cdiv id=\"content-11\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003eimport http from 'k6/http';\nimport { sleep } from 'k6';\n\nexport const options = {\n  stages: [\n    { duration: '10s', target: 10 },\n    { duration: '20s', target: 30 },\n    { duration: '30s', target: 50 },\n    { duration: '10s', target: 0 }\n  ]\n  // // A number specifying the number of VUs to run concurrently.\n  // vus: 10,\n  // // A string specifying the total duration of the test run.\n  // duration: '30s',\n\n  const body = {\n    name: \"Darmawan\"\n  };\n};\n\nexport default function () {\n\n  http.post(\"https://httpbin.test.k6.io/post\", JSON.stringify(body), {\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"Accept\": \"application/json\"\n    }\n  })\n\n  http.get('https://cerberry-backend.vercel.app/blogs/all');\n  sleep(0.1);\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-12\"\u003e\u003cp\u003ewe'll explore a simple yet effective k6 script designed to conduct load testing on an API. The script simulates multiple virtual users (VUs) interacting with the application, allowing us to observe how it performs under varying levels of traffic. To start, the script imports the necessary modules from k6—\u003ccode\u003ehttp\u003c/code\u003e for making HTTP requests and \u003ccode\u003esleep\u003c/code\u003e to simulate user think time by pausing execution for a short duration.\u003c/p\u003e\u003cp\u003eThe core of the script lies in the \u003ccode\u003eoptions\u003c/code\u003e object, where we define a series of stages that dictate how the load should be applied. The test begins by gradually ramping up to 10 virtual users over the first 10 seconds. It then continues to increase to 30 users over the next 20 seconds, and finally reaches 50 users over a 30-second period. After this peak, the number of users is gradually reduced back to zero over the last 10 seconds. This staged approach provides valuable insights into the scalability and robustness of the application as it experiences increasing and then decreasing traffic.\u003c/p\u003e\u003cp\u003eWithin the \u003ccode\u003edefault\u003c/code\u003e function, the script defines two key actions that each virtual user will perform. First, a POST request is made to \u003ccode\u003ehttps://httpbin.test.k6.io/post\u003c/code\u003e, sending a JSON payload with a name value of \"Darmawan\". This simulates a scenario where a user might submit data to the server, such as filling out a form. Immediately after, a GET request is sent to \u003ccode\u003ehttps://cerberry-backend.vercel.app/blogs/all\u003c/code\u003e, retrieving a list of blogs. This sequence mirrors typical user behavior in a web application, where data submission is often followed by data retrieval or refreshing content. To simulate a brief delay between these actions, the script includes a \u003ccode\u003esleep(0.1)\u003c/code\u003e function, representing a short pause of 0.1 seconds.\u003c/p\u003e\u003cp\u003eTo execute this script, you would simply run it using the k6 command in your terminal. This approach allows you to assess how well your application can handle varying levels of traffic, identify potential bottlenecks, and ensure that it remains performant under load. By gradually increasing and then decreasing the load, this script provides a comprehensive view of your application's performance, helping you make informed decisions about optimizations and improvements.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-13\"\u003e\u003ch2\u003eUnderstanding the Summary Output\u003c/h2\u003e\u003cp\u003eAfter running a performance test, k6 generates a summary output that details the test results. By default, this output is displayed in the terminal, but you can also export it as a JSON file:\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-14\"\u003e\u003cpre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"\u003e\u003ccode style=\"color: white;\"\u003ek6 run file/script.js --summary-export output.json\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv id=\"content-15\"\u003e\u003cp\u003eYou can find more information on the output metrics in the \u003ca href=\"https://grafana.com/docs/k6/latest/using-k6/metrics/reference/\" target=\"_blank\"\u003eGrafana documentation\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003cdiv id=\"content-16\"\u003e\u003ch2\u003eUsing the Web Dashboard\u003c/h2\u003e\u003cp\u003ek6 also offers a Web Dashboard for real-time monitoring of your tests. To enable this feature, you need to activate it through an environment variable. Detailed instructions are available in the \u003ca href=\"https://grafana.com/docs/k6/latest/results-output/web-dashboard/\" target=\"_blank\"\u003eWeb Dashboard documentation\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eUsing Stages for Dynamic Load Testing\u003c/h2\u003e\u003cp\u003ek6 provides a powerful feature called \u003cstrong\u003eStages\u003c/strong\u003e, which allows you to increase or decrease the number of virtual users during the test. This is particularly useful for simulating different traffic patterns over time. You can learn more about this feature in the \u003ca href=\"https://grafana.com/docs/k6/latest/using-k6/k6-options/reference/#stages\" target=\"_blank\"\u003eStages documentation\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBy using k6 effectively, you can ensure your application is well-prepared to handle real-world traffic, preventing potential issues before they reach your users.\u003c/p\u003e\u003c/div\u003e","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fk6.png?alt=media\u0026token=ee045685-94ba-421d-a189-aeb94cd7aa8c","image_alt":"k6 - performance testing","slug":"performance-testing-with-k6","index":"6b86b273ff34f"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"LEepomGiW7qIKpSxf8WMV","assetPrefix":"/Labs-6b86b273ff34f","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>